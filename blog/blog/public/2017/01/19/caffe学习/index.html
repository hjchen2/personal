<!doctype html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="caffe,deep learning,framework," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta name="description" content="caffe学习总结
caffe的由来
caffe是贾扬清在UC Berkeley攻读计算机科学博士学位时开发的一套深度学习框架，由于高效、易读和模块化的设计，开源后经过nvidia的帮助优化和社区不断的完善，如今成为视觉领域主流的框架之一。

贾扬清其人
清华大学的本硕，UC Berkeley的计算机科学博士，师承Prof. Trevor Darrell，期间在新加坡国立大学、微软亚洲研究院、NE">
<meta property="og:type" content="article">
<meta property="og:title" content="caffe学习总结">
<meta property="og:url" content="https://hjchen2.github.io/2017/01/19/caffe学习/index.html">
<meta property="og:site_name" content="Don't Respond">
<meta property="og:description" content="caffe学习总结
caffe的由来
caffe是贾扬清在UC Berkeley攻读计算机科学博士学位时开发的一套深度学习框架，由于高效、易读和模块化的设计，开源后经过nvidia的帮助优化和社区不断的完善，如今成为视觉领域主流的框架之一。

贾扬清其人
清华大学的本硕，UC Berkeley的计算机科学博士，师承Prof. Trevor Darrell，期间在新加坡国立大学、微软亚洲研究院、NE">
<meta property="og:image" content="https://github.com/hjchen2/personal/blob/master/blog/caffe框架学习/caffe-001.png?raw=true">
<meta property="og:image" content="https://github.com/hjchen2/personal/blob/master/blog/caffe框架学习/caffe-002.png?raw=true">
<meta property="og:image" content="https://github.com/hjchen2/personal/blob/master/blog/caffe框架学习/caffe-003.png?raw=true">
<meta property="og:image" content="https://github.com/hjchen2/personal/blob/master/blog/caffe框架学习/caffe-004.jpg?raw=true">
<meta property="og:image" content="https://github.com/hjchen2/personal/blob/master/blog/caffe框架学习/caffe-005.jpg?raw=true">
<meta property="og:updated_time" content="2017-08-22T04:56:03.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="caffe学习总结">
<meta name="twitter:description" content="caffe学习总结
caffe的由来
caffe是贾扬清在UC Berkeley攻读计算机科学博士学位时开发的一套深度学习框架，由于高效、易读和模块化的设计，开源后经过nvidia的帮助优化和社区不断的完善，如今成为视觉领域主流的框架之一。

贾扬清其人
清华大学的本硕，UC Berkeley的计算机科学博士，师承Prof. Trevor Darrell，期间在新加坡国立大学、微软亚洲研究院、NE">
<meta name="twitter:image" content="https://github.com/hjchen2/personal/blob/master/blog/caffe框架学习/caffe-001.png?raw=true">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"hide","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '作者'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://hjchen2.github.io/2017/01/19/caffe学习/"/>





  <title> caffe学习总结 | Don't Respond </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  














  
  
    
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Don't Respond</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            全部
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocapitalize="off" autocomplete="off" autocorrect="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://hjchen2.github.io/2017/01/19/caffe学习/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Dou Jiang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="http://ww3.sinaimg.cn/mw690/7ee54514jw8f3qevr3d0oj20hs0hsgme.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Don't Respond">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                caffe学习总结
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-01-19T12:31:08+08:00">
                2017-01-19
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/ML-framework/" itemprop="url" rel="index">
                    <span itemprop="name">ML framework</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
            <!--noindex-->
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/01/19/caffe学习/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count hc-comment-count" data-xid="2017/01/19/caffe学习/" itemprop="commentsCount"></span>
                </a>
              </span>
              <!--/noindex-->
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="caffe学习总结">caffe学习总结</h1>
<h2 id="caffe的由来">caffe的由来</h2>
<p>caffe是贾扬清在UC Berkeley攻读计算机科学博士学位时开发的一套深度学习框架，由于高效、易读和模块化的设计，开源后经过nvidia的帮助优化和社区不断的完善，如今成为视觉领域主流的框架之一。</p>
<ul>
<li><p>贾扬清其人<br>
清华大学的本硕，UC Berkeley的计算机科学博士，师承Prof. Trevor Darrell，期间在新加坡国立大学、微软亚洲研究院、NEC美国实验室和google研究院实习和工作。博士毕业后一直在google brain担任研究科学家，致力于机器视觉、深度学习和tensorflow相关工作。2016年2月加入facebook，主导facebook大多数AI应用的通用、大规模机器学习平台（目前以caffe2为基础的caffe2go已经开源）。</p></li>
<li><p>为什么要开发caffe<br>
贾最早开发的是另一款软件Decaf，主要功能是基于cuda-convnet进行CNN训练。2013年贾扬清读博期间跟心理学老师合作研究使用概率框架来表达人的行为，“但是因为图像上提取的特征比较弱，所以可以外推的结果比较有限”，而2012年Alex Krizhevsky提出的AlexNet在ImageNet比赛中大获成功，贾因此也希望将CNN应用到他们的心理学研究上，于是就开始写了Decaf，通过Decaf验证了“深度学习特征的优异的可移植性”，因此就开始开发一套通用的深度学习框架，即后来的caffe。</p></li>
</ul>
<h2 id="caffe与其他一些主流框架的比较">caffe与其他一些主流框架的比较</h2>
<p>caffe同期也存在其他一些开源框架，比如cuda-convnet、theano、torch等，并且后来又陆续开源了neon、mxnet、tensorflow、CNTK以及paddled等等。现在对于研究者，如何选择一个框架也成了一个麻烦的问题了。下图是2014年贾扬清在caffe论文中对当时的一些框架做的一个比较：</p>
<div align="center">
<img src="https://github.com/hjchen2/personal/blob/master/blog/caffe框架学习/caffe-001.png?raw=true" width="800">
</div>
<p>下面是近年主流框架的一个简单比较：</p>
<ul>
<li>特性</li>
</ul>
<table>
<thead>
<tr class="header">
<th></th>
<th align="center">主语言</th>
<th align="center">从语言</th>
<th align="center">硬件</th>
<th align="center">分布式</th>
<th align="center">命令式</th>
<th align="center">声明式</th>
<th align="center">自动梯度</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>caffe</td>
<td align="center">C++</td>
<td align="center">Python/Matlab</td>
<td align="center">CPU/GPU</td>
<td align="center">✖</td>
<td align="center">✖</td>
<td align="center">✔</td>
<td align="center">✖</td>
</tr>
<tr class="even">
<td>mxnet</td>
<td align="center">C++</td>
<td align="center">Python/R/Julia/Scala</td>
<td align="center">CPU/GPU/Mobile</td>
<td align="center">✔</td>
<td align="center">✔</td>
<td align="center">✔</td>
<td align="center">✔</td>
</tr>
<tr class="odd">
<td>tensorflow</td>
<td align="center">C++</td>
<td align="center">Python</td>
<td align="center">CPU/GPU/Mobile</td>
<td align="center">✔</td>
<td align="center">✖</td>
<td align="center">✔</td>
<td align="center">✔</td>
</tr>
<tr class="even">
<td>Torch</td>
<td align="center">Lua</td>
<td align="center">-</td>
<td align="center">CPU/GPU/FPGA</td>
<td align="center">✔</td>
<td align="center">✔</td>
<td align="center">✖</td>
<td align="center">✔</td>
</tr>
<tr class="odd">
<td>theano</td>
<td align="center">Python</td>
<td align="center">-</td>
<td align="center">CPU/GPU</td>
<td align="center">✖</td>
<td align="center">✖</td>
<td align="center">✔</td>
<td align="center">✔</td>
</tr>
</tbody>
</table>
<ul>
<li>效率
<div align="center">
<img src="https://github.com/hjchen2/personal/blob/master/blog/caffe框架学习/caffe-002.png?raw=true" width="420">
</div></li>
</ul>
<h2 id="caffe代码组织结构">caffe代码组织结构</h2>
<p>caffe代码结构是非常清晰的，主要包含以下文件和目录：</p>
<ul>
<li>Makefile和Makefile.config caffe支持cmake和make两种编译方式，不过大部分人只需要用make编译就可以了。Makefile.config可以对一些编译选项进行配置，比如USE_MPI、CPU_ONLY、DEBUG等等。</li>
<li>include 在caffe中除了proto文件生成的头文件外，所有的c++头文件都放在include目录中。</li>
<li>src src与include的目录结构基本上相同，include目录中的文件基本上都能在src目录中找到对应的实现文件。</li>
<li>tools tools目录下是caffe提供给用户直接使用的接口，比如caffe.cpp用于模型训练、评估以及统计耗时，另外也提供一些数据集转换、计算均值等工具</li>
<li>examples 提供一些训练相关的脚本和网络配置，比如数据预处理脚本、不同的网络配置文件以及训练脚本</li>
<li>models 提供一些模型的网络配置文件，以及训练好的模型，用户可以直接用训练好的模型进行fine-tune或者分类</li>
<li>matlab/python 提供matlab和python的接口</li>
</ul>
<h2 id="caffe网络的组织方式">caffe网络的组织方式</h2>
<p>从LeNet开始，CNN就开始有了一个标准的分层结构——堆叠卷积层，卷积层可能后接一些normalization和pooling层，网络最后接一个或多个全连接层。由于梯度下降算法非常适合逐层计算，因此当时很多的通用框架都将网络（Net）抽象为多个数据处理层（Layer）组成的有向图，并支持灵活地定义网络结构。caffe将神经网络的训练问题分解为四个方面：数据、计算、流动控制以及问题求解，分别对应caffe中的Blob、Layer、Net和Solver。网络中流动的数据以及参数都用Blob来表示，Layer负责前向输出和后向梯度的计算，Net负责控制Layer计算的顺序，Solver是一个求解器的角色，根据Net的梯度对网络参数进行更新。</p>
<p><img src="https://github.com/hjchen2/personal/blob/master/blog/caffe框架学习/caffe-003.png?raw=true" width="800"></p>
<p>[待补充]</p>
<h2 id="caffe中的blob及同步策略">caffe中的Blob及同步策略</h2>
<p>Blob是caffe中存储数据的基本结构，可以简单理解为一个4维的数组，数据组织格式为（N,C,H,W）。在caffe中上下层流动的数据和每层的权重参数都是用Blob来保存的，为了便于使用，Blob具有一些特性：</p>
<ul>
<li>Blob的内存是懒分配的（lazily allocate），只有在真正使用的时候才会分配内存</li>
<li>Blob会在CPU和GPU上各自分配一块相同大小的内存，便于在CPU和GPU之间进行切换</li>
<li>用户不需要关心CPU和GPU数据的同步，Blob会根据需要自动同步</li>
</ul>
<p>下面是Blob的成员变量，data_是Blob存储的数据，diff_保存的是数据的梯度，shape_data_和shape_保存的都是当前数组的形状，count_是当前数据的大小，capacity_是申请的内存的大小，避免每次Reshape都要释放并重新申请内存。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// include/caffe/blob.hpp</span></div><div class="line"><span class="built_in">shared_ptr</span>&lt;SyncedMemory&gt; data_;</div><div class="line"><span class="built_in">shared_ptr</span>&lt;SyncedMemory&gt; diff_;</div><div class="line"><span class="built_in">shared_ptr</span>&lt;SyncedMemory&gt; shape_data_;</div><div class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; shape_;</div><div class="line"><span class="keyword">int</span> count_;</div><div class="line"><span class="keyword">int</span> capacity_;</div></pre></td></tr></table></figure>
<p>下面主要说一下Blob的自动同步策略。首先看一下SyncedMemory的成员变量：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// include/caffe/syncedmem.hpp</span></div><div class="line"><span class="keyword">void</span>* cpu_ptr_; <span class="comment">// CPU内存数据</span></div><div class="line"><span class="keyword">void</span>* gpu_ptr_; <span class="comment">// GPU显存数据</span></div><div class="line"><span class="keyword">size_t</span> size_;   <span class="comment">// 数据大小</span></div><div class="line">SyncedHead head_;  <span class="comment">// 同步标志</span></div><div class="line"><span class="keyword">bool</span> own_cpu_data_; </div><div class="line"><span class="keyword">bool</span> cpu_malloc_use_cuda_;</div><div class="line"><span class="keyword">bool</span> own_gpu_data_;</div><div class="line"><span class="keyword">int</span> gpu_device_; <span class="comment">// GPU设备号</span></div></pre></td></tr></table></figure>
<p>head_的取值范围为UNINITIALIZED, HEAD_AT_CPU, HEAD_AT_GPU, SYNCED。初始化时head_值为UNINITIALIZED，当调用Blob的取值函数时都会调用一次SyncedMemory的to_cpu或者to_gpu进行数据的同步，同步策略为：<br>
1、取cpu数据时，会调用to_cpu函数，如果heda_为HEAD_AT_GPU，则需要将GPU的数据同步至CPU，否则不需要同步<br>
2、取gpu数据时，会调用to_gpu函数，如果heda_为HEAD_AT_CPU，则需要将CPU的数据同步至GPU，否则不需要同步</p>
<p>head_标志的赋值：<br>
1、每次调用SyncedMemory的mutable_cpu_data时，head_都会被置为HEAD_AT_CPU<br>
2、每次调用SyncedMemory的mutable_gpu_data时，head_都会被置为HEAD_AT_GPU<br>
3、每次同步之后heda_会被置为SYNCED。</p>
<p>因此Blob通过判断每次修改的位置来自行决定是否需要对不同设备间的两份数据进行同步，使用时就像只有一份数据一样，非常方便。</p>
<h2 id="caffe中的layer">caffe中的Layer</h2>
<p>layer是caffe模型的主要组成部分和基本的计算单元，与很多框架中的operator对应，一个典型的layer在forward时从下层连接获取输入，经过计算后输出到上层，backward时又从上层连接获取误差，计算本层梯度和误差后，将误差传递到下层连接。因此基类Layer实现了三个基本函数setup、forward和backward。</p>
<ul>
<li>setup：根据下层连接和配置参数完成本层参数的初始化，以及输出blobs的初始化</li>
<li>forward：前向计算过程，并计算本层的loss</li>
<li>backward：后向计算过程，并将本层误差传递到下层</li>
</ul>
<p>forward和backward里面都会对CPU和GPU进行分支，如果是CPU模式，则真正参与计算的是forward_cpu和backward_cpu，如果是GPU模式，则参与计算的是forward_gpu和backward_gpu，并且在基类中forward_gpu和backward_gpu分别调用的是forward_cpu和backward_cpu，当然用户在定义新的layer时可以自行实现forward_gpu和backward_gpu。</p>
<p>基类Layer的成员变量：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// include/caffe/layer.hpp</span></div><div class="line"><span class="comment">/** The protobuf that stores the layer parameters */</span></div><div class="line">LayerParameter layer_param_;</div><div class="line"><span class="comment">/** The phase: TRAIN or TEST */</span></div><div class="line">Phase phase_;</div><div class="line"><span class="comment">/** The vector that stores the learnable parameters as a set of blobs. */</span></div><div class="line"><span class="built_in">vector</span>&lt;<span class="built_in">shared_ptr</span>&lt;Blob&lt;Dtype&gt; &gt; &gt; blobs_;</div><div class="line"><span class="comment">/** Vector indicating whether to compute the diff of each param blob. */</span></div><div class="line"><span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt; param_propagate_down_;</div><div class="line"><span class="comment">/** The vector that indicates whether each top blob has a non-zero weight in</span></div><div class="line"> *  the objective function. */</div><div class="line"><span class="built_in">vector</span>&lt;Dtype&gt; loss_;</div></pre></td></tr></table></figure>
<p>layer_param_是从protobuf文件中反序列化得到的，存放的是layer的配置参数 phase_指示是训练还是测试 blobs_是本层的参数，比如权重和偏置 param_propagate_down_为每一个参数设定是否需要计算梯度 loss_是本层的损失值，loss层每个输出blob都有一个损失值，非loss层损失为0</p>
<p>由基类Layer直接或间接派生出各种layer，比如卷积(convolution)、全连接(fully connected或者inner product)、dropout、pooling、relu、softmaxWithLoss等等，每一个派生layer都会强制实现forward_cpu和backward_cpu。早期的caffe将layer分成5类，</p>
<ul>
<li>dataLayer类： 各类数据读取的接口</li>
<li>neuronLayer类： 各种激活函数、dropout</li>
<li>visionLayer类： 卷积层、采样层等2D图像相关的运算</li>
<li>commonLayer类：全连接层和其他运算</li>
<li>lossLayer类：实现各种代价函数</li>
</ul>
<p>不过目前最新版本的caffe已经取消了visionLayer和commonLayer的分类。此外由于caffe使用了cuDNN运算加速库，因此部分layer有caffe和cuDNN两种实现，使用时可以通过protobuf文件配置需要使用的engine。</p>
<p>为了保持框架的可扩展性，大多数框架在layer或者operator的实现中使用了工厂模式，使用统一的工厂类来对不同的layer或operator进行实例化。下面是caffe使用工厂模式的代码实现，</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// include/caffe/layer_factory.hpp</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">class</span> LayerRegistry &#123;</div><div class="line"><span class="keyword">public</span>:</div><div class="line">  <span class="keyword">typedef</span> <span class="built_in">shared_ptr</span>&lt;Layer&lt;Dtype&gt; &gt; (*Creator)(<span class="keyword">const</span> LayerParameter&amp;);</div><div class="line">  <span class="keyword">typedef</span> <span class="built_in">std</span>::<span class="built_in">map</span>&lt;<span class="built_in">string</span>, Creator&gt; CreatorRegistry;</div><div class="line">  <span class="function"><span class="keyword">static</span> CreatorRegistry&amp; <span class="title">Registry</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">static</span> CreatorRegistry* g_registry_ = <span class="keyword">new</span> CreatorRegistry();</div><div class="line">    <span class="keyword">return</span> *g_registry_;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">// Adds a creator.</span></div><div class="line">  <span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">AddCreator</span><span class="params">(<span class="keyword">const</span> <span class="built_in">string</span>&amp; type, Creator creator)</span> </span>&#123;</div><div class="line">    CreatorRegistry&amp; registry = Registry();</div><div class="line">    CHECK_EQ(registry.count(type), <span class="number">0</span>)</div><div class="line">        &lt;&lt; <span class="string">"Layer type "</span> &lt;&lt; type &lt;&lt; <span class="string">" already registered."</span>;</div><div class="line">    registry[type] = creator;</div><div class="line">  &#125;</div><div class="line">...</div><div class="line">&#125;;</div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">class</span> LayerRegisterer &#123;</div><div class="line"> <span class="keyword">public</span>:</div><div class="line">  LayerRegisterer(<span class="keyword">const</span> <span class="built_in">string</span>&amp; type,</div><div class="line">                  <span class="built_in">shared_ptr</span>&lt;Layer&lt;Dtype&gt; &gt; (*creator)(<span class="keyword">const</span> LayerParameter&amp;)) &#123;</div><div class="line">    <span class="comment">// LOG(INFO) &lt;&lt; "Registering layer type: " &lt;&lt; type;</span></div><div class="line">    LayerRegistry&lt;Dtype&gt;::AddCreator(type, creator);</div><div class="line">  &#125;</div><div class="line">&#125;;</div><div class="line"></div><div class="line"><span class="meta">#<span class="meta-keyword">define</span> REGISTER_LAYER_CREATOR(type, creator)                                  \</span></div><div class="line">  static LayerRegisterer<span class="meta-string">&lt;float&gt; g_creator_f_##type(#type, creator&lt;float&gt;);     \</span></div><div class="line">  <span class="keyword">static</span> LayerRegisterer&lt;<span class="keyword">double</span>&gt; g_creator_d_#<span class="meta">#type(#type, creator<span class="meta-string">&lt;double&gt;)    \</span></span></div></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// src/caffe/layer_factory.cpp</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="built_in">shared_ptr</span>&lt;Layer&lt;Dtype&gt; &gt; GetSigmoidLayer(<span class="keyword">const</span> LayerParameter&amp; param) &#123;</div><div class="line">  SigmoidParameter_Engine engine = param.sigmoid_param().engine();</div><div class="line">  <span class="keyword">if</span> (engine == SigmoidParameter_Engine_DEFAULT) &#123;</div><div class="line">    engine = SigmoidParameter_Engine_CAFFE;</div><div class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> USE_CUDNN</span></div><div class="line">    engine = SigmoidParameter_Engine_CUDNN;</div><div class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></div><div class="line">  &#125;</div><div class="line">  <span class="keyword">if</span> (engine == SigmoidParameter_Engine_CAFFE) &#123;</div><div class="line">    <span class="keyword">return</span> <span class="built_in">shared_ptr</span>&lt;Layer&lt;Dtype&gt; &gt;(<span class="keyword">new</span> SigmoidLayer&lt;Dtype&gt;(param));</div><div class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> USE_CUDNN</span></div><div class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (engine == SigmoidParameter_Engine_CUDNN) &#123;</div><div class="line">    <span class="keyword">return</span> <span class="built_in">shared_ptr</span>&lt;Layer&lt;Dtype&gt; &gt;(<span class="keyword">new</span> CuDNNSigmoidLayer&lt;Dtype&gt;(param));</div><div class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    LOG(FATAL) &lt;&lt; <span class="string">"Layer "</span> &lt;&lt; param.name() &lt;&lt; <span class="string">" has unknown engine."</span>;</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">REGISTER_LAYER_CREATOR(Sigmoid, GetSigmoidLayer);</div></pre></td></tr></table></figure>
<h2 id="caffe中的net">caffe中的Net</h2>
<p>Net是由Layer组成的有向图，表示整个神经网络的拓扑结构，与很多框架中的graph对应，一般用一个protobuf文件来定义。而且Layer作为有向图中的一个组件，是无法感知自己的上层和下层连接的，需要Net将数据feed给Layer，这样数据在有向图中才能真正流动起来。因此Net至少需要提供构建一个有向图和feed数据流两种功能。</p>
<ul>
<li>构建一个有向图：void Init(const NetParameter&amp; in_param)</li>
<li>feed数据流： const vector<blob<dtype\>*&gt;&amp; Forward(Dtype* loss)和void Backward()</blob<dtype\></li>
</ul>
<p>在构建有向图时，caffe首先会对不符合规则的layer进行过滤，比如对于test net，则会把只用于train的layer过滤掉。对于有向图中可能存在分支的情况，caffe会自动插入split层，将原输入blob复制多份，分别输入不同的分支，比如：LeNet网络中的数据层的label需要输入到accuracy层和loss层，那么需要在数据层再插入一层，如下图所示。</p>
<div align="center">
<img src="https://github.com/hjchen2/personal/blob/master/blog/caffe框架学习/caffe-004.jpg?raw=true" width="600">
</div>
<p>Net会根据网络结构逐层创建layer，并指定输入输出blobs，以及是否需要backward。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// src/caffe/net.cpp:Init</span></div><div class="line">...</div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> layer_id = <span class="number">0</span>; layer_id &lt; param.layer_size(); ++layer_id) &#123;</div><div class="line">    ...</div><div class="line">    layers_.push_back(LayerRegistry&lt;Dtype&gt;::CreateLayer(layer_param));</div><div class="line">    ...</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> bottom_id = <span class="number">0</span>; bottom_id &lt; layer_param.bottom_size(); ++bottom_id) &#123;</div><div class="line">      <span class="keyword">const</span> <span class="keyword">int</span> blob_id = AppendBottom(param, layer_id, bottom_id,</div><div class="line">                                       &amp;available_blobs, &amp;blob_name_to_idx);</div><div class="line">      <span class="comment">// If a blob needs backward, this layer should provide it.</span></div><div class="line">      need_backward |= blob_need_backward_[blob_id];</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">int</span> num_top = layer_param.top_size();</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> top_id = <span class="number">0</span>; top_id &lt; num_top; ++top_id) &#123;</div><div class="line">      AppendTop(param, layer_id, top_id, &amp;available_blobs, &amp;blob_name_to_idx);</div><div class="line">      <span class="comment">// Collect Input layer tops as Net inputs.</span></div><div class="line">      <span class="keyword">if</span> (layer_param.type() == <span class="string">"Input"</span>) &#123;</div><div class="line">        <span class="keyword">const</span> <span class="keyword">int</span> blob_id = blobs_.size() - <span class="number">1</span>;</div><div class="line">        net_input_blob_indices_.push_back(blob_id);</div><div class="line">        net_input_blobs_.push_back(blobs_[blob_id].get());</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">    ...</div><div class="line">    layers_[layer_id]-&gt;SetUp(bottom_vecs_[layer_id], top_vecs_[layer_id]);</div><div class="line">    ...</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在训练时，train net会首先初始化，test net之后初始化，每次test时会调用ShareTrainedLayersWith共享train net的参数，这样做可以节省显存并且避免不必要的数据拷贝。</p>
<p>需要注意的是，在protobuf文件中声明网络结构时，必须依照从下到上的顺序一层一层定义网络参数，而且test net和train net对应层的name最好一致(虽然不一致可能不会导致程序报错），因为test net与train net是根据匹配name进行参数共享的，如果name不一致则会导致无法进行参数共享，增加显存消耗的同时还会导致test结果不正确。</p>
<p>当有向图构建完成后，我们只需要调用Forward和Backward，数据就能流经整个网络，得到每层的输出、loss和每个参数的梯度。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// src/caffe/net.cpp</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">Dtype Net&lt;Dtype&gt;::ForwardFromTo(<span class="keyword">int</span> start, <span class="keyword">int</span> end) &#123;</div><div class="line">  CHECK_GE(start, <span class="number">0</span>);</div><div class="line">  CHECK_LT(end, layers_.size());</div><div class="line">  Dtype loss = <span class="number">0</span>;</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = start; i &lt;= end; ++i) &#123;</div><div class="line">    <span class="comment">// LOG(ERROR) &lt;&lt; "Forwarding " &lt;&lt; layer_names_[i];</span></div><div class="line">    Dtype layer_loss = layers_[i]-&gt;Forward(bottom_vecs_[i], top_vecs_[i]);</div><div class="line">    loss += layer_loss;</div><div class="line">    <span class="keyword">if</span> (debug_info_) &#123; ForwardDebugInfo(i); &#125;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">return</span> loss;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> Net&lt;Dtype&gt;::BackwardFromTo(<span class="keyword">int</span> start, <span class="keyword">int</span> end) &#123;</div><div class="line">  CHECK_GE(end, <span class="number">0</span>);</div><div class="line">  CHECK_LT(start, layers_.size());</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = start; i &gt;= end; --i) &#123;</div><div class="line">    <span class="keyword">if</span> (layer_need_backward_[i]) &#123;</div><div class="line">      layers_[i]-&gt;Backward(</div><div class="line">          top_vecs_[i], bottom_need_backward_[i], bottom_vecs_[i]);</div><div class="line">      <span class="keyword">if</span> (debug_info_) &#123; BackwardDebugInfo(i); &#125;</div><div class="line">    &#125; </div><div class="line">  &#125;   </div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="caffe中的solver">caffe中的Solver</h2>
<p>前面讲到Net通过调用Forward和Backward可以得到每个参数的梯度，而Solver的主要作用就是根据这些梯度进行网络参数的更新。由于caffe将Net作为Solver的底层实现，因此Solver也就成了控制整个训练过程的中枢。Solver提供三个主要函数：Init、Solve、ApplyUpdate。</p>
<ul>
<li>Init：创建训练网络和测试网络，初始化一些参数</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// src/caffe/solver.cpp</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> Solver&lt;Dtype&gt;::Init(<span class="keyword">const</span> SolverParameter&amp; param) &#123;</div><div class="line">  ...</div><div class="line">  <span class="comment">// Scaffolding code</span></div><div class="line">  InitTrainNet();</div><div class="line">  <span class="keyword">if</span> (Caffe::root_solver()) &#123;</div><div class="line">    InitTestNets();</div><div class="line">    LOG(INFO) &lt;&lt; <span class="string">"Solver scaffolding done."</span>;</div><div class="line">  &#125;</div><div class="line">  iter_ = <span class="number">0</span>;</div><div class="line">  current_step_ = <span class="number">0</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<ul>
<li>Solve：调用Step进行迭代训练，每次迭代后都会调用ApplyUpdate进行参数的更新</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// src/caffe/solver.cpp</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">Dtype Solver&lt;Dtype&gt;::ForwardBackward() &#123;</div><div class="line">  ...</div><div class="line">  <span class="comment">// accumulate the loss and gradient</span></div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; param_.iter_size(); ++i) &#123;</div><div class="line">    loss += net_-&gt;ForwardBackward();\</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">return</span> loss / param_.iter_size();</div><div class="line">&#125;</div><div class="line"> </div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> Solver&lt;Dtype&gt;::Step(<span class="keyword">int</span> iters) &#123;</div><div class="line">  ...</div><div class="line">  <span class="keyword">while</span> (iter_ &lt; stop_iter) &#123;</div><div class="line">    <span class="keyword">if</span> (param_.test_interval() &amp;&amp; iter_ % param_.test_interval() == <span class="number">0</span></div><div class="line">        &amp;&amp; (iter_ &gt; <span class="number">0</span> || param_.test_initialization())</div><div class="line">        &amp;&amp; Caffe::root_solver()) &#123;</div><div class="line">      TestAll(); <span class="comment">// 进行测试</span></div><div class="line">    &#125;</div><div class="line">    ...</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; callbacks_.size(); ++i) &#123;</div><div class="line">      callbacks_[i]-&gt;on_start();</div><div class="line">    &#125;</div><div class="line">    ...</div><div class="line">    Dtype loss = ForwardBackward();</div><div class="line">    ...</div><div class="line">    UpdateSmoothedLoss(loss, start_iter, average_loss);</div><div class="line">    ...</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; callbacks_.size(); ++i) &#123;</div><div class="line">      callbacks_[i]-&gt;on_gradients_ready();</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">if</span> (!param().disabled_update()) &#123;</div><div class="line">      ApplyUpdate();</div><div class="line">    &#125;</div><div class="line">    ++iter_;</div><div class="line">    ...</div><div class="line">&#125;</div><div class="line"> </div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> Solver&lt;Dtype&gt;::Solve(<span class="keyword">const</span> <span class="keyword">char</span>* resume_file) &#123;</div><div class="line">  ...</div><div class="line">  Step(param_.max_iter() - iter_);</div><div class="line">  ...</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<ul>
<li>ApplyUpdate：调用对应的solver进行参数更新，下面是sgd solver的ApplyUpdate函数</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// src/caffe/solvers/sgd_solver.cpp</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> SGDSolver&lt;Dtype&gt;::ApplyUpdate() &#123;</div><div class="line">  ...</div><div class="line">  Dtype rate = GetLearningRate(); <span class="comment">//获取当前迭代的学习率</span></div><div class="line">  ...</div><div class="line">  ClipGradients(); <span class="comment">// 进行梯度规整</span></div><div class="line">  <span class="comment">// learnable_params存放的是网络中所有需要学习的参数blobs</span></div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> param_id = <span class="number">0</span>; param_id &lt; <span class="keyword">this</span>-&gt;net_-&gt;learnable_params().size();</div><div class="line">       ++param_id) &#123;</div><div class="line">    ApplyUpdate(param_id); <span class="comment">// 逐个更新参数</span></div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>由于梯度下降算法发展出了非常多的优化算法，目前caffe提供了六种优化算法来求解最优参数，在solver配置文件中，通过设置type类型来选择。</p>
<ul>
<li>Stochastic Gradient Descent (type: “SGD”),</li>
<li>AdaDelta (type: “AdaDelta”),</li>
<li>Adaptive Gradient (type: “AdaGrad”),</li>
<li>Adam (type: “Adam”),</li>
<li>Nesterov’s Accelerated Gradient (type: “Nesterov”)</li>
<li>RMSprop (type: “RMSProp”)</li>
</ul>
<h2 id="caffe断点保存和恢复">caffe断点保存和恢复</h2>
<p>由于训练过程往往非常耗时，为了能够在突发情况后快速恢复训练，caffe提供了断点保存和恢复的功能，在solver的配置文件中可以配置保存的频率及保存时文件名的前缀，一个比较完整的solver配置文件如下：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// solver.prototxt</span></div><div class="line">net: <span class="string">"./train_val.prototxt"</span>  <span class="comment">// 定义net的protobuf文件</span></div><div class="line">test_iter: <span class="number">100</span>   <span class="comment">// 测试的迭代次数，这个需要根据测试数据的大小和测试时的batch size计算得到，test_iter = test_dataset_size / test_batch_size</span></div><div class="line">test_interval: <span class="number">1000</span>  <span class="comment">// 设置test的频率，每训练1000次迭代就测试一次</span></div><div class="line">base_lr: <span class="number">0.01</span>  <span class="comment">// 设置学习率</span></div><div class="line">lr_policy: <span class="string">"step"</span>  <span class="comment">// 设置学习率衰减策略</span></div><div class="line">gamma: <span class="number">0.1</span>  <span class="comment">// step衰减因子，</span></div><div class="line">stepsize: <span class="number">10000</span> <span class="comment">// 衰减的频率，每训练10000次迭代衰减一次，衰减后的学习率=当前学习率*gamma</span></div><div class="line">display: <span class="number">500</span>  <span class="comment">// 训练log打印频率</span></div><div class="line">max_iter: <span class="number">45000</span>  <span class="comment">// 设置最大训练多少次迭代</span></div><div class="line">type: <span class="string">"SGD"</span>  <span class="comment">// 设置solver类型 </span></div><div class="line">momentum: <span class="number">0.9</span>  <span class="comment">// 设置SGD中的动量项</span></div><div class="line">weight_decay: <span class="number">0.0005</span>  <span class="comment">// 设置正则系数</span></div><div class="line">snapshot: <span class="number">1000</span>  <span class="comment">// 设置模型保存频率</span></div><div class="line">snapshot_prefix: <span class="string">"../output/caffe_alexnet_train"</span>  <span class="comment">// 设置模型保存时文件名前缀</span></div><div class="line">solver_mode: CPU  <span class="comment">// 设置训练模式，CPU还是GPU</span></div></pre></td></tr></table></figure>
<p>当然还有一些其他的参数，比如正则化类型和模型保存文件格式等，都会使用在proto文件中定义的默认值，具体查看src/caffe/proto/caffe.proto文件中的SolverParameter。</p>
<p>为了实现断点保存和恢复，caffe在Solver中加入了Snapshot和Restore，分别进行模型保存和模型恢复，相应地，在Net中也加入了ToProto/ToHDF5和CopyTrainedLayersFromBinaryProto/CopyTrainedLayersFromHDF5。Solver调用Step进行训练的时候，每次参数更新结束都会判断是否需要保存模型。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// src/caffe/solver.cpp:Step</span></div><div class="line"><span class="keyword">if</span> ((param_.snapshot()</div><div class="line">     &amp;&amp; iter_ % param_.snapshot() == <span class="number">0</span></div><div class="line">     &amp;&amp; Caffe::root_solver()) ||</div><div class="line">     (request == SolverAction::SNAPSHOT)) &#123;</div><div class="line">  Snapshot();</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>Solver中Snapshot对模型参数和训练状态进行保存，模型参数提供两种保存格式——binary protobuf和hdf5。如果是protobuf格式，则会调用Net的ToProto，否则调用ToHDF5。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// src/caffe/net.cpp</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> Net&lt;Dtype&gt;::ToProto(NetParameter* param, <span class="keyword">bool</span> write_diff) <span class="keyword">const</span> &#123;</div><div class="line">  param-&gt;Clear();</div><div class="line">  param-&gt;set_name(name_);</div><div class="line">  <span class="comment">// Add bottom and top</span></div><div class="line">  DLOG(INFO) &lt;&lt; <span class="string">"Serializing "</span> &lt;&lt; layers_.size() &lt;&lt; <span class="string">" layers"</span>;</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; layers_.size(); ++i) &#123;</div><div class="line">    LayerParameter* layer_param = param-&gt;add_layer();</div><div class="line">    layers_[i]-&gt;ToProto(layer_param, write_diff);</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>Solver在开始训练时会尝试调用Restore进行断点恢复，根据文件名后缀判断文件格式，并选择RestoreSolverStateFromHDF5还是RestoreSolverStateFromBinaryProto。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// src/caffe/solver.cpp:Solve</span></div><div class="line"><span class="keyword">if</span> (resume_file) &#123;</div><div class="line">  LOG(INFO) &lt;&lt; <span class="string">"Restoring previous solver status from "</span> &lt;&lt; resume_file;</div><div class="line">  Restore(resume_file);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="in-place计算">in-place计算</h2>
<p>为了节约显存，caffe支持原址计算，就是输入与输出都是同一个blob。如果前一层的输出和本层的输入都与后向计算时无关，而且本层的输入和输出blob大小相同，就可以使用in-place计算，比如卷积层后面的Sigmoid、Relu等都可以用同址计算，而BatchNorm层也支持in-place计算，是因为BatchNorm在实现时会将输入数据进行备份。使用同址计算只要在protobuf文件中指定该层的top和bottom是同名的就可以了，比如：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">layer &#123;</div><div class="line">        bottom: <span class="string">"conv1"</span></div><div class="line">        top: <span class="string">"conv1"</span></div><div class="line">        name: <span class="string">"conv1_relu"</span></div><div class="line">        type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="参数初始化方法">参数初始化方法</h2>
<p>由于神经网络的目标函数往往是非凸的，参数初始化会对最终的收敛结果造成非常大的影响。为了满足不同的参数初始化需求，caffe提供了多种初始化方法，并且在net的配置文件中可以为每个参数选择一个初始化方法。比如下面的weight_filler和bias_filler：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">layer &#123;</div><div class="line">        bottom: <span class="string">"data"</span></div><div class="line">        top: <span class="string">"conv1"</span></div><div class="line">        name: <span class="string">"conv1"</span></div><div class="line">        type: <span class="string">"Convolution"</span></div><div class="line">        convolution_param &#123;</div><div class="line">                num_output: <span class="number">64</span></div><div class="line">                kernel_size: <span class="number">7</span></div><div class="line">                pad: <span class="number">3</span></div><div class="line">                stride: <span class="number">2</span></div><div class="line">                weight_filler &#123;</div><div class="line">                  type: <span class="string">"xavier"</span></div><div class="line">                &#125;</div><div class="line">                bias_filler &#123;</div><div class="line">                  type: <span class="string">"constant"</span></div><div class="line">                  value: <span class="number">0.2</span></div><div class="line">               &#125;</div><div class="line">        &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在include/caffe/filler.hpp中caffe提供如下的初始化方法：</p>
<ul>
<li>constant：常量初始化，参数所有的值都被初始化为相同的值</li>
<li>uniform：均匀初始化，参数的值按照指定区间均匀分布随机初始化</li>
<li>gaussian：高斯初始化，参数的值按照指定均值和方差的正态分布随机初始化</li>
<li>positive unitball</li>
<li>xavier：本质上也是一种指定区间均匀分布的随机初始化方式，只是区间是通过参数大小计算得到</li>
<li>msra：与xavier类似，不过使用的是指定均值和方差的正态分布随机初始化方式</li>
<li>bilinear</li>
</ul>
<h2 id="多卡并行策略">多卡并行策略</h2>
<p>为了提高效率，caffe支持单机多GPU并行训练，目前采用的是数据并行方式，暂不支持模型并行，为此caffe增加了一个P2PSync类，下面主要介绍一下P2PSync如何实现多卡并行的。</p>
<p>P2PSync封装了一个Solver负责训练，每张GPU都会对应一个P2PSync，并且P2PSync之间具有主从关系，它们之间构成一个二叉树的结构。在前向计算时，主P2PSync需要将模型分发给从P2PSync，而在后向传导时，从P2PSync就需要把梯度传给主P2PSync，主P2PSync会在聚合从P2PSync的梯度后传给更上一层的主P2PSync。在二叉树结构中，根节点P2PSync的Solver被叫做root solver，其他solver叫做worker solver，只有root solver才能进行参数更新，worker solver只是将梯度聚合并传递给root solver。</p>
<div align="center">
<img src="https://github.com/hjchen2/personal/blob/master/blog/caffe框架学习/caffe-005.jpg?raw=true" width="720">
</div>
<p>在P2PSync中主要的函数就InternalThreadEntry、on_start和on_gradients_ready。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// src/caffe/parallel.cpp</span></div><div class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> P2PSync&lt;Dtype&gt;::InternalThreadEntry() &#123;</div><div class="line">...</div><div class="line"> solver_-&gt;Step(solver_-&gt;param().max_iter() - initial_iter_);</div><div class="line"> &#125;</div></pre></td></tr></table></figure>
<p>InternalThreadEntry是一个线程函数，Solver调用Step进行训练，在Step中每次前向计算前都会回调on_start获取最新模型，而在后向计算结束后又会回调on_gradients_ready传递梯度。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// src/caffe/solver.cpp</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> Solver&lt;Dtype&gt;::Step(<span class="keyword">int</span> iters) &#123;</div><div class="line"> ...</div><div class="line"> <span class="keyword">while</span> (iter_ &lt; stop_iter) &#123;</div><div class="line"> <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; callbacks_.size(); ++i) &#123;</div><div class="line"> callbacks_[i]-&gt;on_start(); <span class="comment">// 回调P2PSync中的on_start，从主P2PSync获取新模型</span></div><div class="line"> &#125;</div><div class="line"> ...</div><div class="line"> Dtype loss = Forward_backward();</div><div class="line"> ...</div><div class="line"> <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; callbacks_.size(); ++i) &#123;</div><div class="line"> callbacks_[i]-&gt;on_gradients_ready(); <span class="comment">// 回调P2PSync中的on_gradients_ready，依次聚合从P2PSync和自身的梯度，并将梯度发送给主P2PSync</span></div><div class="line"> &#125;</div><div class="line"> <span class="keyword">if</span> (!param().disabled_update()) &#123;</div><div class="line"> ApplyUpdate(); <span class="comment">// 这里只有root solver才会进行参数更新</span></div><div class="line"> &#125;</div><div class="line"> ...</div><div class="line"></div><div class="line"> &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> P2PSync&lt;Dtype&gt;::on_start() &#123;</div><div class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> CPU_ONLY</span></div><div class="line"> ...</div><div class="line"> <span class="comment">// Wait for update from parent</span></div><div class="line"> <span class="keyword">if</span> (parent_) &#123;</div><div class="line">   <span class="comment">/* 除了root solver，其他worker solver都有对应的parent</span></div><div class="line">      程序执行到这里时会阻塞，当主P2PSync将自身入队后就会通知从P2PSync，pop就能返回</div><div class="line">   */</div><div class="line">   P2PSync&lt;Dtype&gt; *parent = queue_.pop(); <span class="comment">// 等待主P2PSync入队</span></div><div class="line">   CHECK(parent == parent_);</div><div class="line"> &#125;</div><div class="line"> <span class="comment">// Update children</span></div><div class="line"> <span class="keyword">for</span> (<span class="keyword">int</span> i = children_.size() - <span class="number">1</span>; i &gt;= <span class="number">0</span>; i--) &#123;</div><div class="line">   Dtype* src = data_;</div><div class="line">   Dtype* dst = children_[i]-&gt;data_;</div><div class="line">   ...</div><div class="line">   <span class="comment">// 主P2PSync将模型直接拷贝给从P2PSync</span></div><div class="line">   CUDA_CHECK(cudaMemcpyAsync(dst, src, size_ * <span class="keyword">sizeof</span>(Dtype),</div><div class="line">   cudaMemcpyDeviceToDevice, cudaStreamDefault));</div><div class="line">   CUDA_CHECK(cudaStreamSynchronize(cudaStreamDefault));</div><div class="line">   <span class="comment">// 主P2PSync将自身入队，并通知从P2PSync</span></div><div class="line">   children_[i]-&gt;queue_.push(<span class="keyword">this</span>);</div><div class="line"> &#125;</div><div class="line"> <span class="meta">#<span class="meta-keyword">endif</span></span></div><div class="line"> &#125;</div></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> P2PSync&lt;Dtype&gt;::on_gradients_ready() &#123;</div><div class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> CPU_ONLY</span></div><div class="line">  ...</div><div class="line">  <span class="comment">// Sum children gradients as they appear in the queue</span></div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; children_.size(); ++i) &#123;</div><div class="line">    P2PSync&lt;Dtype&gt; *child = queue_.pop(); <span class="comment">// 等待从P2PSync入队</span></div><div class="line">    <span class="comment">// 由于parent_grads_是在主P2PSync设备上开辟的一块缓冲区，因此child-&gt;parent_grads_其实就是当前设备上的缓冲区</span></div><div class="line">    Dtype* src = child-&gt;parent_grads_; <span class="comment">// 获取从P2PSync的梯度</span></div><div class="line">    Dtype* dst = diff_;</div><div class="line">    <span class="comment">// 合并从P2PSync的梯度</span></div><div class="line">    caffe_gpu_add(size_, src, dst, dst);</div><div class="line">  &#125;</div><div class="line">  ...</div><div class="line">  <span class="comment">// Send gradients to parent</span></div><div class="line">  <span class="keyword">if</span> (parent_) &#123;</div><div class="line">    Dtype* src = diff_;</div><div class="line">    Dtype* dst = parent_grads_; </div><div class="line">    <span class="comment">// 从P2PSync将梯度复制到主P2PSync的缓冲区</span></div><div class="line">    CUDA_CHECK(cudaMemcpyAsync(dst, src, size_ * <span class="keyword">sizeof</span>(Dtype),  <span class="comment">//</span></div><div class="line">        cudaMemcpyDeviceToDevice, cudaStreamDefault));</div><div class="line">    CUDA_CHECK(cudaStreamSynchronize(cudaStreamDefault));</div><div class="line">    <span class="comment">// 自身入队，通知主P2PSync</span></div><div class="line">    parent_-&gt;queue_.push(<span class="keyword">this</span>);</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    <span class="comment">// Loss functions divide gradients by the batch size, so to compensate</span></div><div class="line">    <span class="comment">// for split batch, the root solver divides by number of solvers.</span></div><div class="line">    caffe_gpu_scal(size_, Dtype(<span class="number">1.0</span> / Caffe::solver_count()), diff_);</div><div class="line">  &#125;</div><div class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></div></pre></td></tr></table></figure>
<h2 id="intel-caffe多机并行策略">intel caffe多机并行策略</h2>
<p>单机多卡的训练方式已经足够解决目前大部分模型训练的需求了，但随着数据量越来越大、模型越来越复杂，分布式异构计算成为行业通行的解决方案。BVLC caffe是不支持分布式训练的，intel有两个部门将caffe进行了再次开发以支持分布式和最新的Intel MKL-DNN，分别为intel caffe和caffe multinode。目前BML API已经支持intel caffe的模型训练、评估和预测了。</p>
<p>intel caffe采用的是数据并行的方式，但不同于目前主流的centralized parameter server通信模型，intel caffe借鉴了单机多卡的策略，采用的是一种all-reduce的binary tree模型，也就是将节点按照二叉树组织起来，每个父节点负责1-2个子节点和自己父节点的通信，相比一个中心的PS需要同时与其他多个节点通信的方式，这种binary tree方式将一部分PS的计算平均到了每个节点上，而且相同level的父节点之间可以并行，增加了梯度合并的并行度。</p>
<p>[待图]</p>
<p>为了更好地掩盖通信开销，子节点不需要等到整个模型的梯度都计算完才发送，而是每个layer计算完梯度后就会立即发送给父节点，父节点收到所有子节点的梯度后将本层的梯度合并后也可以立即发送给上一层的父节点。每个layer的参数会按照buffer的大小分成多个part，每个part都会异步地进行发送，当进行下一次迭代时，除了根节点的所有节点都会被阻塞，等待根节点将最终的梯度进行合并，并更新模型后发送给子节点。</p>
<p>除了分层通信外，intel caffe也支持梯度量化压缩，可以将全精浮点数编码成指定字节数的数值，减少节点间通信量。</p>
<p>intel caffe为了支持多种协议的通信，使用了boost的asio::io_service接口，底层实现支持MPI、TCP和UDP，不过目前只实现了MPI接口。</p>
<p>训练时交叉验证是在单节点(准确来说是根节点)上进行的，但每个节点上都需要存在验证集文件，这是因为即使不进行test，其他节点也会初始化test网络。</p>
<h2 id="实战">实战</h2>
<h2 id="参考">参考</h2>
<p>贾扬清自述http://www.yangfenzi.com/keji/59535.html<br>
caffe官网http://caffe.berkeleyvision.org<br>
http://ucb-icsi-vision-group.github.io/caffe-paper/caffe.pdf<br>
https://www.zhihu.com/question/27982282<br>
http://blog.csdn.net/myarrow/article/details/52064608</p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/caffe/" rel="tag"># caffe</a>
          
            <a href="/tags/deep-learning/" rel="tag"># deep learning</a>
          
            <a href="/tags/framework/" rel="tag"># framework</a>
          
        </div>
      

      
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/03/22/Mac上搭建基于Github的Hexo博客/" rel="prev" title="Mac上搭建基于Github的Hexo博客 — Testing">
                Mac上搭建基于Github的Hexo博客 — Testing <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="hypercomments_widget"></div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="http://ww3.sinaimg.cn/mw690/7ee54514jw8f3qevr3d0oj20hs0hsgme.jpg"
               alt="Dou Jiang" />
          <p class="site-author-name" itemprop="name">Dou Jiang</p>
           
              <p class="site-description motion-element" itemprop="description">凌绝顶，众山小</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">8</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">5</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">13</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/hjchen2" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/p/1005052128954644" target="_blank" title="微博">
                  
                    <i class="fa fa-fw fa-weibo"></i>
                  
                  微博
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#caffe学习总结"><span class="nav-number">1.</span> <span class="nav-text">caffe学习总结</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#caffe的由来"><span class="nav-number">1.1.</span> <span class="nav-text">caffe的由来</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#caffe与其他一些主流框架的比较"><span class="nav-number">1.2.</span> <span class="nav-text">caffe与其他一些主流框架的比较</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#caffe代码组织结构"><span class="nav-number">1.3.</span> <span class="nav-text">caffe代码组织结构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#caffe网络的组织方式"><span class="nav-number">1.4.</span> <span class="nav-text">caffe网络的组织方式</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#caffe中的blob及同步策略"><span class="nav-number">1.5.</span> <span class="nav-text">caffe中的Blob及同步策略</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#caffe中的layer"><span class="nav-number">1.6.</span> <span class="nav-text">caffe中的Layer</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#caffe中的net"><span class="nav-number">1.7.</span> <span class="nav-text">caffe中的Net</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#caffe中的solver"><span class="nav-number">1.8.</span> <span class="nav-text">caffe中的Solver</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#caffe断点保存和恢复"><span class="nav-number">1.9.</span> <span class="nav-text">caffe断点保存和恢复</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#in-place计算"><span class="nav-number">1.10.</span> <span class="nav-text">in-place计算</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参数初始化方法"><span class="nav-number">1.11.</span> <span class="nav-text">参数初始化方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#多卡并行策略"><span class="nav-number">1.12.</span> <span class="nav-text">多卡并行策略</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#intel-caffe多机并行策略"><span class="nav-number">1.13.</span> <span class="nav-text">intel caffe多机并行策略</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#实战"><span class="nav-number">1.14.</span> <span class="nav-text">实战</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考"><span class="nav-number">1.15.</span> <span class="nav-text">参考</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="author" itemprop="copyrightHolder">Dou Jiang</span>
</div>


<div class="powered-by">
  Powered by <a class="theme-link" href="https://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme by <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT</a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  




  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.0"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  



  




	

		<script type="text/javascript">
		_hcwp = window._hcwp || [];

		_hcwp.push({widget:"Bloggerstream", widget_id: 88618, selector:".hc-comment-count", label: "{\%COUNT%\}" });

		
		_hcwp.push({widget:"Stream", widget_id: 88618, xid: "2017/01/19/caffe学习/"});
		

		(function() {
		if("HC_LOAD_INIT" in window)return;
		HC_LOAD_INIT = true;
		var lang = "en";
                //var lang = (navigator.language || navigator.systemLanguage || navigator.userLanguage || "en").substr(0, 2).toLowerCase();
		var hcc = document.createElement("script"); hcc.type = "text/javascript"; hcc.async = true;
		hcc.src = ("https:" == document.location.protocol ? "https" : "http")+"://w.hypercomments.com/widget/hc/88618/"+lang+"/widget.js";
		var s = document.getElementsByTagName("script")[0];
		s.parentNode.insertBefore(hcc, s.nextSibling);
		})();
		</script>

	












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length == 0) {
      search_path = "search.xml";
    }
    var path = "/" + search_path;
    // monitor main search box;

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.popup').toggle();
    }
    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';
      $.ajax({
        url: path,
        dataType: "xml",
        async: true,
        success: function( xmlResponse ) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = $( "entry", xmlResponse ).map(function() {
            return {
              title: $( "title", this ).text(),
              content: $("content",this).text(),
              url: $( "url" , this).text()
            };
          }).get();
          var $input = document.getElementById(search_id);
          var $resultContent = document.getElementById(content_id);
          $input.addEventListener('input', function(){
            var matchcounts = 0;
            var str='<ul class=\"search-result-list\">';
            var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
            $resultContent.innerHTML = "";
            if (this.value.trim().length > 1) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var content_index = [];
                var data_title = data.title.trim().toLowerCase();
                var data_content = data.content.trim().replace(/<[^>]+>/g,"").toLowerCase();
                var data_url = decodeURIComponent(data.url);
                var index_title = -1;
                var index_content = -1;
                var first_occur = -1;
                // only match artiles with not empty titles and contents
                if(data_title != '') {
                  keywords.forEach(function(keyword, i) {
                    index_title = data_title.indexOf(keyword);
                    index_content = data_content.indexOf(keyword);
                    if( index_title >= 0 || index_content >= 0 ){
                      isMatch = true;
                      if (i == 0) {
                        first_occur = index_content;
                      }
                    }

                  });
                }
                // show search results
                if (isMatch) {
                  matchcounts += 1;
                  str += "<li><a href='"+ data_url +"' class='search-result-title'>"+ data_title +"</a>";
                  var content = data.content.trim().replace(/<[^>]+>/g,"");
                  if (first_occur >= 0) {
                    // cut out 100 characters
                    var start = first_occur - 20;
                    var end = first_occur + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if(start == 0){
                      end = 50;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    var match_content = content.substring(start, end);
                    // highlight all keywords
                    keywords.forEach(function(keyword){
                      var regS = new RegExp(keyword, "gi");
                      match_content = match_content.replace(regS, "<b class=\"search-keyword\">"+keyword+"</b>");
                    });

                    str += "<p class=\"search-result\">" + match_content +"...</p>"
                  }
                  str += "</li>";
                }
              })};
            str += "</ul>";
            if (matchcounts == 0) { str = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>' }
            if (keywords == "") { str = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>' }
            $resultContent.innerHTML = str;
          });
          proceedsearch();
        }
      });}

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched == false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(function(e){
      $('.popup').hide();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    });
    $('.popup').click(function(e){
      e.stopPropagation();
    });
  </script>





  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

</body>
</html>
