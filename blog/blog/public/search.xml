<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
    
    <entry>
      <title><![CDATA[阿里KunPeng框架学习]]></title>
      <url>https://hjchen2.github.io/2017/08/22/KunPeng%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</url>
      <content type="html"><![CDATA[<p>KunPeng是阿里最新公布的一个大规模机器学习框架，不仅包括了数据/模型并行、负载均衡、模型同步、稀疏表达、工业级容错等特性，而且还提供了易于使用的接口，在很多机器学习算法上都带来了非常大的性能提升。 原始论文 KunPeng: Parameter Server based Distributed Learning Systems and Its Applications in Alibaba and Ant Financial。</p>
<h2 id="introduction">Introduction</h2>
<p>主要对一些通用分布式计算框架进行比较。</p>
<p>Hadoop：只提供了一些粗粒度的操作，比如Map、Reduce和Join等。很多限制导致基于Hadoop的机器学习算法效率都非常低，这些限制包括中间结果会落盘、只能在shuffling阶段进行数据交换等。</p>
<p>Spark：使用RDD弥补了Hadoop的一些缺点，提供MLlib库，MLlib整合了很多机器学习算法，并且非常容易使用。但MLlib只支持中等规模的特征，计算和通信效率都比较低。一些公司使用第三方组件来弥补Spark的缺陷，但至今没有一个完美的方案。</p>
<p>GraphLab和GraphX：基于图的并行计算框架，允许用户进行细粒度的控制，但并不适合通用的机器学习算法，比如LR、深度学习等，并且也存在效率低的问题。</p>
<p>MPI：接口灵活高效，代码自由度比较高，比如在代码中所有进程之间可以随时通信。但使用MPI开发一个新算法的开销非常大，比如一个复杂的异步矩阵分解算法需要2000多行代码。MPI没有提供分布式ML平台通用的组件，比如分布式数据读取，内存管理和多线程并行的组件。更重要的是MPI没有提供单点失败的本地解决方案，根据他们的统计数据显示MPI作业在节点数越多时失败率越高。</p>
<p>parameter server框架：包含无状态的workers和有状态的servers，workers负责大部分的计算任务，servers负责保存和更新模型参数。servers可以定期将模型参数快照保存到一个缓存位置，一旦有节点失败，parameter server会自动从最新的checkpoint中恢复模型参数。parameter server框架只支持pserver和worker之间通信， 而pserver和pserver、worker和worker之间无法进行点对点通信，并且由于细粒度的接口导致用户编程比较复杂，因此现有的parameter server框架还存在几个问题：一是通信接口比较单一，没有MPI灵活；二是对于用户来说没有Spark易于编程使用。</p>
<p>正是由于上述框架的种种缺点，他们开发了一个产品级的分布式学习系统—KunPeng。KunPeng结合了parameter server和MPI的优点，提供鲁棒的failover机制，高效的稀疏数据通信接口和与MPI类似的通用接口，并且提供一个C++和Python的SDK，该SDK提供了一个类似单机的开发环境。KunPeng也与阿里的Apsara平台深度对接，提供ML的全工具集，包括基于SQL和MapReduce的数据预处理、预测、评估等等。</p>
<h2 id="kunpeng整体架构">KunPeng整体架构</h2>
<h3 id="apsara-cloud-platform">Apsara Cloud Platform</h3>
<p>Apsara是阿里开发的一个大规模分布式操作系统，目前已运行在跨数十个机房的十几万台服务器上。下图中天蓝色部分就是Apsara的模块，白色部分为运行在Apsara之上的各种云服务，KunPeng就属于图中白色部分，运行在Apsara上，由Apsara提供任务调度和监控、文件系统等服务。 <img src="https://coding.net/u/hjchen2/p/personal/git/raw/master/blog/KunPeng/b2b0cb8a6973ec2b4281d68c328e4a0f.png" alt="b2b0cb8a6973ec2b4281d68c328e4a0f"> 图中红色边框的任务调度模块和资源管理模块被统称为Fuxi（伏羲），Fuxi支持多种特性以保证系统的可扩展性和容错性，这些特性包括：增量资源管理协议、用户透明的失败恢复、故障点自动检测和多级黑名单机制。</p>
<h2 id="kunpeng-platform">KunPeng Platform</h2>
<p>KunPeng分为ML-Bridge和PS-Core两个子系统，ML-Bridge是KunPeng提供的高级编程模型，用户通过脚本编程的workflow可以方便地实现数据预处理、训练、预测和评估等算法，PS-Core是一个分布式键值对存储的paramter server框架。 <img src="https://coding.net/u/hjchen2/p/personal/git/raw/master/blog/KunPeng/0313b564c3646a4c4fab16574f9c4b4e.png?api=v2%20=600" alt="0313b564c3646a4c4fab16574f9c4b4e"> ML-Bridge由三个组件构成：</p>
<ul>
<li>解释器。将用户的脚本解释为系统支持的算法</li>
<li>优化器。根据运行状态的历史统计和启发式方法，分析、调试和优化作业配置</li>
<li>执行器。根据作业的配置生成Fuxi调度的配置，提供整个作业生命周期的监控，并提供用户监控UI ML-Bridge简化了用户编程，比如一个算法流程包括数据入库与预处理、训练、评估和AB测试几个流程，在KunPeng中只需要调用下图中的几行命令就可以实现。整个流程对用户来说都是透明的，用户也不需要关心算法的具体实现和作业调度过程。</li>
</ul>
<div class="figure">
<img src="https://coding.net/u/hjchen2/p/personal/git/raw/master/blog/KunPeng/ede2df215585fc86358bc9868565d1ce.png?api=v2%20=500" alt="ede2df215585fc86358bc9868565d1ce">
<p class="caption">ede2df215585fc86358bc9868565d1ce</p>
</div>
<p>PS-Core不仅支持数据并行和模型并行，同时还支持模型同步更新(BSP)、ASP和SSP，稀疏表达和容错机制。 PS-Core在传统的worker和server基础上，增加了一个用于迭代控制的coordinator。coordinator声明了数据计算和参数更新的操作，构建了整个ML workerflows的作业图，并将这些作业调度到worker和server上运行，并参与servers和workers的failover过程。coordinator在迭代结束时会与Apsara的meta对迭代状态进行同步，并且由Fuxi监控管理，因此不存在SPOF（单点失败）的问题。</p>
<h3 id="容错方案">容错方案</h3>
<p>KunPeng也给出了servers和workers的容错解决方案。对于servers，它们会异步地将参数快照保存到分布式文件系统，并且它们会在内存中对参数进行两备份，支持hot failover加速恢复过程。大多数情况下(比如接收到coordinator的恢复请求)，servers可以立刻通过内存备份的参数中恢复。即使是servers或整个任务被中断或被kill，servers也可以通过最近一次保存的参数进行恢复训练。对于stateless的workers，failover非常简单，只需要从servers上pull对应的参数。对于stateful的workers，同样提供保存快照的接口，因此对于一些workers有本地状态的算法（比如LDA），faliover也非常简单。</p>
<p>总的来说，KunPeng的failover过程是当Fuxi检测到有节点失败时，重新调度新的节点，同时给coordinator发送异步节点失败的消息，coordinator接收消息后给servers和workers发送恢复请求，对于正常的servers接收请求后会直接从内存中恢复，而对于新调度的servers会从checkpoint中恢复，对于workers需要先从servers上pull对应的参数，stateful的workers还需要从保存的checkpoint中恢复状态。</p>
<h3 id="dag调度">DAG调度</h3>
<p>这里的调度指的是coordinator对servers和workers的调度。由于coordinator节点会根据算法的workflow构建对应的作业DAG，并将DAG调度到servers和workers上进行执行。为了提高机器资源利用率和作业效率，DAG中相同深度的节点可以并行执行，比如下图中的Calculate for Block 0节点和Load Data for Block 1节点。通过DAG接口用户可以自定义IO操作、计算和通信过程，可以很方便地实现各种模型更新算法。</p>
<div class="figure">
<img src="https://coding.net/u/hjchen2/p/personal/git/raw/master/blog/KunPeng/e76cf7c13015b83ed7696b5fa7c8dac0.png?api=v2%20=600" alt="e76cf7c13015b83ed7696b5fa7c8dac0">
<p class="caption">e76cf7c13015b83ed7696b5fa7c8dac0</p>
</div>
<p>下图表示了PS-Core中bounded delay ASGD算法的C++实现，用户可以重写下面的Iterate函数实现自定义的算法。图中的mServerParam和mServerGrad对应servers上的模型参数和梯度，mWorkerParam和mWorkerGrad对应workers本地的模型参数和梯度，mSubDatasetPtr对应当前worker的数据子集。nSync为最大延迟迭代次数，nPull和nPush分别为从servers获取最新参数和将梯度发送给servers的频率。通过设置nSync、nPull和nPush可以很方便地在BSP和SSP之间切换，而去除SyncBarrier就成了ASP算法的实现。</p>
<div class="figure">
<img src="https://coding.net/u/hjchen2/p/personal/git/raw/master/blog/KunPeng/69ed0d3573fbebf558494bc4a9a14c74.png?api=v2%20=450" alt="69ed0d3573fbebf558494bc4a9a14c74">
<p class="caption">69ed0d3573fbebf558494bc4a9a14c74</p>
</div>
<h3 id="负载均衡和通信接口">负载均衡和通信接口</h3>
<p>由于集群中机器的底层硬件和运行状态存在差异，因此一个任务的执行效率很大程度上取决于运行最慢的那个机器，针对这种情况可以有多种负载均衡的方法，比如可以对负载较高的机器分配更少的数据和计算量，PS-Core也为此设计了一个Backup instance机制。当某个节点被确定为慢节点时，coordinator会把慢节点标记为“dead”节点，请求Fuxi重新调度一个新的节点作为该节点的备份节点，并将该节点的负载转移到备份节点上。这种机制通常可以带来10%-20%的效率提升。</p>
<p>KunPeng对不同稀疏度和不同数据类型的数据通信做了深度优化，并且提供workers之间点对点的通信接口，比如AllReduce，ReduceTo和Bcast，这些灵活的通信接口使得KunPeng可以拓展更多的功能，比如模型并行。</p>
<h2 id="ftrl">FTRL</h2>
<p><span class="math display">\[w_{t+1}=\mathop{\arg\min}_{w}\left(\sum_{s=1}^{t}g_{s}w+\frac{1}{2}\sum_{s=1}^{t}\delta_{s}{\Vert}w-w_{s}{\Vert}_{2}^{2}+\lambda_{1}{\Vert}w{\Vert}_{1}+\lambda_{2}{\Vert}w{\Vert}_{2}^{2}\right)\]</span> 其中<span class="math inline">\(g\)</span>为损失函数对<span class="math inline">\(w\)</span>的梯度，<span class="math inline">\(\delta_{t}=\frac{1}{\eta_{t}}-\frac{1}{\eta_{t-1}}\)</span>，因此<span class="math inline">\(\sum_{s=1}^{t}{\delta_{s}}=\frac{1}{\eta_{t}}\)</span>，<span class="math inline">\(\eta\)</span>为学习率，并且<span class="math inline">\(\eta_{t,i}=\frac{\alpha}{\beta+\sqrt{\sum_{s=1}^{s}{g_{s,i}^2}}}\)</span>，通常<span class="math inline">\(\alpha=1\)</span>，<span class="math inline">\(\beta\)</span>是与数据集和特征相关的超参数。<span class="math inline">\(\lambda_{1}\)</span>为L1系数，<span class="math inline">\(\lambda_{2}\)</span>为L2系数。 更新公式为<br>
<span class="math display">\[w_{t+1}=\begin{cases}0&amp; if\ {\vert}z_{i}{\vert}{\leq}\lambda_{1}\\ -(\frac{\beta+\sqrt{n_{i}}}{\alpha}+\lambda_{2})^{-1}(z_{i}-sign(z_{i})\lambda_{1})&amp; otherwise\end{cases}\]</span> 下图表明了LR FTRL-Proximal算法单机更新过程。</p>
<div class="figure">
<img src="https://coding.net/u/hjchen2/p/personal/git/raw/master/blog/KunPeng/66cf72a181547ae24831af8500b47d72.png?api=v2%20=500" alt="66cf72a181547ae24831af8500b47d72">
<p class="caption">66cf72a181547ae24831af8500b47d72</p>
</div>
<p>这个算法在单机时很容易实现，但在分布式环境必须要考虑通信效率、servers的负载和算法收敛性问题。考虑到BSP的低效和ASP可能不收敛的问题，他们使用了bounded delay的SSP更新方法，并且设置trust region来调节参数范围，避免模型发散。整个算法具体过程如下：</p>
<ul>
<li>workers本地保存了模型<span class="math inline">\(w\)</span>和<span class="math inline">\(z\)</span>、<span class="math inline">\(n\)</span>，<span class="math inline">\(z\)</span>、<span class="math inline">\(n\)</span>通过bounded-asynchronous的方式与servers保持同步</li>
<li>workers加载数据，根据<span class="math inline">\(z\)</span>和<span class="math inline">\(n\)</span>更新本地模型<span class="math inline">\(w\)</span>，计算梯度并更新本地模型<span class="math inline">\(w\)</span>和<span class="math inline">\(z\)</span>、<span class="math inline">\(n\)</span>，同时使用<span class="math inline">\(\delta_{z}\)</span>和<span class="math inline">\(\delta_{n}\)</span>累加<span class="math inline">\(z\)</span>和<span class="math inline">\(n\)</span>的增量，在需要与servers同步的时候将累加的<span class="math inline">\(\delta_{z}\)</span>和<span class="math inline">\(\delta_{n}\)</span> push到servers</li>
<li>servers合并所有workers发送的<span class="math inline">\(\delta_{z}\)</span>和<span class="math inline">\(\delta_{n}\)</span>，最后更新全局<span class="math inline">\(z\)</span>和<span class="math inline">\(n\)</span>。</li>
</ul>
<p>workers向servers传递<span class="math inline">\(z\)</span>和<span class="math inline">\(n\)</span>的增量，而不是直接传递模型梯度，这种做法虽然会带来一些通信开销，但降低了servers的计算负载，这是在通信效率和计算负载之间做的平衡。为了避免发散，servers在trust region下更新模型。trust region的策略有两种：一种是当模型中的元素超出置信阈时，直接回退整个模型；另一种是通过映射的方式将模型的值限制在置信阈中。</p>
<div class="figure">
<img src="https://coding.net/u/hjchen2/p/personal/git/raw/master/blog/KunPeng/0de2241d38a792bb79446944d65d8c66.png?api=v2%20=600" alt="0de2241d38a792bb79446944d65d8c66">
<p class="caption">0de2241d38a792bb79446944d65d8c66</p>
</div>
<h2 id="mart">MART</h2>
<p>MART（多增量回归树）又叫做GBDT，是一种应用比较广泛的机器学习算法。KunPeng实现了一个通用的MART算法，支持千亿级样本量和上千维的特征，并在MART的基础上实现了LambdaMART算法。</p>
<ul>
<li>MART 为了处理超大规模的数据量，KunPeng-MART使用数据并行的方式减少内存使用量，并采用了XGBoost的分布式加权直方图算法优化分裂点查找过程。具体来说就是，每个worker都保存了整颗树，在分割叶节点时， （1）每个worker使用分配的数据子集计算一个局部加权直方图，计算完成后将直方图push到servers （2）servers收到workers发送的直方图后，采用多路合并算法得到全局直方图，并找到最优分割点 （3）workers从servers pull分割点，分裂节点并将数据分到分裂后的叶节点</li>
</ul>
<p>重复上述过程，可以得到整棵树。然后只要按照gradient boosting方法一棵一棵地建树，最终得到MART。随着特征维度和树深度的增加，查找分裂点过程中的计算和通信都可能成为性能瓶颈。为了解决这个问题，他们提到使用KunPeng的通信模式去减少合并局部直方图的开销，但并没有透露具体的方法。</p>
<ul>
<li>LambdaMART LambdaMART建树的过程与上面的MART一样，不同的是LambdaMART计算一阶导数和二阶导数的方式。由于LambdaMART要求同一个query group的训练数据按sample两两组成pair对，因此当训练数据不是按照query group连续存储时就会存在问题。对于这个问题，他们提出了两种解决方法：<br>
（1）先全局统计一下每个query id对应的样本总数，然后按照multiway number partitioning algorithm对query id进行分片，每个worker只加载属于自己的query ids对应的训练样本。<br>
（2）第二种是近似的方法。首先要求相同query id的样本在文件系统中是连续存储的，然后每个worker还是按照正常情况加载属于自己的分片数据。如果相同query id的样本被分在两个不同的worker上，则会把这两个worker上相同query id的样本当做不同query id来处理。</li>
</ul>
<h2 id="其他算法">其他算法</h2>
<ul>
<li>Large-scale sparse Logistic Regression (LR)<br>
实现了不同的优化算法，L-BFGS、OWL-QN和BCD，其中BCD算法是数据和模型同时并行的算法。<br>
</li>
<li>Distributed Factorization Machines<br>
workers异步计算梯度，使用AdaGrad优化算法<br>
</li>
<li>Caffe<br>
实现了Caffe和KunPeng的对接，a generalized CPU-based large-scale deep learning platform，简化DL算法开发</li>
</ul>
<h2 id="实验结果">实验结果</h2>
<p>下面的实验都是在一个拥有5000台服务器的正式集群上进行的，每台机器12个Intel Xeon CPU E5-2430 (2.2 GHz) CPU和96GB内存。</p>
<h3 id="kunpengspark和mpi的lr算法对比">KunPeng、Spark和MPI的LR算法对比</h3>
<div class="figure">
<img src="https://coding.net/u/hjchen2/p/personal/git/raw/master/blog/KunPeng/143e082b7f1a6b54e47e9c8b51026dbb.png?api=v2" alt="143e082b7f1a6b54e47e9c8b51026dbb">
<p class="caption">143e082b7f1a6b54e47e9c8b51026dbb</p>
</div>
<p>不同平台的LR都采用L-BFGS算法更新，并且memory history parameter都设置为10，并且使用同一个集群相同的CPU资源，在7个不同的数据集上KunPeng在效率和内存占用上都取得非常明显的优势。</p>
<p>在另外一个18 billion样本和 7 billion特征的数据集上，他们统计了KunPeng在不同workers数量时的加速比。</p>
<div class="figure">
<img src="https://coding.net/u/hjchen2/p/personal/git/raw/master/blog/KunPeng/00c84f368394ba04d59dbe530f69c387.png?api=v2" alt="00c84f368394ba04d59dbe530f69c387">
<p class="caption">00c84f368394ba04d59dbe530f69c387</p>
</div>
<p>KunPeng仅使用25个workers就可以训练这么大的数据，workers增加时依然能保持较高的加速比，并且内存占用随着workers增加而近乎直线降低。</p>
<h3 id="kunpeng-mart和xgboost的对比">KunPeng-MART和XGBoost的对比</h3>
<p>下图分别为KunPeng-MAR和XGBoost在不同任务上的峰值内存占用和训练时间对比。</p>
<div class="figure">
<img src="https://coding.net/u/hjchen2/p/personal/git/raw/master/blog/KunPeng/1b0888cab293242eaccdc2b6e5bf25d9.png?api=v2%20=500" alt="1b0888cab293242eaccdc2b6e5bf25d9">
<p class="caption">1b0888cab293242eaccdc2b6e5bf25d9</p>
</div>
<div class="figure">
<img src="https://coding.net/u/hjchen2/p/personal/git/raw/master/blog/KunPeng/3b99dc82bc268d3da394a688c0234908.png?api=v2%20=500" alt="3b99dc82bc268d3da394a688c0234908">
<p class="caption">3b99dc82bc268d3da394a688c0234908</p>
</div>
<h3 id="kunpeng-fmlibfm和difacto的对比">KunPeng-FM、LibFM和DiFacto的对比</h3>
<p>下面是在单机情况下的训练效果对比，并没有训练时间的对比数据和多机实验相关的数据。</p>
<div class="figure">
<img src="https://coding.net/u/hjchen2/p/personal/git/raw/master/blog/KunPeng/da511a1bb0db987fb74ebb08fa5352c9.png?api=v2%20=500" alt="da511a1bb0db987fb74ebb08fa5352c9">
<p class="caption">da511a1bb0db987fb74ebb08fa5352c9</p>
</div>
<h2 id="参考资料">参考资料</h2>
<p>1、Ad Click Prediction: a View from the Trenches.</p>
]]></content>
      
        <categories>
            
            <category> ML framework </category>
            
        </categories>
        
        
        <tags>
            
            <tag> large scale ML framework </tag>
            
            <tag> KunPeng </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[C++调用python]]></title>
      <url>https://hjchen2.github.io/2017/07/03/C++%E8%B0%83%E7%94%A8Python%E6%8E%A5%E5%8F%A3/</url>
      <content type="html"><![CDATA[<p>由于需要在组内新开发的一套机器学习框架上开发一个强化学习的demo，但目前开源的一些游戏环境都只提供了python接口，比如Gym。如果要使用Gym去做在线训练的话，就需要在C++代码中调用Python接口，因此找了些例子学习了一下如何使用Python C API。当然Python C API不是唯一的方式，也可以使用boost的Python模块，有时间再研究。</p>
<h2 id="hello-python">hello python</h2>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"python/Python.h"</span></span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</div><div class="line">    Py_Initialize();</div><div class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"hello c++!"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</div><div class="line">    PyRun_SimpleString(<span class="string">"print 'hello python!'"</span>);</div><div class="line">    Py_Finalize();</div><div class="line">    <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>编译：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">g++ test.cpp -o test -lpython</div></pre></td></tr></table></figure>
<p>执行：./test</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">hello c++!</div><div class="line">hello python!</div></pre></td></tr></table></figure>
<h2 id="调用python脚本中的函数">调用python脚本中的函数</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># test_add.py</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(a, b)</span>:</span></div><div class="line">    <span class="keyword">return</span> a+b</div></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"python/Python.h"</span></span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>* argv[])</span> </span>&#123;</div><div class="line">    <span class="keyword">if</span> (argc &lt; <span class="number">3</span>) &#123;</div><div class="line">        <span class="built_in">std</span>::<span class="built_in">cerr</span> &lt;&lt; <span class="string">"Usage: ./exe integer1 integer2"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</div><div class="line">        <span class="keyword">return</span> <span class="number">1</span>;</div><div class="line">    &#125;</div><div class="line">    <span class="built_in">std</span>::<span class="built_in">cerr</span> &lt;&lt; <span class="string">"hello c++!"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</div><div class="line"></div><div class="line">    Py_Initialize();</div><div class="line">    PyRun_SimpleString(<span class="string">"import sys"</span>);</div><div class="line">    PyRun_SimpleString(<span class="string">"sys.path.append('./')"</span>);</div><div class="line"></div><div class="line">    PyRun_SimpleString(<span class="string">"print 'hello python!'"</span>);</div><div class="line">    PyObject* moduleName = PyString_FromString(<span class="string">"test_add"</span>);</div><div class="line">    PyObject* pModule = PyImport_Import(moduleName);</div><div class="line">    <span class="keyword">if</span> (!pModule) &#123;</div><div class="line">        <span class="built_in">std</span>::<span class="built_in">cerr</span> &lt;&lt; <span class="string">"[ERROR] Python get module failed."</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</div><div class="line">        <span class="keyword">return</span> <span class="number">1</span>;</div><div class="line">    &#125;</div><div class="line">    PyObject* pv = PyObject_GetAttrString(pModule, <span class="string">"add"</span>);</div><div class="line">    <span class="keyword">if</span> (!pv || !PyCallable_Check(pv)) &#123;</div><div class="line">        <span class="built_in">std</span>::<span class="built_in">cerr</span> &lt;&lt; <span class="string">"[ERROR] Can't find function (add)"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</div><div class="line">        <span class="keyword">return</span> <span class="number">1</span>;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    PyObject* args = PyTuple_New(<span class="number">2</span>);</div><div class="line">    PyObject* arg1 = PyInt_FromLong(atoi(argv[<span class="number">1</span>]));</div><div class="line">    PyObject* arg2 = PyInt_FromLong(atoi(argv[<span class="number">2</span>]));</div><div class="line">    PyTuple_SetItem(args, <span class="number">0</span>, arg1);</div><div class="line">    PyTuple_SetItem(args, <span class="number">1</span>, arg2);</div><div class="line"></div><div class="line">    PyObject* pRet = PyObject_CallObject(pv, args);</div><div class="line">    <span class="keyword">if</span> (!pRet) &#123;</div><div class="line">        <span class="built_in">std</span>::<span class="built_in">cerr</span> &lt;&lt; <span class="string">"[ERROR] Call funftion (add) failed"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</div><div class="line">        <span class="keyword">return</span> <span class="number">1</span>;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">long</span> result = PyInt_AsLong(pRet);</div><div class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"result: "</span> &lt;&lt; result &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</div><div class="line"></div><div class="line">    Py_Finalize();</div><div class="line">    <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>编译：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">g++ test.cpp -o test -lpython</div></pre></td></tr></table></figure>
<p>执行：./test 3 4</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">hello c++!</div><div class="line">hello python!</div><div class="line">result: 7</div></pre></td></tr></table></figure>
<h2 id="q学习的一个例子">Q学习的一个例子</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># tree.py</span></div><div class="line"><span class="string">"""</span></div><div class="line">author: Houjiang Chen</div><div class="line">"""</div><div class="line"><span class="keyword">import</span> random</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">q_learning</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, states, actions)</span>:</span></div><div class="line">        self.states = states</div><div class="line">        self.actions = actions</div><div class="line">        self.eps = <span class="number">0.1</span></div><div class="line">        self.alpha = <span class="number">0.1</span></div><div class="line">        self.q_table = [[<span class="number">0</span> <span class="keyword">for</span> j <span class="keyword">in</span> range(actions)] <span class="keyword">for</span> i <span class="keyword">in</span> range(states)]</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_action</span><span class="params">(self, current_state)</span>:</span></div><div class="line">        max_action = self.q_table[current_state].index(max(self.q_table[current_state]))</div><div class="line">        <span class="keyword">if</span> random.uniform(<span class="number">0</span>, <span class="number">1</span>) &gt; self.eps:</div><div class="line">            <span class="keyword">return</span> max_action</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            rest = [i <span class="keyword">for</span> i <span class="keyword">in</span> range(len(self.q_table[current_state])) <span class="keyword">if</span> i != max_action]</div><div class="line">            index = random.randint(<span class="number">0</span>, len(rest) - <span class="number">1</span>)</div><div class="line">            <span class="keyword">return</span> rest[index]</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(self, current_state, action, next_state, reward, final)</span>:</span></div><div class="line">        <span class="keyword">if</span> <span class="keyword">not</span> final:</div><div class="line">            reward = reward + max(self.q_table[next_state])</div><div class="line">        self.q_table[current_state][action] += self.alpha * (reward - self.q_table[current_state][action])</div><div class="line"></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">environment</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        self.level = <span class="number">2</span></div><div class="line">        self.actions = <span class="number">2</span></div><div class="line">        self.states = self.actions ** (self.level + <span class="number">1</span>) - <span class="number">1</span></div><div class="line">        self.final_states = self.actions ** self.level</div><div class="line">        self.reward = &#123;<span class="number">0</span> : [<span class="number">10</span>, <span class="number">-10</span>], <span class="number">1</span> : [<span class="number">50</span>, <span class="number">100</span>], <span class="number">2</span> : [<span class="number">100</span>, <span class="number">150</span>]&#125;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">next</span><span class="params">(self, current_state, action)</span>:</span></div><div class="line">        <span class="string">"""action: 0 or 1</span></div><div class="line">           return: next_state reward, is_final</div><div class="line">        """</div><div class="line">        next = <span class="number">2</span> * current_state + (action + <span class="number">1</span>)</div><div class="line">        <span class="keyword">if</span> next &gt;= self.states - self.final_states:</div><div class="line">            <span class="keyword">return</span> <span class="keyword">None</span>, self.reward[current_state][action], <span class="keyword">True</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">return</span> next, self.reward[current_state][action], <span class="keyword">False</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">return</span> random.randint(<span class="number">0</span>, self.states - self.final_states - <span class="number">1</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></div><div class="line">    env = environment()</div><div class="line">    agent = q_learning(env.states, env.actions)</div><div class="line"></div><div class="line">    episode = <span class="number">0</span></div><div class="line">    <span class="keyword">while</span> episode &lt; <span class="number">10000</span>:</div><div class="line">        episode += <span class="number">1</span></div><div class="line">        <span class="keyword">print</span> <span class="string">"episode: %d"</span> % episode</div><div class="line">        current_state = env.reset()</div><div class="line">        <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">            action = agent.get_action(current_state)</div><div class="line">            next_state, reward, final = env.next(current_state, action)</div><div class="line">            agent.update(current_state, action, next_state, reward, final)</div><div class="line">            <span class="keyword">if</span> final:</div><div class="line">                <span class="keyword">break</span></div><div class="line">            current_state = next_state</div><div class="line"></div><div class="line">    <span class="keyword">print</span> agent.q_table</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    main()</div></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"python2.7/Python.h"</span></span></div><div class="line"></div><div class="line"><span class="function">PyObject* <span class="title">New_PyInstance</span><span class="params">(PyObject* cls, PyObject* args)</span> </span>&#123;</div><div class="line">    PyObject* pInstance = PyInstance_New(cls, args, <span class="literal">NULL</span>);</div><div class="line">    <span class="keyword">if</span> (!pInstance) &#123;</div><div class="line">        <span class="built_in">std</span>::<span class="built_in">cerr</span> &lt;&lt; <span class="string">"new instance failed"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</div><div class="line">        <span class="built_in">exit</span>(<span class="number">1</span>);</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> pInstance;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>* argv[])</span> </span>&#123;</div><div class="line">    Py_Initialize();</div><div class="line">    PyRun_SimpleString(<span class="string">"import sys"</span>);</div><div class="line">    PyRun_SimpleString(<span class="string">"sys.path.append('./')"</span>);</div><div class="line"></div><div class="line">    PyObject* moduleName = PyString_FromString(<span class="string">"tree"</span>);</div><div class="line">    PyObject* pModule = PyImport_Import(moduleName);</div><div class="line">    <span class="keyword">if</span> (!pModule) &#123;</div><div class="line">        <span class="built_in">std</span>::<span class="built_in">cerr</span> &lt;&lt; <span class="string">"[ERROR] Python get module failed."</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</div><div class="line">        <span class="keyword">return</span> <span class="number">1</span>;</div><div class="line">    &#125;</div><div class="line">    PyObject* pEnv = PyObject_GetAttrString(pModule, <span class="string">"environment"</span>);</div><div class="line">    <span class="keyword">if</span> (!pEnv) &#123;</div><div class="line">        <span class="built_in">std</span>::<span class="built_in">cerr</span> &lt;&lt; <span class="string">"[ERROR] Can't find class (environment)"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</div><div class="line">        <span class="keyword">return</span> <span class="number">1</span>;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    PyObject* pEnvObject = New_PyInstance(pEnv, <span class="literal">NULL</span>);</div><div class="line">    PyObject* pEnvLevel = PyObject_GetAttrString(pEnvObject, <span class="string">"level"</span>);</div><div class="line">    <span class="keyword">if</span> (!pEnvLevel) &#123;</div><div class="line">        <span class="built_in">std</span>::<span class="built_in">cerr</span> &lt;&lt; <span class="string">"[ERROR] Env has no attr level"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</div><div class="line">        <span class="keyword">return</span> <span class="number">1</span>;</div><div class="line">    &#125;</div><div class="line">    PyObject* pEnvActions = PyObject_GetAttrString(pEnvObject, <span class="string">"actions"</span>);</div><div class="line">    PyObject* pEnvStates = PyObject_GetAttrString(pEnvObject, <span class="string">"states"</span>);</div><div class="line">    PyObject* pEnvFinalState = PyObject_GetAttrString(pEnvObject, <span class="string">"final_states"</span>);</div><div class="line"></div><div class="line">    <span class="keyword">int</span> level = PyInt_AsLong(pEnvLevel);</div><div class="line">    <span class="keyword">int</span> actions = PyInt_AsLong(pEnvActions);</div><div class="line">    <span class="keyword">int</span> states = PyInt_AsLong(pEnvStates);</div><div class="line">    <span class="keyword">int</span> final_state = PyInt_AsLong(pEnvFinalState);</div><div class="line"></div><div class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"env level: "</span> &lt;&lt; level &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</div><div class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"env actions: "</span> &lt;&lt; actions &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</div><div class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"env states: "</span> &lt;&lt; states &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</div><div class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"env final_state: "</span> &lt;&lt; final_state &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</div><div class="line"></div><div class="line">    PyObject* pLearn = PyObject_GetAttrString(pModule, <span class="string">"q_learning"</span>);</div><div class="line">    PyObject* pLearnArgs = Py_BuildValue(<span class="string">"ii"</span>, states, actions);</div><div class="line">    PyObject* pLearnObject = New_PyInstance(pLearn, pLearnArgs);</div><div class="line">    PyObject* pLearnStates = PyObject_GetAttrString(pLearnObject, <span class="string">"states"</span>);</div><div class="line">    PyObject* pLearnActions = PyObject_GetAttrString(pLearnObject, <span class="string">"actions"</span>);</div><div class="line">    PyObject* pLearnEps = PyObject_GetAttrString(pLearnObject, <span class="string">"eps"</span>);</div><div class="line"></div><div class="line">    <span class="keyword">int</span> learn_states = PyInt_AsLong(pLearnStates);</div><div class="line">    <span class="keyword">int</span> learn_actions = PyInt_AsLong(pLearnActions);</div><div class="line">    <span class="keyword">float</span> learn_eps = PyFloat_AsDouble(pLearnEps);</div><div class="line"></div><div class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"learn_states: "</span> &lt;&lt; learn_states &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</div><div class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"learn_actions: "</span> &lt;&lt; learn_actions &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</div><div class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"learn_eps: "</span> &lt;&lt; learn_eps &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</div><div class="line"></div><div class="line">    PyObject* pEnvResetFunc = PyObject_GetAttrString(pEnvObject, <span class="string">"reset"</span>);</div><div class="line">    PyObject* pEnvNextFunc = PyObject_GetAttrString(pEnvObject, <span class="string">"next"</span>);</div><div class="line">    PyObject* pLearnGetActionFunc = PyObject_GetAttrString(pLearnObject, <span class="string">"get_action"</span>);</div><div class="line">    PyObject* pLearnUpdateFunc = PyObject_GetAttrString(pLearnObject, <span class="string">"update"</span>);</div><div class="line">    <span class="keyword">if</span> (!pEnvNextFunc) &#123;</div><div class="line">        <span class="built_in">std</span>::<span class="built_in">cerr</span> &lt;&lt; <span class="string">"[ERROR] env has no function named next"</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</div><div class="line">        <span class="keyword">return</span> <span class="number">1</span>;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</div><div class="line">    <span class="keyword">uint64_t</span> episode = <span class="number">0</span>;</div><div class="line">    <span class="keyword">for</span> (episode = <span class="number">0</span>; episode &lt; <span class="number">10000</span>; ++episode) &#123;</div><div class="line">        <span class="keyword">if</span> (episode % <span class="number">100</span> == <span class="number">0</span>)</div><div class="line">            <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"episode: "</span> &lt;&lt; episode &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</div><div class="line">        PyObject* current_state = PyEval_CallObject(pEnvResetFunc, <span class="literal">NULL</span>);</div><div class="line">        <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</div><div class="line">            PyObject* args1 = PyTuple_New(<span class="number">1</span>);</div><div class="line">            PyObject* args2 = PyTuple_New(<span class="number">2</span>);</div><div class="line">            PyTuple_SetItem(args1, <span class="number">0</span>, current_state);</div><div class="line">            PyObject* action = PyEval_CallObject(pLearnGetActionFunc, args1);</div><div class="line">            PyTuple_SetItem(args2, <span class="number">0</span>, current_state);</div><div class="line">            PyTuple_SetItem(args2, <span class="number">1</span>, action);</div><div class="line">            PyObject* ret = PyEval_CallObject(pEnvNextFunc, args2);</div><div class="line">            PyObject* next_state = PyTuple_GetItem(ret, <span class="number">0</span>);</div><div class="line">            PyObject* final = PyTuple_GetItem(ret ,<span class="number">2</span>);</div><div class="line">            PyObject* args3 = PyTuple_New(<span class="number">5</span>);</div><div class="line">            PyTuple_SetItem(args3, <span class="number">0</span>, current_state);</div><div class="line">            PyTuple_SetItem(args3, <span class="number">1</span>, action);</div><div class="line">            PyTuple_SetItem(args3, <span class="number">2</span>, next_state);</div><div class="line">            PyTuple_SetItem(args3, <span class="number">3</span>, PyTuple_GetItem(ret, <span class="number">1</span>));</div><div class="line">            PyTuple_SetItem(args3, <span class="number">4</span>, final);</div><div class="line"></div><div class="line">            PyEval_CallObject(pLearnUpdateFunc, args3);</div><div class="line">            <span class="keyword">if</span> (PyObject_IsTrue(final)) &#123;</div><div class="line">                <span class="keyword">break</span>;</div><div class="line">            &#125;</div><div class="line">            current_state = next_state;</div><div class="line">            <span class="keyword">if</span> (args3)</div><div class="line">                Py_DECREF(args3);</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">    PyObject* pLearnQTable = PyObject_GetAttrString(pLearnObject, <span class="string">"q_table"</span>);</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; PyList_Size(pLearnQTable); ++i) &#123;</div><div class="line">        <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"state "</span> &lt;&lt; i &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</div><div class="line">        PyObject* term = PyList_GetItem(pLearnQTable, i);</div><div class="line">        <span class="keyword">if</span> (PyList_Check(term)) &#123;</div><div class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; PyList_Size(term); ++j) &#123;</div><div class="line">                <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"    direct: "</span> &lt;&lt; j &lt;&lt; <span class="string">", "</span> &lt;&lt; <span class="string">"Qvalue: "</span></div><div class="line">                          &lt;&lt; PyFloat_AsDouble(PyList_GetItem(term, j)) &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">    Py_Finalize();</div><div class="line">    <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>编译：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">g++ test.cpp -o test -I../python2.7.12/include -L../python2.7.12/lib -lpython2.7</div></pre></td></tr></table></figure>
<p>执行：./test</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div></pre></td><td class="code"><pre><div class="line">env level: 2</div><div class="line">env actions: 2</div><div class="line">env states: 7</div><div class="line">env final_state: 4</div><div class="line">learn_states: 7</div><div class="line">learn_actions: 2</div><div class="line">learn_eps: 0.1</div><div class="line"></div><div class="line">episode: 0</div><div class="line">episode: 100</div><div class="line">episode: 200</div><div class="line">episode: 300</div><div class="line">episode: 400</div><div class="line">episode: 500</div><div class="line">episode: 600</div><div class="line">episode: 700</div><div class="line">episode: 800</div><div class="line">episode: 900</div><div class="line">episode: 1000</div><div class="line">episode: 1100</div><div class="line">episode: 1200</div><div class="line">episode: 1300</div><div class="line">episode: 1400</div><div class="line">episode: 1500</div><div class="line">episode: 1600</div><div class="line">episode: 1700</div><div class="line">episode: 1800</div><div class="line">episode: 1900</div><div class="line">episode: 2000</div><div class="line">episode: 2100</div><div class="line">episode: 2200</div><div class="line">episode: 2300</div><div class="line">episode: 2400</div><div class="line">episode: 2500</div><div class="line">episode: 2600</div><div class="line">episode: 2700</div><div class="line">episode: 2800</div><div class="line">episode: 2900</div><div class="line">episode: 3000</div><div class="line">episode: 3100</div><div class="line">episode: 3200</div><div class="line">episode: 3300</div><div class="line">episode: 3400</div><div class="line">episode: 3500</div><div class="line">episode: 3600</div><div class="line">episode: 3700</div><div class="line">episode: 3800</div><div class="line">episode: 3900</div><div class="line">episode: 4000</div><div class="line">episode: 4100</div><div class="line">episode: 4200</div><div class="line">episode: 4300</div><div class="line">episode: 4400</div><div class="line">episode: 4500</div><div class="line">episode: 4600</div><div class="line">episode: 4700</div><div class="line">episode: 4800</div><div class="line">episode: 4900</div><div class="line">episode: 5000</div><div class="line">episode: 5100</div><div class="line">episode: 5200</div><div class="line">episode: 5300</div><div class="line">episode: 5400</div><div class="line">episode: 5500</div><div class="line">episode: 5600</div><div class="line">episode: 5700</div><div class="line">episode: 5800</div><div class="line">episode: 5900</div><div class="line">episode: 6000</div><div class="line">episode: 6100</div><div class="line">episode: 6200</div><div class="line">episode: 6300</div><div class="line">episode: 6400</div><div class="line">episode: 6500</div><div class="line">episode: 6600</div><div class="line">episode: 6700</div><div class="line">episode: 6800</div><div class="line">episode: 6900</div><div class="line">episode: 7000</div><div class="line">episode: 7100</div><div class="line">episode: 7200</div><div class="line">episode: 7300</div><div class="line">episode: 7400</div><div class="line">episode: 7500</div><div class="line">episode: 7600</div><div class="line">episode: 7700</div><div class="line">episode: 7800</div><div class="line">episode: 7900</div><div class="line">episode: 8000</div><div class="line">episode: 8100</div><div class="line">episode: 8200</div><div class="line">episode: 8300</div><div class="line">episode: 8400</div><div class="line">episode: 8500</div><div class="line">episode: 8600</div><div class="line">episode: 8700</div><div class="line">episode: 8800</div><div class="line">episode: 8900</div><div class="line">episode: 9000</div><div class="line">episode: 9100</div><div class="line">episode: 9200</div><div class="line">episode: 9300</div><div class="line">episode: 9400</div><div class="line">episode: 9500</div><div class="line">episode: 9600</div><div class="line">episode: 9700</div><div class="line">episode: 9800</div><div class="line">episode: 9900</div><div class="line">state 0</div><div class="line">    direct: 0, Qvalue: 110</div><div class="line">    direct: 1, Qvalue: 140</div><div class="line">state 1</div><div class="line">    direct: 0, Qvalue: 50</div><div class="line">    direct: 1, Qvalue: 100</div><div class="line">state 2</div><div class="line">    direct: 0, Qvalue: 100</div><div class="line">    direct: 1, Qvalue: 150</div><div class="line">state 3</div><div class="line">    direct: 0, Qvalue: 0</div><div class="line">    direct: 1, Qvalue: 0</div><div class="line">state 4</div><div class="line">    direct: 0, Qvalue: 0</div><div class="line">    direct: 1, Qvalue: 0</div><div class="line">state 5</div><div class="line">    direct: 0, Qvalue: 0</div><div class="line">    direct: 1, Qvalue: 0</div><div class="line">state 6</div><div class="line">    direct: 0, Qvalue: 0</div><div class="line">    direct: 1, Qvalue: 0</div></pre></td></tr></table></figure>
<h2 id="参考资料">参考资料</h2>
<p>Python/C API Reference Manual: https://docs.python.org/2/c-api/index.html</p>
]]></content>
      
        <categories>
            
            <category> code </category>
            
        </categories>
        
        
        <tags>
            
            <tag> c++ </tag>
            
            <tag> python </tag>
            
            <tag> embedding </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[多节点异步更新中momentum的影响]]></title>
      <url>https://hjchen2.github.io/2017/06/21/ASGD%E4%B8%ADmomentum%E7%9A%84%E5%BD%B1%E5%93%8D/</url>
      <content type="html"><![CDATA[<p>这几天的主要工作是将caffe移植到组内新开发的某个计算框架，在验证正确性时遇到一个问题。由于计算框架只支持异步更新的方式，因此采用全异步SGD算法训练Alexnet时非常容易发散。另外调研了一下近期发布的异步更新算法DC-ASGD，实验结果只能说对收敛有些正向效果，仍无法解决训练发散的问题。在另外一个DNN的网络上发现在多机时momentum对收敛结果有较大影响，momentum会导致收敛出现较大波动。</p>
<p>网上找了一圈，似乎也就这个有些参考价值： http://stanford.edu/~imit/tuneyourmomentum/theory/</p>
<p>看来近期得做一些调momentum和学习率的实验了。。。</p>
]]></content>
      
        <categories>
            
            <category> deep learning </category>
            
        </categories>
        
        
        <tags>
            
            <tag> caffe </tag>
            
            <tag> deep learning </tag>
            
            <tag> momentum </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[强化学习（二）]]></title>
      <url>https://hjchen2.github.io/2017/04/25/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
      <content type="html"><![CDATA[<h2 id="dqn">DQN</h2>
<p>前面我们讲到TD算法结合了动态规划和蒙特卡洛算法的优点，不依赖具体的环境模型，并且更新时采用滑动平均的方式，因此单步就能更新，而不需要生成整个episode，在非episode情况下仍然适用。TD算法又分为on policy的sarsa算法和off policy的Q learning算法，其中Q learning算法直接使用下一状态的最大动作值函数进行更新，加快了算法收敛速度，因此Q learning算法在实际应用中更加普遍。</p>
<h3 id="q-learning例子">Q learning例子</h3>
<p>我们用一个例子来说明Q learning算法的过程。下图是一个二叉树表示的路径规划问题，每一个节点代表环境中的一个状态，叶子节点表示终止状态，每个非叶子节点都可以选择向上或向下的动作，然后转移到下一个节点，并获得相应的得分。</p>

<img src="https://github.com/hjchen2/personal/blob/master/blog/pictures/9930b76dc4a4c37e188ea6363fe6603b.png?raw=true" width="600">

<p><br> 首先初始化所有状态动作对的动作值函数：<span class="math inline">\(Q(S_{i},a)=0, \forall i\in[1，6],a\in[上，下]\)</span>，并且初始化<span class="math inline">\(\epsilon = 0.1，\alpha = 0.1\)</span>。</p>
<ul>
<li>随机选择一个初始状态<span class="math inline">\(S\)</span>，假设为<span class="math inline">\(S_0\)</span><br>
根据<span class="math inline">\(\epsilon-greedy\)</span>策略选择一个动作，假设为上，转移到状态<span class="math inline">\(S_1\)</span>，那么更新<span class="math inline">\(Q(S_0,上)=Q(S_0,上)+\alpha\cdot(R_{1}+\max_aQ(S_1,a)-Q(S_0,上))=0+0.1\cdot(10+0-0)=1\)</span>，接下来继续根据<span class="math inline">\(\epsilon-greedy\)</span>策略选择下一个动作，比如下，并且转移到终止状态<span class="math inline">\(S_4\)</span>，因此<span class="math inline">\(Q(S_1,下)=Q(S_0,下)+\alpha\cdot(R_{2}+\max_aQ(S_4,a)-Q(S_1,下))=0+0.1\cdot(100+0-0)=10\)</span>。</li>
<li>随机选择一个初始状态<span class="math inline">\(S\)</span>，假设为<span class="math inline">\(S_2\)</span><br>
根据<span class="math inline">\(\epsilon-greedy\)</span>策略选择一个动作，假设为上，转移到终止状态<span class="math inline">\(S_5\)</span>，则更新<span class="math inline">\(Q(S_2,上)=0+0.1\cdot（100+0-0）=10\)</span></li>
<li>随机选择一个初始状态<span class="math inline">\(S\)</span>，假设为<span class="math inline">\(S_0\)</span><br>
根据<span class="math inline">\(\epsilon-greedy\)</span>策略选择一个动作，假设为上，转移到状态<span class="math inline">\(S_1\)</span>，则更新<span class="math inline">\(Q(S_0,上)=1+0.1\cdot(10+10-1)=2.9\)</span>，选择下一个动作，比如上，则<span class="math inline">\(Q(S_1,上)=0+0.1\cdot(50+0-0)=5\)</span></li>
<li>随机选择一个初始状态<span class="math inline">\(S\)</span>，假设为<span class="math inline">\(S_0\)</span><br>
根据<span class="math inline">\(\epsilon-greedy\)</span>策略选择一个动作，假设为上，转移到状态<span class="math inline">\(S_1\)</span>，则更新<span class="math inline">\(Q(S_0,上)=2.9+0.1\cdot(10+10-2.9)=4.61\)</span>，选择下一个动作，比如下，则<span class="math inline">\(Q(S_1,下)=10+0.1\cdot(100+0-10)=19\)</span></li>
<li>…</li>
</ul>
<p>下面是该例子的python实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div></pre></td><td class="code"><pre><div class="line"><span class="string">"""</span></div><div class="line">author: Houjiang Chen</div><div class="line">"""</div><div class="line"><span class="keyword">import</span> random</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">q_learning</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, states, actions)</span>:</span></div><div class="line">        self.states = states</div><div class="line">        self.actions = actions</div><div class="line">        self.eps = <span class="number">0.1</span></div><div class="line">        self.alpha = <span class="number">0.1</span></div><div class="line">        self.q_table = [[<span class="number">0</span> <span class="keyword">for</span> j <span class="keyword">in</span> range(actions)] <span class="keyword">for</span> i <span class="keyword">in</span> range(states)]</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_action</span><span class="params">(self, current_state)</span>:</span></div><div class="line">        max_action = self.q_table[current_state].index(max(self.q_table[current_state]))</div><div class="line">        <span class="keyword">if</span> random.uniform(<span class="number">0</span>, <span class="number">1</span>) &gt; self.eps:</div><div class="line">            <span class="keyword">return</span> max_action</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            rest = [i <span class="keyword">for</span> i <span class="keyword">in</span> range(len(self.q_table[current_state])) <span class="keyword">if</span> i != max_action]</div><div class="line">            index = random.randint(<span class="number">0</span>, len(rest) - <span class="number">1</span>)</div><div class="line">            <span class="keyword">return</span> rest[index]</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(self, current_state, action, next_state, reward, final)</span>:</span></div><div class="line">        <span class="keyword">if</span> final != <span class="number">1</span>:</div><div class="line">            reward = reward + max(self.q_table[next_state])</div><div class="line">        self.q_table[current_state][action] += self.alpha * (reward - self.q_table[current_state][action])</div><div class="line">        </div><div class="line">        </div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">environment</span><span class="params">(object)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        self.level = <span class="number">2</span></div><div class="line">        self.actions = <span class="number">2</span></div><div class="line">        self.states = self.actions ** (self.level + <span class="number">1</span>) - <span class="number">1</span></div><div class="line">        self.final_states = self.actions ** self.level</div><div class="line">        self.reward = &#123;<span class="number">0</span> : [<span class="number">10</span>, <span class="number">-10</span>], <span class="number">1</span> : [<span class="number">50</span>, <span class="number">100</span>], <span class="number">2</span> : [<span class="number">100</span>, <span class="number">150</span>]&#125;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">next</span><span class="params">(self, current_state, action)</span>:</span></div><div class="line">        <span class="string">"""action: 0 or 1</span></div><div class="line">           return: next_state, reward, is_final</div><div class="line">        """</div><div class="line">        next = <span class="number">2</span> * current_state + (action + <span class="number">1</span>)</div><div class="line">        <span class="keyword">if</span> next &gt;= self.states - self.final_states:</div><div class="line">            <span class="keyword">return</span> <span class="keyword">None</span>, self.reward[current_state][action], <span class="number">1</span></div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            <span class="keyword">return</span> next, self.reward[current_state][action], <span class="number">0</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">return</span> random.randint(<span class="number">0</span>, self.states - self.final_states - <span class="number">1</span>)</div><div class="line">    </div><div class="line"></div><div class="line">env = environment()</div><div class="line">agent = q_learning(env.states, env.actions)</div><div class="line"></div><div class="line">episode = <span class="number">0</span></div><div class="line"><span class="keyword">while</span> episode &lt; <span class="number">100000</span>:</div><div class="line">    episode += <span class="number">1</span></div><div class="line">    <span class="keyword">print</span> <span class="string">"episode: %d"</span> % episode</div><div class="line">    current_state = env.reset()</div><div class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">        action = agent.get_action(current_state)</div><div class="line">        next_state, reward, final = env.next(current_state, action)</div><div class="line">        agent.update(current_state, action, next_state, reward, final)</div><div class="line">        <span class="keyword">if</span> final:</div><div class="line">            <span class="keyword">break</span></div><div class="line">        current_state = next_state</div><div class="line"></div><div class="line"><span class="keyword">print</span> agent.q_table</div></pre></td></tr></table></figure>
<p>最终收敛结果为:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[[<span class="number">109.99999999999989</span>, <span class="number">139.99999999999977</span>], </div><div class="line">[<span class="number">49.99999999999997</span>, <span class="number">99.99999999999994</span>], </div><div class="line">[<span class="number">99.99999999999994</span>, <span class="number">149.9999999999999</span>], </div><div class="line">[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>]]</div></pre></td></tr></table></figure>
<h3 id="函数逼近">函数逼近</h3>
<p>上面的例子中非终止状态数只有3个，每个非终止状态对应的动作只有2个，因此状态动作对总共有6个，使用表格存储完全没有问题，但实际上我们需要解决的并不是一个如此简单的问题。比如在【Playing Atari with Deep Reinforcement Learning】中DeepMind就使用Q learning使得agent玩Atari 2600游戏的水平超越了人类水平。在Atari 2600游戏中，每个游戏画面都是一个状态，如果每个画面都是像素为84*84的256灰度图像，那么将会产生<span class="math inline">\(256^{84\cdot84}\)</span>个状态，用表格进行存储将会变得非常不现实。为了解决状态数爆炸的问题，通常可以使用函数逼近的方法。下面有几种函数表示的方式：</p>
<div align="center">
<img src="https://github.com/hjchen2/personal/blob/master/blog/pictures/30EFF3D4-0562-4544-BFF9-D43B3EC7AFF7.png?raw=true">
</div>
<p><br></p>
<p>并且逼近函数的形式可以采用：</p>
<ul>
<li>Linear combinations of features</li>
<li>Neural network</li>
<li>Decision tree</li>
<li>Nearest neighbour</li>
<li>Fourier / wavelet bases</li>
<li>…</li>
</ul>
<p>下面我们研究的DQN（Deep Q Network）就是采用Deep neural network进行动作值函数逼近的一种方法，结构如下。</p>
<div align="center">
<img src="https://github.com/hjchen2/personal/blob/master/blog/pictures/8e238f9d9836b789276e0e58d4aa1e34.png?raw=true" width="400">
</div>
<p><br></p>
<p>为推导方便，假设中间的Network为一层的全连接，即<span class="math inline">\(\hat{V}(s, a)=x(S)^{T}w=\sum_{j=1}^{n}{x_{j}(S)w_{j}}​\)</span>，代价函数选择最小均方误差：<span class="math inline">\(J(w)=\frac{1}{2}(V(s,a)-\hat{V}(s,a))^2​\)</span>，采用随机梯度下降算法进行优化。</p>
<p><span class="math display">\[\begin{split}\frac{\partial{J(w)}}{\partial{w}}&amp;=\left(V(s,a)-\hat{V}(s,a)\right)\frac{\partial{\hat{V}(s,a)}} {\partial{w}} \\ &amp;=\left(V(s,a)-\hat{V}(s,a)\right)x(S) \end{split}\tag{1-1}\]</span></p>
<p><span class="math display">\[\begin{split}w^k&amp;=w^{k-1}+\eta \Delta(w)\\&amp;=w^{k-1}-\eta \frac{\partial{J(w)}}{\partial{w}}\\&amp;=w^{k-1}-\eta \left(V(s,a)-\hat{V}(s,a;w^{k})\right)x(S)\end{split}\tag{1-2}\]</span></p>
<p>由于我们并没有动作值函数的真实值，因此与Q learning类似，<span class="math inline">\(V(s,a,)\)</span>可以使用下一个状态的动作值函数进行估计，即<span class="math inline">\(V(s,a)=V(s,a;w^{k-1})=r+\gamma \max_{a^{&#39;}}V(s^{&#39;},a^{&#39;};w^{k-1})\)</span>。</p>
<p>整个训练过程仍然与Q learning一样，采用<span class="math inline">\(\epsilon-greedy\)</span>策略选择动作，并按照公式(1-2)更新权重<span class="math inline">\(w\)</span>，实际上也就更新了策略的动作值函数。使用值函数逼近的方法不需要枚举每个状态动作对，突破了状态数的限制，使得Q learning在一些复杂任务上得到广泛应用，但仍然没有解决动作数爆炸或者连续动作的问题。</p>
<h3 id="dqn-1">DQN</h3>
<p>DQN最先出现于DeepMind发表的【Playing Atari with Deep Reinforcement Learning】论文中，由于需要直接输入图像画面，因此论文中使用CNN来表示Q函数，下面简单剖析一下该论文。</p>
<p>使用的是典型的CNN，其结构为：</p>
<div align="center">
<img src="https://github.com/hjchen2/personal/blob/master/blog/pictures/93F5C516-8E53-4F89-B03E-3EDD95DF1C76.png?raw=true">
</div>
<p><br> 与一般的CNN有所不同的是，没有pooling层，因为我们这里不是做图像分类，pooling层带来的旋转和数值不变性对分类是有作用的，但在这个任务中对物体的具体位置是非常敏感的，因此移除了pooling层。</p>
<p>Atari原始的游戏帧为210*160像素的RGB图像，由于该任务对画面色彩不敏感，为了减少计算开销，将游戏帧预处理成84*84的灰度图像。但为了获得动态特征，最终是将前3帧图像与当前帧stack到一起组成一个4*84*84的图像作为CNN的输入，输出为每个动作对应的Q值。</p>
<h3 id="经验回放">经验回放</h3>
<p>现在我们知道可以使用Q learning去估计每个状态的未来回报的期望，并且可以使用CNN去逼近动作值函数，也就是可以使用DQN去解决一个复杂的MDP任务。但在实际应用时会出现更新波动较大，导致收敛非常慢的问题，DeepMind因此使用了一个经验回放（Experience Replay）机制，就是将每步的经验数据<span class="math inline">\(&lt;s,a,r,s^{&#39;}&gt;\)</span>存放在回放内存中，更新时都从回放内存中随机采样一个batch的数据进行更新。</p>
<p>经验回放机制相比标准的DQN有两个好处：首先每一步的经验数据会被保存起来，更新时可以多次使用到经验数据，使得数据利用更高效；此外直接从连续的样本中学习是低效的，因为一个episode内样本具有很强的相关性，随机挑选样本打破了这种相关性，因此减小了更新时的变化，使得更新更加稳定（注：因为同一次实验过程的样本相关性很强，不同实验之间的相关性就显得相对比较小，如果使用连续的样本进行训练，在切换到下一次实验的样本时会导致模型更新不稳定）。</p>
<p>由于内存大小限制，回放内存不可能将所有的经验数据都保存起来，因此只会保留最新的N组经验数据，比较久远的数据就会被遗忘。</p>
<h3 id="训练">训练</h3>
<p>DeepMind使用DQN对 ATARI中七个游戏进行了实验，由于每个游戏的得分尺度不一致，因此他们将得分分为正回报、负回报和无回报，正回报得分为1，负回报得分为-1，无回报得分为0。</p>
<p>使用 RMSProp算法进行优化，batch size为32，采用<span class="math inline">\(\epsilon-greedy\)</span>行动策略，前一百万帧的<span class="math inline">\(\epsilon\)</span>从1线性减少到0.1，最后固定为0.1。总共训练了一千万帧，并且使用了一百万大小的回放内存。</p>
<p>训练过程伪代码：</p>
<div align="center">
<img src="https://github.com/hjchen2/personal/blob/master/blog/pictures/1E5C7D95-519A-4B54-BF09-C27A163D12C8.png?raw=true" width="600">
</div>
<h2 id="gym使用">Gym使用</h2>
<h3 id="gym简介">Gym简介</h3>
<p>目前强化学习的研究主要由DeepMind和OpenAI两家在主导，去年底到今年初DeepMind和OpenAI相继开源了自家的3D learning environment平台DeepMind Lab和Universe。DeepMind Lab目前给出的文档和例子都比较少，使用也稍显复杂，所以暂时可以不考虑使用。Universe包含了1000+的游戏环境，并且将程序打包在docker环境中运行，提供与Gym一致的接口。Universe的环境由一个client和一个remote组成，client是一个VNCenv，主要负责接收agent的动作，传递回报和管理本地episode的状态，remote是指在docker环境中运行的程序，remote可以运行在本地、远程服务器或在cloud上。client和remote通过VNC远程桌面系统进行交互，通过WebSocket传递回报、诊断和控制信息。</p>
<p>由于Universe环境提供Gym接口，而Gym是OpenAI去年4月份发布的一套开发和比较强化学习算法的toolkit。Gym本身是可以独立于Universe使用的，并且Universe和Gym中agent代码基本没有什么区别。我们下面就单独讲讲Gym接口和如何使用Gym训练自己的agent。</p>
<p>Gym目前提供python接口，并支持任何的计算框架，比如tensorflow、theano等。强化学习解决的是agent和环境交互的任务，agent根据当前环境状态做出某个动作，然后观察下一个状态和回报，环境根据agent的动作转移到下一个状态，并发送回报。Gym提供的实际上是环境这个角色，每个Gym环境都提供一致的接口。</p>
<h3 id="创建一个gym环境">创建一个Gym环境</h3>
<p>创建一个环境时只需要指定环境id，比如agent需要玩Atari Breakout-v0这个游戏，可以如下创建一个Breakout-v0的环境。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> gym</div><div class="line">env = gym.make(<span class="string">'Breakout-v0'</span>)</div></pre></td></tr></table></figure>
<h3 id="step">step</h3>
<p>输入agent的动作，返回4个值，分别为：</p>
<ul>
<li>observation：表示agent观察到的下一个状态，比如在一些游戏中，observation为RGB的图像</li>
<li>reward：表示执行输入的动作后得到的回报值</li>
<li>done：表示返回的observation是不是结束状态</li>
<li>info：调试信息，一般没什么用处</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">next_state, reward, terminal, _ = env.step(action)</div></pre></td></tr></table></figure>
<h3 id="reset">reset</h3>
<p>在开始一个新的episode时，Gym环境都要reset，获得一个初始状态。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">init_state = env.reset()</div></pre></td></tr></table></figure>
<h3 id="render">render</h3>
<p>render是Gym用来渲染环境状态的函数，当调用该函数时会出现一个动图框。一般agent执行一个动作，环境都要渲染一次，这样就可以实时看到agent的执行情况了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">env.render()</div></pre></td></tr></table></figure>
<h3 id="spaces">Spaces</h3>
<p>Gym环境有两个space属性，一个是action_space，一个是observation_space，分别表示该Gym环境下合法的动作和状态。action_space是Gym中的一个Discrete对象，Discrete对象有一个成员n，表示合法的动作数，比如Discrete(2)表示有两个合法动作，编号从0开始，因此两个动作编号为0和1。observation_space是Gym中的一个Box对象，Box的shape表示observation的数据组织方式，比如Box(210, 160, 3)表示合法的observation是一个210*160*3的数组，而Box(4,)表示observation是一个大小为4的向量。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">observation_space = env.observation_space <span class="comment"># observation_space: Discrete(6)</span></div><div class="line">action_space = env.action_space <span class="comment"># action_space: Box(210, 160, 3)</span></div></pre></td></tr></table></figure>
<h3 id="breakout-v0例子">Breakout-v0例子</h3>
<p>采用了github上Flood Sung的DQN实现，感谢Flood Sung大神的无私贡献。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div><div class="line">187</div><div class="line">188</div><div class="line">189</div><div class="line">190</div><div class="line">191</div><div class="line">192</div><div class="line">193</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -----------------------------</span></div><div class="line"><span class="comment"># File: Deep Q-Learning Algorithm</span></div><div class="line"><span class="comment"># Author: Flood Sung</span></div><div class="line"><span class="comment"># Date: 2016.3.21</span></div><div class="line"><span class="comment"># -----------------------------</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> random</div><div class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> deque</div><div class="line"></div><div class="line"><span class="comment"># Hyper Parameters:</span></div><div class="line">FRAME_PER_ACTION = <span class="number">1</span></div><div class="line">GAMMA = <span class="number">0.99</span> <span class="comment"># decay rate of past observations</span></div><div class="line">OBSERVE = <span class="number">100.</span> <span class="comment"># timesteps to observe before training</span></div><div class="line">EXPLORE = <span class="number">200000.</span> <span class="comment"># frames over which to anneal epsilon</span></div><div class="line">FINAL_EPSILON = <span class="number">0</span><span class="comment">#0.001 # final value of epsilon</span></div><div class="line">INITIAL_EPSILON = <span class="number">0</span><span class="comment">#0.01 # starting value of epsilon</span></div><div class="line">REPLAY_MEMORY = <span class="number">50000</span> <span class="comment"># number of previous transitions to remember</span></div><div class="line">BATCH_SIZE = <span class="number">32</span> <span class="comment"># size of minibatch</span></div><div class="line">UPDATE_TIME = <span class="number">100</span></div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">BrainDQN</span>:</span></div><div class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,actions)</span>:</span></div><div class="line">		<span class="comment"># init replay memory</span></div><div class="line">		self.replayMemory = deque()</div><div class="line">		<span class="comment"># init some parameters</span></div><div class="line">		self.timeStep = <span class="number">0</span></div><div class="line">		self.epsilon = INITIAL_EPSILON</div><div class="line">		self.actions = actions</div><div class="line">		<span class="comment"># init Q network</span></div><div class="line">		self.stateInput,self.QValue,self.W_conv1,self.b_conv1,self.W_conv2,self.b_conv2,self.W_conv3,self.b_conv3,self.W_fc1,self.b_fc1,self.W_fc2,self.b_fc2 = self.createQNetwork()</div><div class="line"></div><div class="line">		<span class="comment"># init Target Q Network</span></div><div class="line">		self.stateInputT,self.QValueT,self.W_conv1T,self.b_conv1T,self.W_conv2T,self.b_conv2T,self.W_conv3T,self.b_conv3T,self.W_fc1T,self.b_fc1T,self.W_fc2T,self.b_fc2T = self.createQNetwork()</div><div class="line"></div><div class="line">		self.copyTargetQNetworkOperation = [self.W_conv1T.assign(self.W_conv1),self.b_conv1T.assign(self.b_conv1),self.W_conv2T.assign(self.W_conv2),self.b_conv2T.assign(self.b_conv2),self.W_conv3T.assign(self.W_conv3),self.b_conv3T.assign(self.b_conv3),self.W_fc1T.assign(self.W_fc1),self.b_fc1T.assign(self.b_fc1),self.W_fc2T.assign(self.W_fc2),self.b_fc2T.assign(self.b_fc2)]</div><div class="line"></div><div class="line">		self.createTrainingMethod()</div><div class="line"></div><div class="line">		<span class="comment"># saving and loading networks</span></div><div class="line">		self.saver = tf.train.Saver()</div><div class="line">		self.session = tf.InteractiveSession()</div><div class="line">		self.session.run(tf.initialize_all_variables())</div><div class="line">		checkpoint = tf.train.get_checkpoint_state(<span class="string">"saved_networks"</span>)</div><div class="line">		<span class="keyword">if</span> checkpoint <span class="keyword">and</span> checkpoint.model_checkpoint_path:</div><div class="line">				self.saver.restore(self.session, checkpoint.model_checkpoint_path)</div><div class="line">				<span class="keyword">print</span> <span class="string">"Successfully loaded:"</span>, checkpoint.model_checkpoint_path</div><div class="line">		<span class="keyword">else</span>:</div><div class="line">				<span class="keyword">print</span> <span class="string">"Could not find old network weights"</span></div><div class="line"></div><div class="line"></div><div class="line">	<span class="function"><span class="keyword">def</span> <span class="title">createQNetwork</span><span class="params">(self)</span>:</span></div><div class="line">		<span class="comment"># network weights</span></div><div class="line">		W_conv1 = self.weight_variable([<span class="number">8</span>,<span class="number">8</span>,<span class="number">4</span>,<span class="number">32</span>])</div><div class="line">		b_conv1 = self.bias_variable([<span class="number">32</span>])</div><div class="line"></div><div class="line">		W_conv2 = self.weight_variable([<span class="number">4</span>,<span class="number">4</span>,<span class="number">32</span>,<span class="number">64</span>])</div><div class="line">		b_conv2 = self.bias_variable([<span class="number">64</span>])</div><div class="line"></div><div class="line">		W_conv3 = self.weight_variable([<span class="number">3</span>,<span class="number">3</span>,<span class="number">64</span>,<span class="number">64</span>])</div><div class="line">		b_conv3 = self.bias_variable([<span class="number">64</span>])</div><div class="line"></div><div class="line">		W_fc1 = self.weight_variable([<span class="number">1600</span>,<span class="number">512</span>])</div><div class="line">		b_fc1 = self.bias_variable([<span class="number">512</span>])</div><div class="line"></div><div class="line">		W_fc2 = self.weight_variable([<span class="number">512</span>,self.actions])</div><div class="line">		b_fc2 = self.bias_variable([self.actions])</div><div class="line"></div><div class="line">		<span class="comment"># input layer</span></div><div class="line"></div><div class="line">		stateInput = tf.placeholder(<span class="string">"float"</span>,[<span class="keyword">None</span>,<span class="number">80</span>,<span class="number">80</span>,<span class="number">4</span>])</div><div class="line"></div><div class="line">		<span class="comment"># hidden layers</span></div><div class="line">		h_conv1 = tf.nn.relu(self.conv2d(stateInput,W_conv1,<span class="number">4</span>) + b_conv1)</div><div class="line">		h_pool1 = self.max_pool_2x2(h_conv1)</div><div class="line"></div><div class="line">		h_conv2 = tf.nn.relu(self.conv2d(h_pool1,W_conv2,<span class="number">2</span>) + b_conv2)</div><div class="line"></div><div class="line">		h_conv3 = tf.nn.relu(self.conv2d(h_conv2,W_conv3,<span class="number">1</span>) + b_conv3)</div><div class="line"></div><div class="line">		h_conv3_flat = tf.reshape(h_conv3,[<span class="number">-1</span>,<span class="number">1600</span>])</div><div class="line">		h_fc1 = tf.nn.relu(tf.matmul(h_conv3_flat,W_fc1) + b_fc1)</div><div class="line"></div><div class="line">		<span class="comment"># Q Value layer</span></div><div class="line">		QValue = tf.matmul(h_fc1,W_fc2) + b_fc2</div><div class="line"></div><div class="line">		<span class="keyword">return</span> stateInput,QValue,W_conv1,b_conv1,W_conv2,b_conv2,W_conv3,b_conv3,W_fc1,b_fc1,W_fc2,b_fc2</div><div class="line"></div><div class="line">	<span class="function"><span class="keyword">def</span> <span class="title">copyTargetQNetwork</span><span class="params">(self)</span>:</span></div><div class="line">		self.session.run(self.copyTargetQNetworkOperation)</div><div class="line"></div><div class="line">	<span class="function"><span class="keyword">def</span> <span class="title">createTrainingMethod</span><span class="params">(self)</span>:</span></div><div class="line">		self.actionInput = tf.placeholder(<span class="string">"float"</span>,[<span class="keyword">None</span>,self.actions])</div><div class="line">		self.yInput = tf.placeholder(<span class="string">"float"</span>, [<span class="keyword">None</span>])</div><div class="line">		Q_Action = tf.reduce_sum(tf.mul(self.QValue, self.actionInput), reduction_indices = <span class="number">1</span>)</div><div class="line">		self.cost = tf.reduce_mean(tf.square(self.yInput - Q_Action))</div><div class="line">		self.trainStep = tf.train.AdamOptimizer(<span class="number">1e-6</span>).minimize(self.cost)</div><div class="line"></div><div class="line"></div><div class="line">	<span class="function"><span class="keyword">def</span> <span class="title">trainQNetwork</span><span class="params">(self)</span>:</span></div><div class="line">		<span class="comment"># Step 1: obtain random minibatch from replay memory</span></div><div class="line">		minibatch = random.sample(self.replayMemory,BATCH_SIZE)</div><div class="line">		state_batch = [data[<span class="number">0</span>] <span class="keyword">for</span> data <span class="keyword">in</span> minibatch]</div><div class="line">		action_batch = [data[<span class="number">1</span>] <span class="keyword">for</span> data <span class="keyword">in</span> minibatch]</div><div class="line">		reward_batch = [data[<span class="number">2</span>] <span class="keyword">for</span> data <span class="keyword">in</span> minibatch]</div><div class="line">		nextState_batch = [data[<span class="number">3</span>] <span class="keyword">for</span> data <span class="keyword">in</span> minibatch]</div><div class="line"></div><div class="line">		<span class="comment"># Step 2: calculate y</span></div><div class="line">		y_batch = []</div><div class="line">		QValue_batch = self.QValueT.eval(feed_dict=&#123;self.stateInputT:nextState_batch&#125;)</div><div class="line">		<span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,BATCH_SIZE):</div><div class="line">			terminal = minibatch[i][<span class="number">4</span>]</div><div class="line">			<span class="keyword">if</span> terminal:</div><div class="line">				y_batch.append(reward_batch[i])</div><div class="line">			<span class="keyword">else</span>:</div><div class="line">				y_batch.append(reward_batch[i] + GAMMA * np.max(QValue_batch[i]))</div><div class="line"></div><div class="line">		self.trainStep.run(feed_dict=&#123;</div><div class="line">			self.yInput : y_batch,</div><div class="line">			self.actionInput : action_batch,</div><div class="line">			self.stateInput : state_batch</div><div class="line">			&#125;)</div><div class="line"></div><div class="line">		<span class="comment"># save network every 100000 iteration</span></div><div class="line">		<span class="keyword">if</span> self.timeStep % <span class="number">10000</span> == <span class="number">0</span>:</div><div class="line">			self.saver.save(self.session, <span class="string">'saved_networks/'</span> + <span class="string">'network'</span> + <span class="string">'-dqn'</span>, global_step = self.timeStep)</div><div class="line"></div><div class="line">		<span class="keyword">if</span> self.timeStep % UPDATE_TIME == <span class="number">0</span>:</div><div class="line">			self.copyTargetQNetwork()</div><div class="line"></div><div class="line"></div><div class="line">	<span class="function"><span class="keyword">def</span> <span class="title">setPerception</span><span class="params">(self,nextObservation,action,reward,terminal)</span>:</span></div><div class="line">		<span class="comment">#newState = np.append(nextObservation,self.currentState[:,:,1:],axis = 2)</span></div><div class="line">		newState = np.append(self.currentState[:,:,<span class="number">1</span>:],nextObservation,axis = <span class="number">2</span>)</div><div class="line">		self.replayMemory.append((self.currentState,action,reward,newState,terminal))</div><div class="line">		<span class="keyword">if</span> len(self.replayMemory) &gt; REPLAY_MEMORY:</div><div class="line">			self.replayMemory.popleft()</div><div class="line">		<span class="keyword">if</span> self.timeStep &gt; OBSERVE:</div><div class="line">			<span class="comment"># Train the network</span></div><div class="line">			self.trainQNetwork()</div><div class="line"></div><div class="line">		<span class="comment"># print info</span></div><div class="line">		state = <span class="string">""</span></div><div class="line">		<span class="keyword">if</span> self.timeStep &lt;= OBSERVE:</div><div class="line">			state = <span class="string">"observe"</span></div><div class="line">		<span class="keyword">elif</span> self.timeStep &gt; OBSERVE <span class="keyword">and</span> self.timeStep &lt;= OBSERVE + EXPLORE:</div><div class="line">			state = <span class="string">"explore"</span></div><div class="line">		<span class="keyword">else</span>:</div><div class="line">			state = <span class="string">"train"</span></div><div class="line"></div><div class="line">		<span class="keyword">print</span> <span class="string">"TIMESTEP"</span>, self.timeStep, <span class="string">"/ STATE"</span>, state, \</div><div class="line">            <span class="string">"/ EPSILON"</span>, self.epsilon</div><div class="line"></div><div class="line">		self.currentState = newState</div><div class="line">		self.timeStep += <span class="number">1</span></div><div class="line"></div><div class="line">	<span class="function"><span class="keyword">def</span> <span class="title">getAction</span><span class="params">(self)</span>:</span></div><div class="line">		QValue = self.QValue.eval(feed_dict= &#123;self.stateInput:[self.currentState]&#125;)[<span class="number">0</span>]</div><div class="line">		action = np.zeros(self.actions)</div><div class="line">		action_index = <span class="number">0</span></div><div class="line">		<span class="keyword">if</span> self.timeStep % FRAME_PER_ACTION == <span class="number">0</span>:</div><div class="line">			<span class="keyword">if</span> random.random() &lt;= self.epsilon:</div><div class="line">				action_index = random.randrange(self.actions)</div><div class="line">				action[action_index] = <span class="number">1</span></div><div class="line">			<span class="keyword">else</span>:</div><div class="line">				action_index = np.argmax(QValue)</div><div class="line">				action[action_index] = <span class="number">1</span></div><div class="line">		<span class="keyword">else</span>:</div><div class="line">			action[<span class="number">0</span>] = <span class="number">1</span> <span class="comment"># do nothing</span></div><div class="line"></div><div class="line">		<span class="comment"># change episilon</span></div><div class="line">		<span class="keyword">if</span> self.epsilon &gt; FINAL_EPSILON <span class="keyword">and</span> self.timeStep &gt; OBSERVE:</div><div class="line">			self.epsilon -= (INITIAL_EPSILON - FINAL_EPSILON)/EXPLORE</div><div class="line"></div><div class="line">		<span class="keyword">return</span> action</div><div class="line"></div><div class="line">	<span class="function"><span class="keyword">def</span> <span class="title">setInitState</span><span class="params">(self,observation)</span>:</span></div><div class="line">		self.currentState = np.stack((observation, observation, observation, observation), axis = <span class="number">2</span>)</div><div class="line"></div><div class="line">	<span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(self,shape)</span>:</span></div><div class="line">		initial = tf.truncated_normal(shape, stddev = <span class="number">0.01</span>)</div><div class="line">		<span class="keyword">return</span> tf.Variable(initial)</div><div class="line"></div><div class="line">	<span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(self,shape)</span>:</span></div><div class="line">		initial = tf.constant(<span class="number">0.01</span>, shape = shape)</div><div class="line">		<span class="keyword">return</span> tf.Variable(initial)</div><div class="line"></div><div class="line">	<span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(self,x, W, stride)</span>:</span></div><div class="line">		<span class="keyword">return</span> tf.nn.conv2d(x, W, strides = [<span class="number">1</span>, stride, stride, <span class="number">1</span>], padding = <span class="string">"SAME"</span>)</div><div class="line"></div><div class="line">	<span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span><span class="params">(self,x)</span>:</span></div><div class="line">		<span class="keyword">return</span> tf.nn.max_pool(x, ksize = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding = <span class="string">"SAME"</span>)</div></pre></td></tr></table></figure>
<p>下面是使用上面的DQN让agent玩Gym的Breakout-v0游戏。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># -------------------------</span></div><div class="line"><span class="comment"># Project: Deep Q-Learning on Breakout-v0</span></div><div class="line"><span class="comment"># Author: Houjiang Chen</span></div><div class="line"><span class="comment"># Date: 2017.4.25</span></div><div class="line"><span class="comment"># -------------------------</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> cv2</div><div class="line"><span class="keyword">import</span> gym</div><div class="line"><span class="keyword">from</span> BrainDQN_Nature <span class="keyword">import</span> BrainDQN</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="comment"># preprocess raw image to 80*80 gray image</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess</span><span class="params">(observation)</span>:</span></div><div class="line">    observation = cv2.cvtColor(cv2.resize(observation, (<span class="number">80</span>, <span class="number">80</span>)), cv2.COLOR_BGR2GRAY)</div><div class="line">    <span class="comment">#ret, observation = cv2.threshold(observation, 1, 255, cv2.THRESH_BINARY)</span></div><div class="line">    <span class="keyword">return</span> np.reshape(observation, (<span class="number">80</span>, <span class="number">80</span>, <span class="number">1</span>))</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">play</span><span class="params">()</span>:</span></div><div class="line">    env = gym.make(<span class="string">'Breakout-v0'</span>)</div><div class="line">    actions = env.action_space.n</div><div class="line"></div><div class="line">    <span class="comment"># init BrainDQN</span></div><div class="line">    brain = BrainDQN(actions)</div><div class="line"></div><div class="line">    <span class="keyword">while</span> <span class="number">1</span>:</div><div class="line">        state = env.reset()</div><div class="line">        state = cv2.cvtColor(cv2.resize(state, (<span class="number">80</span>, <span class="number">80</span>)), cv2.COLOR_BGR2GRAY)</div><div class="line">        <span class="comment">#ret, state = cv2.threshold(state, 1, 255, cv2.THRESH_BINARY)</span></div><div class="line">        brain.setInitState(state)</div><div class="line">        <span class="keyword">while</span> <span class="number">1</span>:</div><div class="line">            action = brain.getAction()</div><div class="line">            state, reward, terminal, _ = env.step(np.argmax(action))</div><div class="line">            env.render()</div><div class="line">            <span class="keyword">if</span> terminal:</div><div class="line">                <span class="keyword">break</span></div><div class="line">            state = preprocess(state)</div><div class="line">            brain.setPerception(state, action, reward, terminal)</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></div><div class="line">    play()</div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    main()</div></pre></td></tr></table></figure>
<h2 id="参考资料">参考资料</h2>
<p>1、Reinforcement Learning: An Introduction, Richard S. Sutton and Andrew G. Barto，2012<br>
2、Playing Atari with Deep Reinforcement Learning，DeepMind Technologies，Arxiv 2013.12<br>
3、Human-level control through deep reinforcement learning，DeepMind Technologies，Nature 2015.02<br>
4、DeepMind官网 https://deepmind.com/blog/deep-reinforcement-learning<br>
5、https://www.nervanasys.com/demystifying-deep-reinforcement-learning<br>
6、http://www.cnblogs.com/jinxulin/p/3511298.html<br>
7、Introduction to Reinforcement Learning，David Silver</p>
]]></content>
      
        <categories>
            
            <category> reinforcement learning </category>
            
        </categories>
        
        
        <tags>
            
            <tag> reinforcement learning </tag>
            
            <tag> machine learning </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[值函数的贝尔曼公式推导]]></title>
      <url>https://hjchen2.github.io/2017/04/10/%E5%80%BC%E5%87%BD%E6%95%B0%E7%9A%84%E8%B4%9D%E5%B0%94%E6%9B%BC%E5%85%AC%E5%BC%8F%E6%8E%A8%E5%AF%BC/</url>
      <content type="html"><![CDATA[<p>下面的推导过程中第2步和第5步两次用到重期望公式: <span class="math inline">\(\bf{EX}=\bf{E\left(E\left[X\mid Y\right]\right)}\)</span>。</p>
<p><span class="math display">\[\begin{split}
\upsilon_{\pi}(s)&amp;={\bf{E_{\pi}}}\left[G_{t}\mid{S_{t}=s}\right] \\
&amp;={\bf{E_{\pi}}}\left({\bf{E_{\pi}}}\left[G_t\mid S_t=s,A_t\right]\right) \\
&amp;={\bf{E_{\pi}}}\left[\sum_a\pi(a|s)G_t\mid S_t=s,A_t=a\right] \\
&amp;=\sum_a\pi(a|s){\bf{E_{\pi}}}\left[G_t\mid S_t=s,A_t=a\right] \\
&amp;=\sum_a\pi(a|s){\bf{E_{\pi}}}\left({\bf{E_{\pi}}}\left[G_t\mid S_t=s,A_t=a,S_{t+1}\right]\right) \\
&amp;=\sum_a\pi(a|s){\bf{E_{\pi}}}\left[\sum_{s^{&#39;}}p(s^{&#39;}\mid s,a)G_t\mid S_t=s,A_t=a,S_{t+1}=s^{&#39;}\right] \\
&amp;=\sum_a\pi(a|s)\sum_{s^{&#39;}}p(s^{&#39;}\mid s,a){\bf{E_{\pi}}}\left[G_t\mid S_t=s,A_t=a,S_{t+1}=s^{&#39;}\right] \\
&amp;=\sum_{a}\pi(a\mid{s})\sum_{s^{&#39;}}p(s^{&#39;}\mid s,a){\bf E}_{\pi}\left[R_{t+1}+\gamma\sum_{k=0}^{\infty}\gamma^{k}R_{t+k+2}\mid{S_{t}=s,A_{t}=a,S_{t+1}=s^{&#39;}}\right] \\
&amp;=\sum_{a}\pi(a\mid{s})\sum_{s^{&#39;}}p(s^{&#39;}\mid{s,a})\left[r(s,a,s^{&#39;})+\gamma{\bf E}_{\pi}\left[\sum_{k=0}^{\infty}\gamma^{k}R_{t+k+2}\mid{S_{t+1}=s^{&#39;}}\right]\right] \\
&amp;=\sum_{a}\pi(a\mid{s})\sum_{s^{&#39;}}p(s^{&#39;}\mid{s,a})\left[r(s,a,s^{&#39;})+\gamma\upsilon_{\pi}(s^{&#39;})\right]
\end{split}\]</span></p>
]]></content>
      
        <categories>
            
            <category> reinforcement learning </category>
            
        </categories>
        
        
        <tags>
            
            <tag> reinforcement learning </tag>
            
            <tag> machine learning，贝尔曼公式推导 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[强化学习（一）]]></title>
      <url>https://hjchen2.github.io/2017/03/27/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88%E4%B8%80%EF%BC%89/</url>
      <content type="html"><![CDATA[<h2 id="前言">前言</h2>
<p>近几年，由于DeepMind成功地将强化学习（reinforcement learning）运用在AlphaGo上，机器首次在复杂任务上取得了超过人类的表现，使得强化学习成为目前机器学习研究的前沿方向之一。强化学习由来已久，Sutton等在1979年就已经开始研究强化学习，1998年出版了强化学习介绍一书，并于2012年发布第二版，本文前几部分内容主要参考该书。</p>
<p>强化学习最早主要用于智能控制领域，比如机器人控制、电梯调度、电信通讯等，如今已经在自动驾驶、NLP、内容推荐<sup>[4]</sup>和语音交互领域都有相关的应用。2013年底DeepMind发表文章Playing Atari with Deep Reinforcement Learning，首次成功地将深度学习运用到强化学习任务上，通过无监督学习实现从纯图像输入来玩Atari 2600游戏的效果。而后DeepMind逐渐改进算法，使得DQN在Atari几乎一半的游戏中超过人类水平，以至2016年AlphaGo和无人车的出现，人们惊奇地发现人工智能即将颠覆我们的生活，甚至有人评论说传统的深度学习已经可以很好地感知理解了，强化学习可以利用这些感知生成策略，因而可以创造更高的机器智能。</p>
<p>下面是DeepMind使用DQN让机器学习玩Atari 2600游戏的视频。 <iframe width="895" height="503" src="https://www.youtube.com/embed/TmPfTpjtdgg" frameborder="0" allowfullscreen></iframe></p>
<h2 id="什么是强化学习">什么是强化学习</h2>
<p>Reinforcement learning is learning what to do—how to map situations to actions—so as to maximize a numerical reward signal<sup>[1]</sup>.</p>
<p>强化学习研究的是智能体agent与环境之间交互的任务，也就是让agent像人类一样通过试错，不断地学习在不同的环境下做出最优的动作，而不是有监督地直接告诉agent在什么环境下应该做出什么动作。在这里我们需要引入回报（reward）这个概念，回报是执行一个动作或一系列动作后得到的奖励，比如在游戏超级玛丽中，向上跳可以获得一个金币，也就是回报值为1，而不跳时回报就是0。回报又分为立即回报和长期回报，立即回报指的是执行当前动作后能立刻获得的奖励，但很多时候我们执行一个动作后并不能立即得到回报，而是在游戏结束时才能返回一个回报值，这就是长期回报。强化学习唯一的准则就是学习通过一序列的最优动作，获得最大的长期回报。比较有挑战性的是，任一状态下做出的动作不仅影响当前状态的立即回报，而且也会影响到下一个状态，因此也就会影响整个执行过程的回报。</p>
<p>因此，强化学习和监督学习的区别主要有以下两点<sup>[6]</sup>：</p>
<ol style="list-style-type: decimal">
<li>强化学习是试错学习(Trail-and-error)，由于没有直接的指导信息，智能体要以不断与环境进行交互，通过试错的方式来获得最佳策略。</li>
<li>延迟回报，强化学习的指导信息很少，而且往往是在事后（最后一个状态）才给出的，这就导致了一个问题，就是获得正回报或者负回报以后，如何将回报分配给前面的状态。</li>
</ol>
<h2 id="问题描述与mdp">问题描述与MDP</h2>
<p>前面已经提到强化学习是尝试并发现回报最大动作的过程，下面就具体来描述一下这个过程。首先考虑一个问题，一个之前完全没有接触过国际象棋的小白怎样和一个专业棋手对弈。刚开始小白对棋面并没有任何概念，只能随机下，但假设双方每一轮下完后都会得到立即回报，比如吃子回报为1，被吃回报为-1，其他回报为0。可以想象一开始小白会输得很惨，但如果小白很聪明，随着不断地尝试小白不仅理解了下棋的规则，并且知道在什么棋面下做出什么动作可以吃更多的棋子。在这里我们将小白作为我们的智能体agent，棋面就是状态，下棋就是agent根据当前状态做出的动作，每个动作执行完后都会引起状态改变，如果状态的改变只与前一个状态和当前的动作有关，而与之前的状态和动作无关（即满足马尔可夫性），那么整个过程可以用马尔可夫决策过程（Markov Decision Processes）来描述，而Sutton在书中直接将满足马尔可夫性的强化学习任务定义为马尔可夫决策过程，并将状态和动作都是有限空间的MDP定义为有限马尔可夫决策过程（finite MDP）。</p>
<p>下面引入一些定义<sup>[1]</sup>：马尔可夫决策过程是一个agent与环境交互的过程，因此有一个离散的时间序列，<span class="math inline">\(t=0,1,2,3,...\)</span>，在每一个时刻<span class="math inline">\(t\)</span>，agent都会接收一个用来表示环境的状态<span class="math inline">\(S_{t}\in\bf{S}\)</span>，其中<span class="math inline">\(\bf{S}\)</span>表示所有可能状态的集合，并且在状态的基础上选择一个动作<span class="math inline">\(A_{t}\in{\bf{A}}(S_{t})\)</span>，其中<span class="math inline">\({\bf{A}}(S_{t})\)</span>表示在状态<span class="math inline">\(S_{t}\)</span>时所有可能采取的动作的集合，在<span class="math inline">\(t\)</span>时刻agent采取一个动作后都会收到一个回报值<span class="math inline">\(R_{t+1}\in\bf{R}\)</span>，然后接收一个新状态<span class="math inline">\(S_{t+1}\)</span>。下图为整个过程的示意图。</p>
<div align="center">
<img src="https://github.com/hjchen2/personal/blob/master/blog/pictures/19e9ab8239fe6be8a413990a592b83c2.png?raw=true" width="600">
</div>
<p><br> 在任意时刻和状态下，agent都可以选择一个动作，选择的依据就是我们说的策略—即状态到动作的映射<span class="math inline">\(\pi(a\mid{s})\)</span>，而一个使得在任意时刻和状态下的长期回报都是最大的策略是我们最终需要得到的。所谓长期回报我们可以用每个时刻的立即回报来表示：</p>
<p><span class="math inline">\(G_{t}=R_{t+1}+R_{t+2}+R_{t+3}+...=\sum_{k=t+1}^{\infty}R_{k}\tag{1.1}\)</span></p>
<p>但实际上我们一般会用下面更通用的公式来代替：</p>
<p><span class="math inline">\(G_{t}=R_{t+1}+\gamma{R_{t+2}}+\gamma^2{R_{t+3}}+...+\gamma^{T-t-1}{R_{T}}=\sum_{k=0}^{T-t-1}\gamma^{k}R_{t+k+1}\tag{1.2}\)</span></p>
<p>其中<span class="math inline">\(\gamma\in[0,1]\)</span>称为回报折扣因子，表明了未来的回报相对于当前回报的重要程度。<span class="math inline">\(\gamma=0\)</span>时，相当于只考虑立即回报不考虑长期回报，<span class="math inline">\(\gamma=1\)</span>时，将长期回报和立即回报看得同等重要。<span class="math inline">\(T\in[1,\infty]\)</span>表示完成一次实验过程的总步数，<span class="math inline">\(T=\infty\)</span>和<span class="math inline">\(\gamma=1\)</span>不能同时满足，否则长期回报将无法收敛。特别地，我们将一次有限步数的实验称作一个单独的episodes，也就是经过有限步数后最终会接收一个终止状态，这一类的任务也叫做episodic tasks。下面讨论的强化学习任务都是有限MDP的episodic tasks。</p>
<h3 id="马尔可夫决策过程">马尔可夫决策过程</h3>
<p>一个有限马尔可夫决策过程由一个四元组构成 <span class="math inline">\(M=({\bf{S}}, {\bf{A}}, {\bf{P}}, {\bf{R}})\)</span><sup>[6]</sup>。如上所述，<span class="math inline">\(\bf{S}\)</span>表示状态集空间，<span class="math inline">\({\bf{A}}\)</span>表示动作集空间，<span class="math inline">\({\bf{P}}\)</span>表示状态转移概率矩阵，<span class="math inline">\({\bf{R}}\)</span>表示期望回报值。</p>
<p>在MDP中给定任何一个状态<span class="math inline">\(s\in\bf{S}\)</span>和动作<span class="math inline">\(a\in\bf{A}\)</span>，都会以某个概率转移到下一个状态<span class="math inline">\(s^{&#39;}\)</span>，这个概率为<span class="math inline">\(p(s^{&#39;}\mid s, a)={\bf{Pr}}\{S_{t+1}=s^{&#39;}\mid S_{t}=s, A_{t}=a\}\in\bf{P}\)</span>，并获得下一个回报的期望值为<span class="math inline">\(r(s,a,s^{&#39;})={\bf{E}}\left[R_{t+1}\mid{S_{t}=s,A_{t}=a,S_{t+1}=s^{&#39;}}\right]\in\bf{R}\)</span>。</p>
<h3 id="值函数及贝尔曼公式">值函数及贝尔曼公式</h3>
<p>增强学习的最终结果是找到一个环境到动作的映射—即策略<span class="math inline">\(\pi(a\mid{s})\)</span>。如果一个策略只考虑立即回报，那么很可能就会掉入眼前陷阱。比如说有一个岔路口，往左回报是100，往右回报是10，如果策略只考虑立即回报，那肯定是往左，但往左走的下一次回报只有10，而往右走的下一次回报有200，可以看到这个策略并不是最优的策略，此外增强学习又往往有具有延迟回报的特点，在很多情况下的动作并不会产生立即回报，但这一系列动作的累积效果又的确会导致后续回报的产生，因此立即回报并不能说明策略的好坏。在几乎所有的强化学习理论中都会定义值函数来表示给定策略下期望的未来回报，并将值函数作为评估学习效果的指标。</p>
<p>值函数有多种定义，目前常见的是将值函数直接定义为未来回报的期望：</p>
<p><span class="math inline">\(\upsilon_{\pi}(s)={\bf{E_{\pi}}}\left[G_{t}\mid{S_{t}=s}\right]={\bf{E_{\pi}}}\left[\sum_{k=0}^{\infty}\gamma^{k}R_{t+k+1}\mid{S_{t}=s}\right]\tag{2.1}\)</span></p>
<p>上面表示的是在某个策略<span class="math inline">\(\pi\)</span>下，当环境处于状态<span class="math inline">\(s\)</span>时未来回报的期望，因此又叫做状态值函数(state-value function for policy)，只跟当前状态有关。同样，我们也可以定义动作值函数(action-value function for policy)，如下：</p>
<p><span class="math inline">\(q_{\pi}(s,a)={\bf{E_{\pi}}}\left[G_{t}\mid{S_{t}=s,A_{t}=a}\right]={\bf{E_{\pi}}}\left[\sum_{k=0}^{\infty}\gamma^{k}R_{t+k+1}\mid{S_{t}=s,A_{t}=a}\right]\tag{2.2}\)</span></p>
动作值函数表示在某个策略<span class="math inline">\(\pi\)</span>下，当环境处于状态<span class="math inline">\(s\)</span>时采取动作<span class="math inline">\(a\)</span>的未来回报的期望。可以看到动作值函数与状态值函数唯一的不同是动作值函数不仅指定了一个初始状态，而且也指定了初始动作，而状态值函数的初始动作是根据策略产生的。由于在MDP中，给定状态<span class="math inline">\(s\)</span>，agent根据策略选择动作<span class="math inline">\(a\)</span>，下个时刻将以概率<span class="math inline">\(p(s^{&#39;}\mid{s,a})\)</span>转移到状态<span class="math inline">\(s^{&#39;}\)</span>，因此值函数又可以改写成如下形式：
<div align="center">
<span class="math display">\[\begin{split}
\upsilon_{\pi}(s)&amp;={\bf{E_{\pi}}}\left[G_{t}\mid{S_{t}=s}\right] \\
&amp;={\bf{E_{\pi}}}\left[\sum_{k=0}^{\infty}\gamma^{k}R_{t+k+1}\mid{S_{t}=s}\right] \\
&amp;={\bf{E_{\pi}}}\left[R_{t+1}+\gamma\sum_{k=0}^{\infty}\gamma^{k}R_{t+k+2}\mid{S_{t}=s}\right] \\
&amp;=\sum_{a}\pi(a\mid{s})\cdot{\bf E}_{\pi}\left[R_{t+1}+\gamma\sum_{k=0}^{\infty}\gamma^{k}R_{t+k+2}\mid{S_{t}=s,A_{t}}\right] \\
&amp;=\sum_{a}\pi(a\mid{s})\sum_{s^{&#39;}}p(s^{&#39;}\mid{s,a})\left[r(s,a,s^{&#39;})+\gamma{\bf E}_{\pi}\left[\sum_{k=0}^{\infty}\gamma^{k}R_{t+k+2}\mid{S_{t+1}=s^{&#39;}}\right]\right] \\
&amp;=\sum_{a}\pi(a\mid{s})\sum_{s^{&#39;}}p(s^{&#39;}\mid{s,a})\left[r(s,a,s^{&#39;})+\gamma\upsilon_{\pi}(s^{&#39;})\right]
\end{split}\tag{2.3}\]</span>
</div>
<p>也就是说在策略<span class="math inline">\(\pi\)</span>下当前状态的值函数可以通过下一个状态的值函数来迭代求解，这个公式被称为<span class="math inline">\(\upsilon_{\pi}\)</span>的贝尔曼公式（Bellman equation for <span class="math inline">\(\upsilon_{\pi}\)</span>）。</p>
同样，动作值函数也可以写成相似的形式：
<div align="center">
<span class="math display">\[\begin{split}
q_{\pi}(s,a)&amp;={\bf{E_{\pi}}}\left[G_{t}\mid{S_{t}=s,A_{t}=a}\right] \\
&amp;={\bf{E_{\pi}}}\left[R_{t+1}+\gamma\sum_{k=0}^{\infty}\gamma^{k}R_{t+k+2}\mid{S_{t}=s,A_{t}=a}\right] \\
&amp;=\sum_{s^{&#39;}}p(s^{&#39;}\mid{s,a})\left[r(s,a,s^{&#39;})+\gamma\upsilon_{\pi}(s^{&#39;})\right]
\end{split}\tag{2.4}\]</span>
</div>
<p><span class="math inline">\(\upsilon_{\pi}(s)\)</span>也可以用<span class="math inline">\(q_{\pi}(s,a)\)</span>来表示： <span class="math display">\[\upsilon_{\pi}(s)=\sum_{a}\pi(a\mid{s})q_{\pi}(s,a)\tag{2.5}\]</span></p>
下面是迭代计算<span class="math inline">\(\upsilon_{\pi}(s)\)</span>和<span class="math inline">\(q_{\pi}(s,a)\)</span>的图解<sup>[1]</sup>，可以与上述公式对照理解。
<div align="center">
<img src="https://github.com/hjchen2/personal/blob/master/blog/pictures/205fd62a7177a841cdc79585cf1ed6ae.png?raw=true" width="600">
</div>
<h3 id="最优值函数及贝尔曼最优公式">最优值函数及贝尔曼最优公式</h3>
<p>上面所说的值函数都是未来回报的期望值，而我们需要得到的最优策略必然是使得任意时刻未来回报的期望值都是最大的，也就是说我们的优化目标可以表示为：<span class="math display">\[\pi_{*}=\mathop{\arg\max}_{\mathbf{\pi}}\upsilon_{\pi}(s)\tag{2.6}\]</span> 当然最优策略可能不止一个，但这些最优策略都有一个共同的特点，就是它们共享同样的状态值函数，这个状态值函数叫做最优状态值函数（optimal state-value function），用<span class="math inline">\(\upsilon_{*}\)</span>来表示。对于所有的<span class="math inline">\(s\in\bf{S}\)</span>， <span class="math display">\[\upsilon_{*}(s)=\max_{\mathbf{\pi}}\upsilon_{\pi}(s)\tag{2.7}\]</span> 最优策略同样也共享相同的动作值函数（optimal action-value function），用<span class="math inline">\(q_{*}\)</span>来表示。对于所有的<span class="math inline">\(s\in\bf{S}\)</span>，<span class="math inline">\(a\in{\bf{A}}(s)\)</span>， <span class="math display">\[q_{*}(s,a)=\max_{\mathbf{\pi}}q_{\pi}(s,a)\tag{2.8}\]</span> 回顾一下上面动作值函数的改写公式(2.4)，<span class="math inline">\(q_{\pi}(s,a)=\sum_{s^{&#39;}}p(s^{&#39;}\mid{s,a})\left[r(s,a,s^{&#39;})+\gamma\upsilon_{\pi}(s^{&#39;})\right]\)</span>，由于动作值函数表示的是给定初始动作，后面的动作遵循策略<span class="math inline">\(\pi\)</span>，因此最优动作值函数后面的动作应当遵循最优策略<span class="math inline">\(\pi_{*}\)</span>，不难得到下面的公式。 <span class="math display">\[q_{*}(s,a)=\sum_{s^{&#39;}}p(s^{&#39;}\mid{s,a})\left[r(s,a,s^{&#39;})+\gamma\upsilon_{*}(s^{&#39;})\right]\tag{2.9}\]</span> 至此，最优值函数的形式已经给出了，现在我们继续回顾一下公式(2.5)的意义，<span class="math inline">\(\upsilon_{\pi}(s)\)</span>的值是<span class="math inline">\(q_{\pi}(s,a)\)</span>的期望，那么必然存在<span class="math inline">\(\upsilon_{\pi}(s)\leq \max q_{\pi}(s,a)\)</span>。但对于最优策略来说， <span class="math display">\[\begin{split}\upsilon_{*}(s)&amp;=\max_{\mathbf{a}} q_{*}(s,a) \\
&amp;=\max_{\mathbf{a}}\sum_{s^{&#39;}}p(s^{&#39;}\mid{s,a})\left[r(s,a,s^{&#39;})+\gamma\upsilon_{*}(s^{&#39;})\right]
\end{split}\tag{2.10}\]</span> <span class="math display">\[q_{*}(s,a)=\sum_{s^{&#39;}}p(s^{&#39;}\mid{s,a})\left[r(s,a,s^{&#39;})+\gamma\max_{\mathbf{a^{&#39;}}}q_{*}(s^{&#39;},a^{&#39;})\right]\tag{2.11}\]</span> 与状态值函数的贝尔曼公式一样，最优状态值函数和最优动作值函数也可以表示成递归的形式，因此公式(2.10)和公式(2.11)又分别叫做状态值函数和动作值函数的贝尔曼最优公式（Bellman optimality equation）。因为没有<span class="math inline">\(\pi(a\mid{s})\)</span>，不需要根据策略生成动作，因此贝尔曼最优公式完全独立于策略，但如果我们已知<span class="math inline">\(\upsilon_{*}\)</span>或<span class="math inline">\(q_{*}\)</span>，都可以很容易地得到最优策略。</p>
<p>如果我们已知<span class="math inline">\(\upsilon_{*}\)</span>，而且在每一步都有多个动作可以选择，可以想到最优策略的<span class="math inline">\(\upsilon_{*}(s)\)</span>必然是满足贝尔曼最优公式的，因此至少有一个动作会满足公式中的最大化条件。任何一个采用上述动作并能够以非零概率转移到下一个状态的策略都是最优策略。我们可以把当前动作的选择看成是一个单步搜索（one-step search）的问题，在某个状态下单步搜索结果最大的动作即最优动作，而每个状态下都采取最优动作的策略即最优策略。如果我们已知<span class="math inline">\(q_{*}\)</span>，那么只需要在每一步都选择使得<span class="math inline">\(q_{*}(s,a)\)</span>最大的动作，就可以得到一个最优策略。</p>
<p>贝尔曼公式与贝尔曼最优公式是MDP求解的基础，下面主要介绍几种MDP求解的方法。</p>
<h2 id="动态规划方法">动态规划方法</h2>
<p>动态规划（dynamic programming）指的是能够用来解决给定环境模型，计算最优策略的算法总称。典型的动态规划算法存在两个问题，一是需要依赖一个非常好的环境状态转移模型，二是计算的开销非常大，因此在增强学习中几乎不会直接用动态规划求解MDP，但动态规划理论还是非常重要的，因为后面的一些算法都是在动态规划的基础上，摆脱模型依赖并尽可能地减少计算量。</p>
<h3 id="策略估计">策略估计</h3>
<p>首先，我们考虑一下如果已知策略<span class="math inline">\(\pi\)</span>，如何来计算<span class="math inline">\(\upsilon_{\pi}\)</span>。这个问题被称作DP迭代中的策略估计（policy evaluation）。</p>
<p>先举一个例子，一个岔路口有向左和向右两个方向，向左回报为10，向右回报为100，我们没有任何先验知识，但我们需要估计站在路口的值函数，也就是估计当前状态的值函数，该如何来估计呢？首先我们将值函数初始化为0，然后进行大量的尝试，每次都以0.5的概率选择方向左，并获得回报10，以0.5的概率选择方向右，获得回报100。那么只要能将这两个方向都至少遍历一遍，就可以得到该状态的值函数<span class="math inline">\(\upsilon_{随机策略}=\frac{1}{N}\sum_{i=0}^{N}{0.5\cdot R_{i}}\)</span>，其中<span class="math inline">\(N\)</span>为实验的总次数。</p>
<p>同样，我们也是采用相似的方法迭代来进行策略估计的。首先将所有的<span class="math inline">\(\upsilon_{\pi}(s)\)</span>都初始化为0（或者任意值，但终止状态必须为0），然后采用如下公式更新所有状态<span class="math inline">\(s\)</span>的值函数。 <span class="math display">\[\begin{split}\upsilon_{k+1}(s) &amp;={\bf{E}}_{\pi}\left[R_{t+1}+\gamma \upsilon_{k}(S_{t+1})\mid S_{t}=s \right] \\
&amp;=\sum_{a}\pi(a\mid{s})\sum_{s^{&#39;}}p(s^{&#39;}\mid{s,a})\left[r(s,a,s^{&#39;})+\gamma\upsilon_{k}(s^{&#39;})\right]
\end{split}\tag{3.1}\]</span> 其中<span class="math inline">\(\upsilon_{k+1}(s)\)</span>表示在当前策略下第<span class="math inline">\(k+1\)</span>次迭代状态<span class="math inline">\(s\)</span>的值函数，<span class="math inline">\(\upsilon_{k}(s^{&#39;})\)</span>表示在当前策略下第<span class="math inline">\(k\)</span>次迭代状态<span class="math inline">\(s^{&#39;}\)</span>的值函数，该公式就是用上一次迭代计算得到的值函数来更新本次迭代的值函数。在具体操作时，又有两种更新方法<sup>[6]</sup>，</p>
<ul>
<li>将第<span class="math inline">\(k\)</span>次迭代计算得到的所有状态值函数<span class="math inline">\(\left[\upsilon_{k}(s_{1}),\upsilon_{k}(s_{2}),\upsilon_{k}(s_{3}),...\right]\)</span>保存在一个数组中，第<span class="math inline">\(k+1\)</span>次迭代的<span class="math inline">\(\upsilon_{k+1}(s)\)</span>使用第<span class="math inline">\(k\)</span>次的<span class="math inline">\(\upsilon_{k}(s^{&#39;})\)</span>进行更新，更新后的值保存在另一个数组中。</li>
<li>仅用一个数组来保存各状态的值函数，每次更新后就将原来的值覆盖。这样在第<span class="math inline">\(k+1\)</span>次迭代时<span class="math inline">\(\upsilon_{k+1}(s)\)</span>就有可能使用的是第<span class="math inline">\(k+1\)</span>次更新后的<span class="math inline">\(\upsilon_{k+1}(s^{&#39;})\)</span>，这样可以及时地利用更新的值函数，收敛更快。</li>
</ul>
下面为整个策略估计的算法过程：
<div align="center">
<img src="https://github.com/hjchen2/personal/blob/master/blog/pictures/19f3246af64a89e7bf38a4d53ea26819.png?raw=true" width="560">
</div>
<h3 id="策略改进">策略改进</h3>
<p>策略估计是为了计算当前策略下各状态的值函数，那得到值函数又有什么用呢？首先我们可以用来比较两个策略的好坏，如果状态值函数是已知的，那么就可以根据公式(2.4)计算动作值函数，如果一个策略<span class="math inline">\(\pi\)</span>的所有动作值函数都大于另一个策略<span class="math inline">\(\pi^{&#39;}\)</span>，那么可以认为策略<span class="math inline">\(\pi\)</span>比策略<span class="math inline">\(\pi^{&#39;}\)</span>更好。其次，最主要的用处是可以用来进行策略改进（policy improvement）。</p>
<p>仍然是上面岔路口的例子，但是假设无论向左还是向右，下一个路口都是唯一且相同的。起初由于没有任何先验知识，因此采用了一个随机策略，然后我们可以计算得到随机策略下的状态值函数，那么我们就可以进行策略改进了。具体的做法就是前面提到的单步搜索，向左时当前动作的回报为10，因此单步搜索的结果为10+<span class="math inline">\(\gamma\upsilon\)</span>，<span class="math inline">\(\upsilon\)</span>为下一个路口的值函数，而向右为100+<span class="math inline">\(\gamma\upsilon\)</span>，因此策略会更新为向右，而不再是随机了，显然策略被改进了。同时我们注意到，单步搜索计算的值正是动作值函数。</p>
<p>根据上面的例子，我们可以总结一下策略改进的方法：遍历所有的状态和所有可能的动作，采用贪婪算法进行策略的更新，即对所有<span class="math inline">\(s\in\bf S\)</span>，</p>
<p><span class="math display">\[\begin{split}\pi^{&#39;}(s)&amp;=\arg\max_{\mathbf{a}}q_{\pi}(s,a)\\
&amp;=\arg\max_{\mathbf{a}}\sum_{s^{&#39;}}p(s^{&#39;}\mid s,a)\left[r(s,a,s^{&#39;})+\gamma\upsilon_{\pi}(s^{&#39;})\right]\end{split}\tag{3.2}
\]</span> 现在我们已经知道如何计算当前策略的状态值函数，也知道可以根据动作值函数来更新策略，那下面就来讲讲如何从零开始求解最优策略。</p>
<h3 id="策略迭代">策略迭代</h3>
一旦策略<span class="math inline">\(\pi\)</span>通过策略改进得到一个更好的策略<span class="math inline">\(\pi^{&#39;}\)</span>，那么我们就可以通过策略估计算法，计算策略<span class="math inline">\(\pi^{&#39;}\)</span>的状态值函数，并用公式(3.2)进行策略改进得到一个比策略<span class="math inline">\(\pi^{&#39;}\)</span>更好的策略<span class="math inline">\(\pi^{&#39;&#39;}\)</span>。如下图所示，经过无数次的策略估计和策略改进后，我们终将会收敛于最优策略<span class="math inline">\(\pi_{*}\)</span>。这种通过不断迭代地去改进策略的方法叫做策略迭代（policy iteration）。
<div align="center">
<img src="https://github.com/hjchen2/personal/blob/master/blog/pictures/c9c7ec7b0709d5492f5e8cb8a6096b7e.png?raw=true" width="600">
</div>
<br> 下面为整个策略迭代的算法过程：
<div align="center">
<img src="https://github.com/hjchen2/personal/blob/master/blog/pictures/1b44935438fee7046950fcfddfd405c0.png?raw=true" width="600">
</div>
<h3 id="值迭代">值迭代</h3>
<p>策略迭代算法需要不断地进行策略估计和策略改进，每次策略估计和改进都需要遍历一次所有的状态和动作，因此算法的计算量非常大，效率非常低。同时可以看到策略迭代的依据是贝尔曼公式，而如果直接利用贝尔曼最优公式会不会加速求解过程呢？事实上是可以的，下面的值迭代（value iteration）算法就是利用贝尔曼最优公式来提高求解效率的一种算法。</p>
<p>我们还是需要先迭代估计状态值函数，但不必每次迭代都进行策略改进。根据贝尔曼最优公式，可以直接用上一次迭代的最大动作值函数对当前迭代的状态值函数进行更新，如下所示：</p>
<p><span class="math display">\[\begin{split}\upsilon_{k+1}(s)&amp;=\max_{\mathbf{a}} q_{k}(s,a) \\
&amp;=\max_{\mathbf{a}}\sum_{s^{&#39;}}p(s^{&#39;}\mid{s,a})\left[r(s,a,s^{&#39;})+\gamma\upsilon_{k}(s^{&#39;})\right]
\end{split}\tag{3.3}\]</span></p>
<p>值迭代算法的好处就是省去了每次迭代时的策略改进过程，并且由于每次迭代得到的<span class="math inline">\(\upsilon_{k+1}(s)\)</span>都要<span class="math inline">\(\geq\)</span>策略迭代得到的<span class="math inline">\(\upsilon_{k+1}(s)\)</span>，也就是说相同迭代次数下，策略迭代得到的策略肯定没有值迭代得到的策略好，因此能大大加快算法收敛。直到值函数收敛到最优值函数后，再通过最优值函数来计算得到最优策略，下面是值迭代算法的完整过程：</p>
<div align="center">
<img src="https://github.com/hjchen2/personal/blob/master/blog/pictures/c94f41587e075ba0ab3af4a82ff99a17.png?raw=true" width="560">
</div>
<p><br> 一般来说值迭代和策略迭代都需要经过无数次迭代才能精确收敛到最优策略， 而实践中我们往往会设定一个阈值<span class="math inline">\(\Delta\)</span>来作为迭代中止条件，即当所有的<span class="math inline">\(\upsilon_{\pi}(s)\)</span>变化量小于<span class="math inline">\(\Delta\)</span>时，我们就近似的认为获得了最优策略。值迭代和策略迭代都可以用来求解最优策略，但是都需要依赖一个现有的环境模型，而对环境进行精确建模往往是非常困难的，所以导致了动态规划方法在MDP求解时几乎不可用，当然如果状态转移是确定性的（<span class="math inline">\(p(s^{&#39;}\mid s,a)=1\)</span>），那就另当别论了。</p>
<h2 id="蒙特卡罗方法">蒙特卡罗方法</h2>
<p>下面我们要讲的是蒙特卡罗方法（Monte Carlo Methods）。与动态规划不同，蒙特卡罗方法不需要知道环境的完整模型，仅仅需要经验就可以获得最优策略，这些经验可以通过与环境在线或模拟交互的方式获得。在线交互显然是不需要任何环境的先验知识，模拟交互虽然需要知道环境状态的转移，但与动态规划不同的是这里不需要知道具体的转移概率。</p>
<p>蒙特卡罗方法也称统计模拟方法，基本思想是通过对大量的重复随机事件进行统计，估计随机事件的概率分布或期望。一个典型的例子是利用蒙特卡罗方法计算圆周率。假设我们知道圆的面积公式为<span class="math inline">\(S=\pi r^{2}\)</span>，那计算圆周率的公式自然就是<span class="math inline">\(\pi = \frac{S}{r^{2}}\)</span>，因此如果我们知道圆面积和圆半径，那么就可以求到圆周率。那么如何计算一个圆的面积呢？给定一个圆，我们可以画出这个圆的外切正方形，那么这个外切正方形的面积为<span class="math inline">\(S_{正方形}=4r^{2}\)</span>，现在我们往正方形区域随机投点，并统计点落在圆内的概率<span class="math inline">\(p\)</span>，那么圆面积可以这么计算：<span class="math inline">\(S_{圆}=p\cdot S_{正方形}\)</span>，因此<span class="math inline">\(\pi=4\cdot p\)</span>。可以想到，如果投点次数越多，<span class="math inline">\(p\)</span>估计越精确，<span class="math inline">\(\pi\)</span>的结果也就越接近真实值。</p>
<h3 id="蒙特卡罗策略估计">蒙特卡罗策略估计</h3>
<p>我们现在来考虑一下如何利用蒙特卡罗方法估计给定策略下的状态值函数。与上面计算圆周率的例子稍有不同的是，现在我们估计的是未来回报的期望，而不是概率，但基本思想是一样的。很显然，如果要估计<span class="math inline">\(\upsilon_{\pi}(s)\)</span>，我们首先需要根据给定策略生成大量的经验数据，然后从中统计从状态<span class="math inline">\(s\)</span>开始的未来回报的平均值，这个平均值就是我们估计的状态值函数。这种利用蒙特卡罗方法进行策略估计的算法又叫做蒙特卡罗策略估计（Monte Carlo Policy Evaluation）。</p>
<p>蒙特卡罗策略估计在具体实现时又分为first-visit MC methods和every-visit MC methods。由于在一个episode中，状态<span class="math inline">\(s\)</span>可能会出现多次，first-visit MC methods就是只统计第一次到达该状态的未来回报，而every-visit MC methods是所有达到该状态的未来回报都会统计累加起来。下面我们举例说明first-visit MC methods的估计方法<sup>[6]</sup>。</p>
<p>现在我们假设有如下一些样本（下图每一行都是在当前策略下的一个独立的episode），紫色实心点为状态<span class="math inline">\(s\)</span>，取折扣因子γ=1，即直接计算累积回报。</p>
<div align="center">
<img src="https://github.com/hjchen2/personal/blob/master/blog/pictures/221402112851854.png?raw=true">
</div>
<p><br> 第一个episode中到达过两次状态<span class="math inline">\(s\)</span>，我们只计算第一次的未来回报<span class="math inline">\(R_{1}(s)=1-2+0+1-3+5=2\)</span>。假设我们已经用相同的方法计算得到<span class="math inline">\(R_{2}(s)=1\)</span>，<span class="math inline">\(R_{3}(s)=-5\)</span>，<span class="math inline">\(R_{4}(s)=4\)</span>。那么当前策略下状态<span class="math inline">\(s\)</span>的值函数</p>
<p><span class="math display">\[\upsilon_{\pi}(s)={\bf E}\left[R(s)\right]=\frac{1}{N}\sum_{i=1}^{N}\left[R_{i}(s)\right]=\frac{1}{4}\left(2+1-5+4\right)=0.5\]</span></p>
<p>同样，如果生成的episode数量越多，<span class="math inline">\(\upsilon_{\pi}(s)\)</span>的估计就越接近真实值，下面是具体的算法流程：</p>
<div align="center">
<img src="https://github.com/hjchen2/personal/blob/master/blog/pictures/079fef1ab5cd0065007ae82d893b0520.png?raw=true" width="560">
</div>
<p><br> 注意这里使用大写的<span class="math inline">\(V\)</span>表示状态值函数的估计，Sutton的理由是状态值函数一旦初始化，就会立即变成一个随机的值了，因为<span class="math inline">\(G\)</span>会随着生成的episode不同而不断变化。可以认为每次<span class="math inline">\(G\)</span>都为<span class="math inline">\(\upsilon_{\pi}(s)\)</span>的一个独立同分布估计，当数据量非常大时<span class="math inline">\(V(s)\)</span>将最终收敛于这个分布的均值。</p>
<h3 id="动作值函数的蒙特卡罗估计">动作值函数的蒙特卡罗估计</h3>
<p>由于我们没有完整的环境状态转移模型，因此即使我们得到当前策略的值函数，根据公式(3.2)也无法进行策略改进。既然我们可以估计得到状态值函数，那么肯定也可以用相同的方法直接估计动作值函数，在这里叫做动作值函数的蒙特卡罗估计（Monte Carlo Estimation of Action Values）。</p>
<p>估计方法跟蒙特卡罗策略估计差不多，只不过我们需要找到所有的状态动作对(pair of state <span class="math inline">\(s\)</span> and action <span class="math inline">\(a\)</span>)，然后统计每一个状态动作对的未来回报的平均值，即<span class="math inline">\(q_{\pi}(s,a)\)</span>的估计值。得到了<span class="math inline">\(q_{\pi}(s,a)\)</span>，我们就可以根据公式(3.2)进行策略改进了。</p>
<h3 id="蒙特卡罗控制">蒙特卡罗控制</h3>
<p>蒙特卡罗控制（Monte Carlo Control）首要的问题就是如何估计最优策略。跟之前动态规划一样，这里也可以采用策略迭代和策略改进交替进行的方式，经过大量的迭代后收敛到最优策略。但蒙特卡罗方法有一个最大的问题，即我们需要产生无数的episode才能保证收敛到最优结果。无数的episode和大量的迭代导致计算量巨大，效率非常低。Sutton在书<sup>[1]</sup>中提到两种解决方法，其中一种方法是采用episode-by-episode的方式进行优化。</p>
<p>episode-by-episode的思想与动态规划中值迭代的in-place版本非常相似。在动态规划的值迭代中，我们每次迭代都直接覆盖更新值函数，因此能及时地利用到更新后的值函数，从而能加快收敛。episode-by-episode则是先用当前策略生成一个episode，然后根据这个episode进行动作值函数的更新，同时更新策略，并利用更新后的策略继续生成后续的episode。</p>
<p>下面是exploring starts的蒙特卡罗控制（Monte Carlo ES，exploring starts指的是从一个随机的开始状态和动作生成一个episode）算法的完整过程：</p>
<div align="center">
<img src="https://github.com/hjchen2/personal/blob/master/blog/pictures/608a1293a52fa134b5168042bf7fd519.png?raw=true" width="560">
</div>
<p><br> 至于为何要使用exploring starts，这与episode-by-episode在线生成episode的更新策略有关。还是上面的岔路口的例子，我们先随机指定一个策略，比如指定向左，那么使用该策略生成一个episode时必然也是向左，那么也就只能更新向左的动作值函数了，而无法更新向右的动作值函数。由于动作值函数是随机初始化的，如果向右的动作值函数初始值小于更新后的向左的动作值函数，那么下一次生成episode时仍然是向左，并且可以想象可能永远不会选择向右。但其实向右才是最优动作，因此上述更新的策略永远不可能是最优策略。但随机选择开始状态和动作，可以避免某些动作的值函数不会更新的问题，因此可以保证能获得最优策略。</p>
<p>当然也可以采用其他方法避免使用exploring starts，下面要介绍的on-policy方法和off-policy方法就是其中的两种方法。</p>
<h3 id="on-policy蒙特卡罗控制">On-Policy蒙特卡罗控制</h3>
<p>前面的Monte Carlo ES算法使用exploring starts是为了保证所有可能的动作值函数都能得到更新，从而保证能获得最优策略。如果策略本身就可以在任何状态下都采取所有可能的动作，而不是贪婪地只选择动作值函数最大的那个，那问题不就迎刃而解了吗。下面要讨论策略是非确定性的，也就是对于所有的状态<span class="math inline">\(s\)</span>和该状态下所有可能的动作<span class="math inline">\(a\)</span>都有<span class="math inline">\(\pi(a\mid s)&gt;0\)</span>，并且用<span class="math inline">\(\epsilon-soft\)</span>策略生成episode。由于我们评估和改进的策略与生成episode的策略是相同的，因此叫做on-policy方法。</p>
<p>在<span class="math inline">\(\epsilon-soft\)</span>策略中，大多数时候策略会选择动作值函数最大的动作（或者换句话说，以<span class="math inline">\(1-\epsilon\)</span>的概率选择动作值函数最大的动作，<span class="math inline">\(\epsilon\)</span>是一个非常小的正数），但也会以概率<span class="math inline">\(\epsilon\)</span>从其他动作中随机挑选一个动作，整体算法流程：</p>
<div align="center">
<img src="https://github.com/hjchen2/personal/blob/master/blog/pictures/a50e2ce4a881eea7b6b1a2a830f2db1d.png?raw=true" width="560">
</div>
<h3 id="off-policy蒙特卡罗控制">Off-Policy蒙特卡罗控制</h3>
<p>在off-policy方法中，生成episode的策略与评估和改进的策略并非同一个策略。其中生成episode的策略我们叫行为策略（behavior policy），而评估和改进的策略叫估计策略（estimation policy）。这种方法的好处是可以使行为策略是<span class="math inline">\(\epsilon-soft\)</span>策略，但估计策略是确定性的。下面只给出算法流程，具体推导请参考Sutton在书中的介绍<sup>[1]</sup>。</p>
<div align="center">
<img src="https://github.com/hjchen2/personal/blob/master/blog/pictures/6f3c3cd1ddbcbfb3fe3df6dc881ce4b8.png?raw=true" width="560">
</div>
<h2 id="时间差分学习">时间差分学习</h2>
<p>时间差分学习（temporal-dierence (TD) learning）结合了动态规划和蒙特卡罗方法的优点，与蒙特卡罗方法一样不需要环境模型，与动态规划一样更新估计值时只依赖于下一个状态可用的估计值，而不需要等到策略自举出完整的episode。</p>
<h3 id="td预测">TD预测</h3>
<p>TD预测（TD prediction）又叫TD策略估计，就是从给定的一系列经验数据中估计出当前策略的状态值函数<span class="math inline">\(\upsilon_{\pi}\)</span>。回顾一下蒙特卡罗控制，我们是先自举一个episode，然后根据历史episode和当前最新的episode计算从状态<span class="math inline">\(s\)</span>开始未来回报的均值，作为当前状态值函数的更新值。对上面更新方式稍做修改，我们可以用一种滑动平均的方法来更新，即只用当前episode的未来回报与状态值函数的差值来更新。一个简单的every-visit MC方法的更新公式就如下所示：</p>
<p><span class="math display">\[V(S_{t})=（1-\alpha）V(S_{t})+\alpha G_{t}=V(S_{t})+\alpha\left[G_{t}-V(S_{t}) \right]\tag{4-1}\]</span></p>
<p><span class="math inline">\(V(S_{t})\)</span>表示第<span class="math inline">\(t\)</span>个时刻为状态<span class="math inline">\(S_{t}\)</span>的状态值函数，<span class="math inline">\(G_{t}\)</span>表示从状态<span class="math inline">\(S_{t}\)</span>开始到episode结束时的总回报，<span class="math inline">\(\alpha\)</span>是一个常数步长参数（梯度下降算法中叫学习率），这个公式叫做<span class="math inline">\(constant-\alpha\)</span> MC。在这个公式中，<span class="math inline">\(G_{t}\)</span>是需要等到整个episode结束才能得到的，因此只有在自举完整的episode后才能进行更新。下面要说的TD算法就很好地解决了这个问题，只需要等到下一个时刻转移到下一个状态和获得回报值。下面是一种最简单的TD算法，叫做TD(0)。</p>
<p><span class="math display">\[V(S_{t})=V(S_{t})+\alpha\left[R_{t+1}+\gamma V(S_{t+1})-V(S_{t}) \right]\tag{4-2}\]</span></p>
<p>我们这里只是用<span class="math inline">\(R_{t+1}+\gamma V(S_{t+1})\)</span>来估计<span class="math inline">\(constant-\alpha\)</span> MC中未来回报的真实值。与蒙特卡罗控制一样，TD(0)也能确保收敛到最优状态值函数，当然前提也是需要大量的经验数据。至于TD(0)与蒙特卡罗控制哪个算法收敛更快，这个问题并没有准确的答案，不过Sutton在书中指出，在一些随机任务上TD(0)比<span class="math inline">\(constant-\alpha\)</span> MC收敛更快。TD(0)算法在每个时刻都要进行一次更新，更高效的方法是在训练时使用batch updating的方式，即一个batch进行一次更新。</p>
<p>显然，TD learning相比MC有以下优点<sup>[7]</sup>：</p>
<ul>
<li>由于TD预测使用差值进行更新，加上步进参数<span class="math inline">\(\alpha\)</span>的存在，TD learning的更新更平稳，方差更小。</li>
<li>TD learning可以用于在线训练，因为不需要等到整个episode结束才更新。</li>
<li>TD learning应用更广，可以用于非有限步数的情况。</li>
</ul>
<p>但也存在一些缺点，比如TD learning对初始值比较敏感，以及收敛结果是有偏的。</p>
<h3 id="tdλ">TD(λ)</h3>
<p>在介绍TD(λ)之前，我们先介绍一下n-Step TD预测。前面介绍的TD(0)算法在当前状态的基础上往后执行一步就可以进行更新，并且在更新时使用了贝尔曼公式对当前状态的未来回报进行估计，那我们是不是也可以往后执行n步之后再更新，这样用贝尔曼公式估计的未来回报是不是会更加精确呢？实际上，当n等于整个episode的总步数时，n-Step TD预测就完全成了MC估计了。</p>
<div align="center">
<img src="https://github.com/hjchen2/personal/blob/master/blog/强化学习/8aabe6f419dfeca3f4ee9de376ceb3bd.png?raw=true" width="540">
</div>
<p><br></p>
<p>对于1-step来说，未来回报的值等于第一个回报值加上下一个状态值函数折扣后的值，用公式表示：</p>
<p><span class="math display">\[G_{t}^{(1)}=R_{t+1}+\gamma V(S_{t+1})\]</span></p>
<p>2-step比1-step多执行一步，其未来回报值为：</p>
<p><span class="math display">\[G_{t}^{(2)}=R_{t+1}+\gamma R_{t+2}+\gamma^{2} V(S_{t+2})\]</span></p>
<p>那么n-step的未来回报值为：</p>
<p><span class="math display">\[G_{t}^{(n)}=R_{t+1}+\gamma R_{t+2}+\gamma^{2} V(S_{t+2})+...+\gamma^{n}V(S_{t+n})\]</span></p>
<p>在公式(4-1)中我们用<span class="math inline">\(G_{t}^{(n)}\)</span>替代<span class="math inline">\(G_{t}\)</span>，最后n-Step TD预测的更新公式为：</p>
<p><span class="math display">\[V(S_{t})=V(S_{t})+\alpha\left[G_{t}^{(n)}-V(S_{t}) \right]\tag{4-3}\]</span></p>
n-Step TD预测一定程度上可以使得估计的值函数更准确，因此收敛效果会更好，但更新时需要等待的步数增加了。下图是使用n-Step TD方法在random walk任务上的RMS error对比。
<div align="center">
<img src="https://github.com/hjchen2/personal/blob/master/blog/强化学习/3a775aa18ad1b86a07d3b75d52b1c25c.png?raw=true" width="600">
</div>
<p><br> n-Step TD只使用了从当前状态开始执行n步未来回报的估计值<span class="math inline">\(G_{t}^{(n)}\)</span>，其实为了充分利用中间每个step的信息，也可以使用不同的n对应的<span class="math inline">\(G_{t}^{(n)}\)</span>的平均值。比如可以把2-step和4-step的均值作为<span class="math inline">\(G_{t}\)</span>的估计值，</p>
<p><span class="math display">\[G_{t}^{avg}=\frac{1}{2}G_{t}^{(2)}+\frac{1}{2}G_{t}^{(4)}\]</span></p>
<p>TD(λ)也可以理解为一种特殊的n-step平均算法，每个n-step的权重为<span class="math inline">\((1-\lambda)\lambda^{(n-1)}\)</span>，所有权重和仍然为1，因此有：</p>
<p><span class="math display">\[G_{t}^{(\lambda)}=(1-\lambda)\sum_{n=1}^{\infty}\lambda^{n-1}G_{t}^{(n)}\tag{4-4}\]</span></p>
<p>公式(4-4)表示的是没有终止状态的情况，对于最终存在终止状态的episode任务或截断任务<sup>[注1]</sup>来讲，为了保证所有权重的和为1，最后一个n-step的权重被设置为<span class="math inline">\(\lambda^{T-t-1}\)</span>，其中<span class="math inline">\(T\)</span>为episode总步数。</p>
<p><span class="math display">\[G_{t}^{(\lambda)}=(1-\lambda)\sum_{n=1}^{T-t-1}\lambda^{n-1}G_{t}^{(n)}+\lambda^{T-t-1}G_{t}\tag{4-5}\]</span></p>
<p>当<span class="math inline">\(\lambda=1\)</span>时，这时TD(λ)就相当于MC，而当<span class="math inline">\(\lambda=0\)</span>时，TD(λ)就退化成了TD(0)。</p>
<div align="center">
<img src="https://github.com/hjchen2/personal/blob/master/blog/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/294acecc263a9668bd48e3403f9b5225.png?raw=true" width="540">
</div>
<p><br></p>
<h3 id="sarsa">Sarsa</h3>
<p>接下来我们考虑一下如何使用TD预测进行策略改进。首先我们知道可以使用TD预测来估计状态值函数，并且可以使用公式(3-2)进行策略改进。但问题来了，公式(3-2)中的<span class="math inline">\(p(s^{&#39;}\mid s,a)\)</span>是未知参数，无法直接进行策略改进。回顾一下蒙特卡洛控制方法，TD也可以直接对动作值函数<span class="math inline">\(q_{\pi}\)</span>进行估计。与<span class="math inline">\(\upsilon_{\pi}\)</span>的更新公式一样，下面是<span class="math inline">\(q_{\pi}\)</span>的更新公式，</p>
<p><span class="math display">\[Q(S_t,A_t)=Q(S_t,A_t)+\alpha[R_{t+1}+\gamma Q(S_{t+1},A_{t+1})-Q(S_t,A_t)]\tag{4-3}\]</span></p>
有了状态值函数，接下来就可以使用公式(3-2)进行策略改进了。在公式(4-3)中，每次非结束状态<span class="math inline">\(S_t\)</span>转移到下一个状态时都进行一次值函数的更新，每次更新都只与<span class="math inline">\((S_t,A_t,R_{t+1},S_{t+1},A_{t+1})\)</span>有关，因此叫做Sarsa算法。如果状态<span class="math inline">\(S_{t+1}\)</span>为终止状态，则<span class="math inline">\(Q(S_{t+1},A_{t+1})=0\)</span>。下面是Sarsa <span class="math inline">\(\epsilon-greedy\)</span>算法的完整过程，由于评估和改进时采用的策略与生成episode的策略是同一个策略，因此Sarsa算法是一种on-policy方法。
<div align="center">
<img src="https://github.com/hjchen2/personal/blob/master/blog/强化学习/a8d5cc18d1df07802931d29487b29542.png?raw=true" width="600">
</div>
<p><br> Sarsa的<span class="math inline">\(Q\)</span>值更新公式与<span class="math inline">\(TD(0)\)</span>一致，实际上也可以采用<span class="math inline">\(TD(λ)\)</span>的形式进行<span class="math inline">\(Q\)</span>值更新，这个改进算法就是Sarsa(λ)。关于Sarsa(λ)的具体介绍请参考《Reinforcement Learning: An Introduction》一书第七章。</p>
<h3 id="q-learning">Q-Learning</h3>
<p>下面介绍的Q学习是一种off-policy方法，并被认为是强化学习算法最重要的突破之一。在Q-learning中，动作值函数的更新完全独立于生成episode的策略，使得学习到的<span class="math inline">\(Q(S_t,A_t)\)</span>直接是最优动作值函数<span class="math inline">\(q_{*}\)</span>的估计值。</p>
<p><span class="math display">\[Q(S_t,A_t)=Q(S_t,A_t)+\alpha[R_{t+1}+\gamma \mathop \max_{a} Q(S_{t+1},a)-Q(S_t,A_t)]\tag{4-4}\]</span></p>
<p>公式(4-4)为Q-learning的单步更新公式，与Sarsa唯一的不同是：类似于动态规划中的值迭代算法，Q学习也是直接使用最优的<span class="math inline">\(Q(S_{t+1}, A_{t+1})\)</span>进行更新，也就相当于策略只采用了最大<span class="math inline">\(Q\)</span>值对应的动作。 Q-learning简化了算法分析和收敛性证明的难度，使得它的收敛性很早就得到了证明。但与前面介绍的蒙特卡洛控制一样，由于每次只选择<span class="math inline">\(Q\)</span>值最大的动作，因此这个算法也会导致部分state-action对不会被策略生成，相应的动作值函数也无法得到更新。为了确保能收敛到最优策略，下面的算法在生成episode时同样使用了<span class="math inline">\(\epsilon-greedy\)</span>策略，但更新时仍然采用确定性策略（即策略只选择<span class="math inline">\(Q\)</span>值最大的动作）。</p>
<div align="center">
<img src="https://github.com/hjchen2/personal/blob/master/blog/强化学习/3e01e229dc9f53393a25ded669fc0971.png?raw=true" width="600">
</div>
<h2 id="dqn">DQN</h2>
<h2 id="dqn改进算法">DQN改进算法</h2>
<h2 id="强化学习在内容推荐中的应用">强化学习在内容推荐中的应用</h2>
<h2 id="参考资料">参考资料</h2>
<p>1、Reinforcement Learning: An Introduction, Richard S. Sutton and Andrew G. Barto，2012<br>
2、Playing Atari with Deep Reinforcement Learning，DeepMind Technologies，Arxiv 2013.12<br>
3、Human-level control through deep reinforcement learning，DeepMind Technologies，Nature 2015.02<br>
4、DeepMind官网 https://deepmind.com/blog/deep-reinforcement-learning<br>
5、https://www.nervanasys.com/demystifying-deep-reinforcement-learning<br>
6、http://www.cnblogs.com/jinxulin/p/3511298.html<br>
7、Introduction to Reinforcement Learning，David Silver</p>
<h2 id="注释">注释</h2>
<p>1、截断任务：在强化学习中，非episode任务由于不存在终止状态，为了便于训练可以将非episode任务截断成episode。</p>
]]></content>
      
        <categories>
            
            <category> reinforcement learning </category>
            
        </categories>
        
        
        <tags>
            
            <tag> reinforcement learning </tag>
            
            <tag> machine learning </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Mac上搭建基于Github的Hexo博客 — Testing]]></title>
      <url>https://hjchen2.github.io/2017/03/22/Mac%E4%B8%8A%E6%90%AD%E5%BB%BA%E5%9F%BA%E4%BA%8EGithub%E7%9A%84Hexo%E5%8D%9A%E5%AE%A2/</url>
      <content type="html"><![CDATA[<h2 id="博客搭建">博客搭建</h2>
<p>搭建过程请参考<a href="http://www.jianshu.com/p/13e64c9e2295" target="_blank" rel="external">原文链接</a>。</p>
<p>注意在mac上安装hexo时选择安装hexo-cli，否则可能会出现以下报错：<br>
[Error: Cannot find module ‘./DTraceProviderBindings’]</p>
<h2 id="主题美化">主题美化</h2>
<p>简单说一下，下次有时间写个详细过程。主要做了以下修改：</p>
<ol style="list-style-type: decimal">
<li>使用NexT主题替换默认的landscape主题<br>
</li>
<li>简化了页脚，看起来更美观<br>
</li>
<li>修改了左边侧栏黑色背景，改成灰色<br>
</li>
<li>侧栏加入本地搜索功能<br>
</li>
<li>使用hypercomments评论插件，支持匿名评论</li>
</ol>
]]></content>
      
        <categories>
            
            <category> Daily </category>
            
        </categories>
        
        
        <tags>
            
            <tag> web technology </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[caffe学习总结]]></title>
      <url>https://hjchen2.github.io/2017/01/19/caffe%E5%AD%A6%E4%B9%A0/</url>
      <content type="html"><![CDATA[<h1 id="caffe学习总结">caffe学习总结</h1>
<h2 id="caffe的由来">caffe的由来</h2>
<p>caffe是贾扬清在UC Berkeley攻读计算机科学博士学位时开发的一套深度学习框架，由于高效、易读和模块化的设计，开源后经过nvidia的帮助优化和社区不断的完善，如今成为视觉领域主流的框架之一。</p>
<ul>
<li><p>贾扬清其人<br>
清华大学的本硕，UC Berkeley的计算机科学博士，师承Prof. Trevor Darrell，期间在新加坡国立大学、微软亚洲研究院、NEC美国实验室和google研究院实习和工作。博士毕业后一直在google brain担任研究科学家，致力于机器视觉、深度学习和tensorflow相关工作。2016年2月加入facebook，主导facebook大多数AI应用的通用、大规模机器学习平台（目前以caffe2为基础的caffe2go已经开源）。</p></li>
<li><p>为什么要开发caffe<br>
贾最早开发的是另一款软件Decaf，主要功能是基于cuda-convnet进行CNN训练。2013年贾扬清读博期间跟心理学老师合作研究使用概率框架来表达人的行为，“但是因为图像上提取的特征比较弱，所以可以外推的结果比较有限”，而2012年Alex Krizhevsky提出的AlexNet在ImageNet比赛中大获成功，贾因此也希望将CNN应用到他们的心理学研究上，于是就开始写了Decaf，通过Decaf验证了“深度学习特征的优异的可移植性”，因此就开始开发一套通用的深度学习框架，即后来的caffe。</p></li>
</ul>
<h2 id="caffe与其他一些主流框架的比较">caffe与其他一些主流框架的比较</h2>
<p>caffe同期也存在其他一些开源框架，比如cuda-convnet、theano、torch等，并且后来又陆续开源了neon、mxnet、tensorflow、CNTK以及paddled等等。现在对于研究者，如何选择一个框架也成了一个麻烦的问题了。下图是2014年贾扬清在caffe论文中对当时的一些框架做的一个比较：</p>
<div align="center">
<img src="https://github.com/hjchen2/personal/blob/master/blog/caffe框架学习/caffe-001.png?raw=true" width="800">
</div>
<p>下面是近年主流框架的一个简单比较：</p>
<ul>
<li>特性</li>
</ul>
<table>
<thead>
<tr class="header">
<th></th>
<th align="center">主语言</th>
<th align="center">从语言</th>
<th align="center">硬件</th>
<th align="center">分布式</th>
<th align="center">命令式</th>
<th align="center">声明式</th>
<th align="center">自动梯度</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>caffe</td>
<td align="center">C++</td>
<td align="center">Python/Matlab</td>
<td align="center">CPU/GPU</td>
<td align="center">✖</td>
<td align="center">✖</td>
<td align="center">✔</td>
<td align="center">✖</td>
</tr>
<tr class="even">
<td>mxnet</td>
<td align="center">C++</td>
<td align="center">Python/R/Julia/Scala</td>
<td align="center">CPU/GPU/Mobile</td>
<td align="center">✔</td>
<td align="center">✔</td>
<td align="center">✔</td>
<td align="center">✔</td>
</tr>
<tr class="odd">
<td>tensorflow</td>
<td align="center">C++</td>
<td align="center">Python</td>
<td align="center">CPU/GPU/Mobile</td>
<td align="center">✔</td>
<td align="center">✖</td>
<td align="center">✔</td>
<td align="center">✔</td>
</tr>
<tr class="even">
<td>Torch</td>
<td align="center">Lua</td>
<td align="center">-</td>
<td align="center">CPU/GPU/FPGA</td>
<td align="center">✔</td>
<td align="center">✔</td>
<td align="center">✖</td>
<td align="center">✔</td>
</tr>
<tr class="odd">
<td>theano</td>
<td align="center">Python</td>
<td align="center">-</td>
<td align="center">CPU/GPU</td>
<td align="center">✖</td>
<td align="center">✖</td>
<td align="center">✔</td>
<td align="center">✔</td>
</tr>
</tbody>
</table>
<ul>
<li>效率
<div align="center">
<img src="https://github.com/hjchen2/personal/blob/master/blog/caffe框架学习/caffe-002.png?raw=true" width="420">
</div></li>
</ul>
<h2 id="caffe代码组织结构">caffe代码组织结构</h2>
<p>caffe代码结构是非常清晰的，主要包含以下文件和目录：</p>
<ul>
<li>Makefile和Makefile.config caffe支持cmake和make两种编译方式，不过大部分人只需要用make编译就可以了。Makefile.config可以对一些编译选项进行配置，比如USE_MPI、CPU_ONLY、DEBUG等等。</li>
<li>include 在caffe中除了proto文件生成的头文件外，所有的c++头文件都放在include目录中。</li>
<li>src src与include的目录结构基本上相同，include目录中的文件基本上都能在src目录中找到对应的实现文件。</li>
<li>tools tools目录下是caffe提供给用户直接使用的接口，比如caffe.cpp用于模型训练、评估以及统计耗时，另外也提供一些数据集转换、计算均值等工具</li>
<li>examples 提供一些训练相关的脚本和网络配置，比如数据预处理脚本、不同的网络配置文件以及训练脚本</li>
<li>models 提供一些模型的网络配置文件，以及训练好的模型，用户可以直接用训练好的模型进行fine-tune或者分类</li>
<li>matlab/python 提供matlab和python的接口</li>
</ul>
<h2 id="caffe网络的组织方式">caffe网络的组织方式</h2>
<p>从LeNet开始，CNN就开始有了一个标准的分层结构——堆叠卷积层，卷积层可能后接一些normalization和pooling层，网络最后接一个或多个全连接层。由于梯度下降算法非常适合逐层计算，因此当时很多的通用框架都将网络（Net）抽象为多个数据处理层（Layer）组成的有向图，并支持灵活地定义网络结构。caffe将神经网络的训练问题分解为四个方面：数据、计算、流动控制以及问题求解，分别对应caffe中的Blob、Layer、Net和Solver。网络中流动的数据以及参数都用Blob来表示，Layer负责前向输出和后向梯度的计算，Net负责控制Layer计算的顺序，Solver是一个求解器的角色，根据Net的梯度对网络参数进行更新。</p>
<p><img src="https://github.com/hjchen2/personal/blob/master/blog/caffe框架学习/caffe-003.png?raw=true" width="800"></p>
<p>[待补充]</p>
<h2 id="caffe中的blob及同步策略">caffe中的Blob及同步策略</h2>
<p>Blob是caffe中存储数据的基本结构，可以简单理解为一个4维的数组，数据组织格式为（N,C,H,W）。在caffe中上下层流动的数据和每层的权重参数都是用Blob来保存的，为了便于使用，Blob具有一些特性：</p>
<ul>
<li>Blob的内存是懒分配的（lazily allocate），只有在真正使用的时候才会分配内存</li>
<li>Blob会在CPU和GPU上各自分配一块相同大小的内存，便于在CPU和GPU之间进行切换</li>
<li>用户不需要关心CPU和GPU数据的同步，Blob会根据需要自动同步</li>
</ul>
<p>下面是Blob的成员变量，data_是Blob存储的数据，diff_保存的是数据的梯度，shape_data_和shape_保存的都是当前数组的形状，count_是当前数据的大小，capacity_是申请的内存的大小，避免每次Reshape都要释放并重新申请内存。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// include/caffe/blob.hpp</span></div><div class="line"><span class="built_in">shared_ptr</span>&lt;SyncedMemory&gt; data_;</div><div class="line"><span class="built_in">shared_ptr</span>&lt;SyncedMemory&gt; diff_;</div><div class="line"><span class="built_in">shared_ptr</span>&lt;SyncedMemory&gt; shape_data_;</div><div class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; shape_;</div><div class="line"><span class="keyword">int</span> count_;</div><div class="line"><span class="keyword">int</span> capacity_;</div></pre></td></tr></table></figure>
<p>下面主要说一下Blob的自动同步策略。首先看一下SyncedMemory的成员变量：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// include/caffe/syncedmem.hpp</span></div><div class="line"><span class="keyword">void</span>* cpu_ptr_; <span class="comment">// CPU内存数据</span></div><div class="line"><span class="keyword">void</span>* gpu_ptr_; <span class="comment">// GPU显存数据</span></div><div class="line"><span class="keyword">size_t</span> size_;   <span class="comment">// 数据大小</span></div><div class="line">SyncedHead head_;  <span class="comment">// 同步标志</span></div><div class="line"><span class="keyword">bool</span> own_cpu_data_; </div><div class="line"><span class="keyword">bool</span> cpu_malloc_use_cuda_;</div><div class="line"><span class="keyword">bool</span> own_gpu_data_;</div><div class="line"><span class="keyword">int</span> gpu_device_; <span class="comment">// GPU设备号</span></div></pre></td></tr></table></figure>
<p>head_的取值范围为UNINITIALIZED, HEAD_AT_CPU, HEAD_AT_GPU, SYNCED。初始化时head_值为UNINITIALIZED，当调用Blob的取值函数时都会调用一次SyncedMemory的to_cpu或者to_gpu进行数据的同步，同步策略为：<br>
1、取cpu数据时，会调用to_cpu函数，如果heda_为HEAD_AT_GPU，则需要将GPU的数据同步至CPU，否则不需要同步<br>
2、取gpu数据时，会调用to_gpu函数，如果heda_为HEAD_AT_CPU，则需要将CPU的数据同步至GPU，否则不需要同步</p>
<p>head_标志的赋值：<br>
1、每次调用SyncedMemory的mutable_cpu_data时，head_都会被置为HEAD_AT_CPU<br>
2、每次调用SyncedMemory的mutable_gpu_data时，head_都会被置为HEAD_AT_GPU<br>
3、每次同步之后heda_会被置为SYNCED。</p>
<p>因此Blob通过判断每次修改的位置来自行决定是否需要对不同设备间的两份数据进行同步，使用时就像只有一份数据一样，非常方便。</p>
<h2 id="caffe中的layer">caffe中的Layer</h2>
<p>layer是caffe模型的主要组成部分和基本的计算单元，与很多框架中的operator对应，一个典型的layer在forward时从下层连接获取输入，经过计算后输出到上层，backward时又从上层连接获取误差，计算本层梯度和误差后，将误差传递到下层连接。因此基类Layer实现了三个基本函数setup、forward和backward。</p>
<ul>
<li>setup：根据下层连接和配置参数完成本层参数的初始化，以及输出blobs的初始化</li>
<li>forward：前向计算过程，并计算本层的loss</li>
<li>backward：后向计算过程，并将本层误差传递到下层</li>
</ul>
<p>forward和backward里面都会对CPU和GPU进行分支，如果是CPU模式，则真正参与计算的是forward_cpu和backward_cpu，如果是GPU模式，则参与计算的是forward_gpu和backward_gpu，并且在基类中forward_gpu和backward_gpu分别调用的是forward_cpu和backward_cpu，当然用户在定义新的layer时可以自行实现forward_gpu和backward_gpu。</p>
<p>基类Layer的成员变量：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// include/caffe/layer.hpp</span></div><div class="line"><span class="comment">/** The protobuf that stores the layer parameters */</span></div><div class="line">LayerParameter layer_param_;</div><div class="line"><span class="comment">/** The phase: TRAIN or TEST */</span></div><div class="line">Phase phase_;</div><div class="line"><span class="comment">/** The vector that stores the learnable parameters as a set of blobs. */</span></div><div class="line"><span class="built_in">vector</span>&lt;<span class="built_in">shared_ptr</span>&lt;Blob&lt;Dtype&gt; &gt; &gt; blobs_;</div><div class="line"><span class="comment">/** Vector indicating whether to compute the diff of each param blob. */</span></div><div class="line"><span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt; param_propagate_down_;</div><div class="line"><span class="comment">/** The vector that indicates whether each top blob has a non-zero weight in</span></div><div class="line"> *  the objective function. */</div><div class="line"><span class="built_in">vector</span>&lt;Dtype&gt; loss_;</div></pre></td></tr></table></figure>
<p>layer_param_是从protobuf文件中反序列化得到的，存放的是layer的配置参数 phase_指示是训练还是测试 blobs_是本层的参数，比如权重和偏置 param_propagate_down_为每一个参数设定是否需要计算梯度 loss_是本层的损失值，loss层每个输出blob都有一个损失值，非loss层损失为0</p>
<p>由基类Layer直接或间接派生出各种layer，比如卷积(convolution)、全连接(fully connected或者inner product)、dropout、pooling、relu、softmaxWithLoss等等，每一个派生layer都会强制实现forward_cpu和backward_cpu。早期的caffe将layer分成5类，</p>
<ul>
<li>dataLayer类： 各类数据读取的接口</li>
<li>neuronLayer类： 各种激活函数、dropout</li>
<li>visionLayer类： 卷积层、采样层等2D图像相关的运算</li>
<li>commonLayer类：全连接层和其他运算</li>
<li>lossLayer类：实现各种代价函数</li>
</ul>
<p>不过目前最新版本的caffe已经取消了visionLayer和commonLayer的分类。此外由于caffe使用了cuDNN运算加速库，因此部分layer有caffe和cuDNN两种实现，使用时可以通过protobuf文件配置需要使用的engine。</p>
<p>为了保持框架的可扩展性，大多数框架在layer或者operator的实现中使用了工厂模式，使用统一的工厂类来对不同的layer或operator进行实例化。下面是caffe使用工厂模式的代码实现，</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// include/caffe/layer_factory.hpp</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">class</span> LayerRegistry &#123;</div><div class="line"><span class="keyword">public</span>:</div><div class="line">  <span class="keyword">typedef</span> <span class="built_in">shared_ptr</span>&lt;Layer&lt;Dtype&gt; &gt; (*Creator)(<span class="keyword">const</span> LayerParameter&amp;);</div><div class="line">  <span class="keyword">typedef</span> <span class="built_in">std</span>::<span class="built_in">map</span>&lt;<span class="built_in">string</span>, Creator&gt; CreatorRegistry;</div><div class="line">  <span class="function"><span class="keyword">static</span> CreatorRegistry&amp; <span class="title">Registry</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">static</span> CreatorRegistry* g_registry_ = <span class="keyword">new</span> CreatorRegistry();</div><div class="line">    <span class="keyword">return</span> *g_registry_;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  <span class="comment">// Adds a creator.</span></div><div class="line">  <span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="title">AddCreator</span><span class="params">(<span class="keyword">const</span> <span class="built_in">string</span>&amp; type, Creator creator)</span> </span>&#123;</div><div class="line">    CreatorRegistry&amp; registry = Registry();</div><div class="line">    CHECK_EQ(registry.count(type), <span class="number">0</span>)</div><div class="line">        &lt;&lt; <span class="string">"Layer type "</span> &lt;&lt; type &lt;&lt; <span class="string">" already registered."</span>;</div><div class="line">    registry[type] = creator;</div><div class="line">  &#125;</div><div class="line">...</div><div class="line">&#125;;</div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">class</span> LayerRegisterer &#123;</div><div class="line"> <span class="keyword">public</span>:</div><div class="line">  LayerRegisterer(<span class="keyword">const</span> <span class="built_in">string</span>&amp; type,</div><div class="line">                  <span class="built_in">shared_ptr</span>&lt;Layer&lt;Dtype&gt; &gt; (*creator)(<span class="keyword">const</span> LayerParameter&amp;)) &#123;</div><div class="line">    <span class="comment">// LOG(INFO) &lt;&lt; "Registering layer type: " &lt;&lt; type;</span></div><div class="line">    LayerRegistry&lt;Dtype&gt;::AddCreator(type, creator);</div><div class="line">  &#125;</div><div class="line">&#125;;</div><div class="line"></div><div class="line"><span class="meta">#<span class="meta-keyword">define</span> REGISTER_LAYER_CREATOR(type, creator)                                  \</span></div><div class="line">  static LayerRegisterer<span class="meta-string">&lt;float&gt; g_creator_f_##type(#type, creator&lt;float&gt;);     \</span></div><div class="line">  <span class="keyword">static</span> LayerRegisterer&lt;<span class="keyword">double</span>&gt; g_creator_d_#<span class="meta">#type(#type, creator<span class="meta-string">&lt;double&gt;)    \</span></span></div></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// src/caffe/layer_factory.cpp</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="built_in">shared_ptr</span>&lt;Layer&lt;Dtype&gt; &gt; GetSigmoidLayer(<span class="keyword">const</span> LayerParameter&amp; param) &#123;</div><div class="line">  SigmoidParameter_Engine engine = param.sigmoid_param().engine();</div><div class="line">  <span class="keyword">if</span> (engine == SigmoidParameter_Engine_DEFAULT) &#123;</div><div class="line">    engine = SigmoidParameter_Engine_CAFFE;</div><div class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> USE_CUDNN</span></div><div class="line">    engine = SigmoidParameter_Engine_CUDNN;</div><div class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></div><div class="line">  &#125;</div><div class="line">  <span class="keyword">if</span> (engine == SigmoidParameter_Engine_CAFFE) &#123;</div><div class="line">    <span class="keyword">return</span> <span class="built_in">shared_ptr</span>&lt;Layer&lt;Dtype&gt; &gt;(<span class="keyword">new</span> SigmoidLayer&lt;Dtype&gt;(param));</div><div class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> USE_CUDNN</span></div><div class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (engine == SigmoidParameter_Engine_CUDNN) &#123;</div><div class="line">    <span class="keyword">return</span> <span class="built_in">shared_ptr</span>&lt;Layer&lt;Dtype&gt; &gt;(<span class="keyword">new</span> CuDNNSigmoidLayer&lt;Dtype&gt;(param));</div><div class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    LOG(FATAL) &lt;&lt; <span class="string">"Layer "</span> &lt;&lt; param.name() &lt;&lt; <span class="string">" has unknown engine."</span>;</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">REGISTER_LAYER_CREATOR(Sigmoid, GetSigmoidLayer);</div></pre></td></tr></table></figure>
<h2 id="caffe中的net">caffe中的Net</h2>
<p>Net是由Layer组成的有向图，表示整个神经网络的拓扑结构，与很多框架中的graph对应，一般用一个protobuf文件来定义。而且Layer作为有向图中的一个组件，是无法感知自己的上层和下层连接的，需要Net将数据feed给Layer，这样数据在有向图中才能真正流动起来。因此Net至少需要提供构建一个有向图和feed数据流两种功能。</p>
<ul>
<li>构建一个有向图：void Init(const NetParameter&amp; in_param)</li>
<li>feed数据流： const vector<blob<dtype\>*&gt;&amp; Forward(Dtype* loss)和void Backward()</blob<dtype\></li>
</ul>
<p>在构建有向图时，caffe首先会对不符合规则的layer进行过滤，比如对于test net，则会把只用于train的layer过滤掉。对于有向图中可能存在分支的情况，caffe会自动插入split层，将原输入blob复制多份，分别输入不同的分支，比如：LeNet网络中的数据层的label需要输入到accuracy层和loss层，那么需要在数据层再插入一层，如下图所示。</p>
<div align="center">
<img src="https://github.com/hjchen2/personal/blob/master/blog/caffe框架学习/caffe-004.jpg?raw=true" width="600">
</div>
<p>Net会根据网络结构逐层创建layer，并指定输入输出blobs，以及是否需要backward。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// src/caffe/net.cpp:Init</span></div><div class="line">...</div><div class="line"><span class="keyword">for</span> (<span class="keyword">int</span> layer_id = <span class="number">0</span>; layer_id &lt; param.layer_size(); ++layer_id) &#123;</div><div class="line">    ...</div><div class="line">    layers_.push_back(LayerRegistry&lt;Dtype&gt;::CreateLayer(layer_param));</div><div class="line">    ...</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> bottom_id = <span class="number">0</span>; bottom_id &lt; layer_param.bottom_size(); ++bottom_id) &#123;</div><div class="line">      <span class="keyword">const</span> <span class="keyword">int</span> blob_id = AppendBottom(param, layer_id, bottom_id,</div><div class="line">                                       &amp;available_blobs, &amp;blob_name_to_idx);</div><div class="line">      <span class="comment">// If a blob needs backward, this layer should provide it.</span></div><div class="line">      need_backward |= blob_need_backward_[blob_id];</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">int</span> num_top = layer_param.top_size();</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> top_id = <span class="number">0</span>; top_id &lt; num_top; ++top_id) &#123;</div><div class="line">      AppendTop(param, layer_id, top_id, &amp;available_blobs, &amp;blob_name_to_idx);</div><div class="line">      <span class="comment">// Collect Input layer tops as Net inputs.</span></div><div class="line">      <span class="keyword">if</span> (layer_param.type() == <span class="string">"Input"</span>) &#123;</div><div class="line">        <span class="keyword">const</span> <span class="keyword">int</span> blob_id = blobs_.size() - <span class="number">1</span>;</div><div class="line">        net_input_blob_indices_.push_back(blob_id);</div><div class="line">        net_input_blobs_.push_back(blobs_[blob_id].get());</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">    ...</div><div class="line">    layers_[layer_id]-&gt;SetUp(bottom_vecs_[layer_id], top_vecs_[layer_id]);</div><div class="line">    ...</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在训练时，train net会首先初始化，test net之后初始化，每次test时会调用ShareTrainedLayersWith共享train net的参数，这样做可以节省显存并且避免不必要的数据拷贝。</p>
<p>需要注意的是，在protobuf文件中声明网络结构时，必须依照从下到上的顺序一层一层定义网络参数，而且test net和train net对应层的name最好一致(虽然不一致可能不会导致程序报错），因为test net与train net是根据匹配name进行参数共享的，如果name不一致则会导致无法进行参数共享，增加显存消耗的同时还会导致test结果不正确。</p>
<p>当有向图构建完成后，我们只需要调用Forward和Backward，数据就能流经整个网络，得到每层的输出、loss和每个参数的梯度。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// src/caffe/net.cpp</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">Dtype Net&lt;Dtype&gt;::ForwardFromTo(<span class="keyword">int</span> start, <span class="keyword">int</span> end) &#123;</div><div class="line">  CHECK_GE(start, <span class="number">0</span>);</div><div class="line">  CHECK_LT(end, layers_.size());</div><div class="line">  Dtype loss = <span class="number">0</span>;</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = start; i &lt;= end; ++i) &#123;</div><div class="line">    <span class="comment">// LOG(ERROR) &lt;&lt; "Forwarding " &lt;&lt; layer_names_[i];</span></div><div class="line">    Dtype layer_loss = layers_[i]-&gt;Forward(bottom_vecs_[i], top_vecs_[i]);</div><div class="line">    loss += layer_loss;</div><div class="line">    <span class="keyword">if</span> (debug_info_) &#123; ForwardDebugInfo(i); &#125;</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">return</span> loss;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> Net&lt;Dtype&gt;::BackwardFromTo(<span class="keyword">int</span> start, <span class="keyword">int</span> end) &#123;</div><div class="line">  CHECK_GE(end, <span class="number">0</span>);</div><div class="line">  CHECK_LT(start, layers_.size());</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = start; i &gt;= end; --i) &#123;</div><div class="line">    <span class="keyword">if</span> (layer_need_backward_[i]) &#123;</div><div class="line">      layers_[i]-&gt;Backward(</div><div class="line">          top_vecs_[i], bottom_need_backward_[i], bottom_vecs_[i]);</div><div class="line">      <span class="keyword">if</span> (debug_info_) &#123; BackwardDebugInfo(i); &#125;</div><div class="line">    &#125; </div><div class="line">  &#125;   </div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="caffe中的solver">caffe中的Solver</h2>
<p>前面讲到Net通过调用Forward和Backward可以得到每个参数的梯度，而Solver的主要作用就是根据这些梯度进行网络参数的更新。由于caffe将Net作为Solver的底层实现，因此Solver也就成了控制整个训练过程的中枢。Solver提供三个主要函数：Init、Solve、ApplyUpdate。</p>
<ul>
<li>Init：创建训练网络和测试网络，初始化一些参数</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// src/caffe/solver.cpp</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> Solver&lt;Dtype&gt;::Init(<span class="keyword">const</span> SolverParameter&amp; param) &#123;</div><div class="line">  ...</div><div class="line">  <span class="comment">// Scaffolding code</span></div><div class="line">  InitTrainNet();</div><div class="line">  <span class="keyword">if</span> (Caffe::root_solver()) &#123;</div><div class="line">    InitTestNets();</div><div class="line">    LOG(INFO) &lt;&lt; <span class="string">"Solver scaffolding done."</span>;</div><div class="line">  &#125;</div><div class="line">  iter_ = <span class="number">0</span>;</div><div class="line">  current_step_ = <span class="number">0</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<ul>
<li>Solve：调用Step进行迭代训练，每次迭代后都会调用ApplyUpdate进行参数的更新</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// src/caffe/solver.cpp</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line">Dtype Solver&lt;Dtype&gt;::ForwardBackward() &#123;</div><div class="line">  ...</div><div class="line">  <span class="comment">// accumulate the loss and gradient</span></div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; param_.iter_size(); ++i) &#123;</div><div class="line">    loss += net_-&gt;ForwardBackward();\</div><div class="line">  &#125;</div><div class="line">  <span class="keyword">return</span> loss / param_.iter_size();</div><div class="line">&#125;</div><div class="line"> </div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> Solver&lt;Dtype&gt;::Step(<span class="keyword">int</span> iters) &#123;</div><div class="line">  ...</div><div class="line">  <span class="keyword">while</span> (iter_ &lt; stop_iter) &#123;</div><div class="line">    <span class="keyword">if</span> (param_.test_interval() &amp;&amp; iter_ % param_.test_interval() == <span class="number">0</span></div><div class="line">        &amp;&amp; (iter_ &gt; <span class="number">0</span> || param_.test_initialization())</div><div class="line">        &amp;&amp; Caffe::root_solver()) &#123;</div><div class="line">      TestAll(); <span class="comment">// 进行测试</span></div><div class="line">    &#125;</div><div class="line">    ...</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; callbacks_.size(); ++i) &#123;</div><div class="line">      callbacks_[i]-&gt;on_start();</div><div class="line">    &#125;</div><div class="line">    ...</div><div class="line">    Dtype loss = ForwardBackward();</div><div class="line">    ...</div><div class="line">    UpdateSmoothedLoss(loss, start_iter, average_loss);</div><div class="line">    ...</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; callbacks_.size(); ++i) &#123;</div><div class="line">      callbacks_[i]-&gt;on_gradients_ready();</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">if</span> (!param().disabled_update()) &#123;</div><div class="line">      ApplyUpdate();</div><div class="line">    &#125;</div><div class="line">    ++iter_;</div><div class="line">    ...</div><div class="line">&#125;</div><div class="line"> </div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> Solver&lt;Dtype&gt;::Solve(<span class="keyword">const</span> <span class="keyword">char</span>* resume_file) &#123;</div><div class="line">  ...</div><div class="line">  Step(param_.max_iter() - iter_);</div><div class="line">  ...</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<ul>
<li>ApplyUpdate：调用对应的solver进行参数更新，下面是sgd solver的ApplyUpdate函数</li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// src/caffe/solvers/sgd_solver.cpp</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> SGDSolver&lt;Dtype&gt;::ApplyUpdate() &#123;</div><div class="line">  ...</div><div class="line">  Dtype rate = GetLearningRate(); <span class="comment">//获取当前迭代的学习率</span></div><div class="line">  ...</div><div class="line">  ClipGradients(); <span class="comment">// 进行梯度规整</span></div><div class="line">  <span class="comment">// learnable_params存放的是网络中所有需要学习的参数blobs</span></div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> param_id = <span class="number">0</span>; param_id &lt; <span class="keyword">this</span>-&gt;net_-&gt;learnable_params().size();</div><div class="line">       ++param_id) &#123;</div><div class="line">    ApplyUpdate(param_id); <span class="comment">// 逐个更新参数</span></div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>由于梯度下降算法发展出了非常多的优化算法，目前caffe提供了六种优化算法来求解最优参数，在solver配置文件中，通过设置type类型来选择。</p>
<ul>
<li>Stochastic Gradient Descent (type: “SGD”),</li>
<li>AdaDelta (type: “AdaDelta”),</li>
<li>Adaptive Gradient (type: “AdaGrad”),</li>
<li>Adam (type: “Adam”),</li>
<li>Nesterov’s Accelerated Gradient (type: “Nesterov”)</li>
<li>RMSprop (type: “RMSProp”)</li>
</ul>
<h2 id="caffe断点保存和恢复">caffe断点保存和恢复</h2>
<p>由于训练过程往往非常耗时，为了能够在突发情况后快速恢复训练，caffe提供了断点保存和恢复的功能，在solver的配置文件中可以配置保存的频率及保存时文件名的前缀，一个比较完整的solver配置文件如下：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// solver.prototxt</span></div><div class="line">net: <span class="string">"./train_val.prototxt"</span>  <span class="comment">// 定义net的protobuf文件</span></div><div class="line">test_iter: <span class="number">100</span>   <span class="comment">// 测试的迭代次数，这个需要根据测试数据的大小和测试时的batch size计算得到，test_iter = test_dataset_size / test_batch_size</span></div><div class="line">test_interval: <span class="number">1000</span>  <span class="comment">// 设置test的频率，每训练1000次迭代就测试一次</span></div><div class="line">base_lr: <span class="number">0.01</span>  <span class="comment">// 设置学习率</span></div><div class="line">lr_policy: <span class="string">"step"</span>  <span class="comment">// 设置学习率衰减策略</span></div><div class="line">gamma: <span class="number">0.1</span>  <span class="comment">// step衰减因子，</span></div><div class="line">stepsize: <span class="number">10000</span> <span class="comment">// 衰减的频率，每训练10000次迭代衰减一次，衰减后的学习率=当前学习率*gamma</span></div><div class="line">display: <span class="number">500</span>  <span class="comment">// 训练log打印频率</span></div><div class="line">max_iter: <span class="number">45000</span>  <span class="comment">// 设置最大训练多少次迭代</span></div><div class="line">type: <span class="string">"SGD"</span>  <span class="comment">// 设置solver类型 </span></div><div class="line">momentum: <span class="number">0.9</span>  <span class="comment">// 设置SGD中的动量项</span></div><div class="line">weight_decay: <span class="number">0.0005</span>  <span class="comment">// 设置正则系数</span></div><div class="line">snapshot: <span class="number">1000</span>  <span class="comment">// 设置模型保存频率</span></div><div class="line">snapshot_prefix: <span class="string">"../output/caffe_alexnet_train"</span>  <span class="comment">// 设置模型保存时文件名前缀</span></div><div class="line">solver_mode: CPU  <span class="comment">// 设置训练模式，CPU还是GPU</span></div></pre></td></tr></table></figure>
<p>当然还有一些其他的参数，比如正则化类型和模型保存文件格式等，都会使用在proto文件中定义的默认值，具体查看src/caffe/proto/caffe.proto文件中的SolverParameter。</p>
<p>为了实现断点保存和恢复，caffe在Solver中加入了Snapshot和Restore，分别进行模型保存和模型恢复，相应地，在Net中也加入了ToProto/ToHDF5和CopyTrainedLayersFromBinaryProto/CopyTrainedLayersFromHDF5。Solver调用Step进行训练的时候，每次参数更新结束都会判断是否需要保存模型。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// src/caffe/solver.cpp:Step</span></div><div class="line"><span class="keyword">if</span> ((param_.snapshot()</div><div class="line">     &amp;&amp; iter_ % param_.snapshot() == <span class="number">0</span></div><div class="line">     &amp;&amp; Caffe::root_solver()) ||</div><div class="line">     (request == SolverAction::SNAPSHOT)) &#123;</div><div class="line">  Snapshot();</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>Solver中Snapshot对模型参数和训练状态进行保存，模型参数提供两种保存格式——binary protobuf和hdf5。如果是protobuf格式，则会调用Net的ToProto，否则调用ToHDF5。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// src/caffe/net.cpp</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> Net&lt;Dtype&gt;::ToProto(NetParameter* param, <span class="keyword">bool</span> write_diff) <span class="keyword">const</span> &#123;</div><div class="line">  param-&gt;Clear();</div><div class="line">  param-&gt;set_name(name_);</div><div class="line">  <span class="comment">// Add bottom and top</span></div><div class="line">  DLOG(INFO) &lt;&lt; <span class="string">"Serializing "</span> &lt;&lt; layers_.size() &lt;&lt; <span class="string">" layers"</span>;</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; layers_.size(); ++i) &#123;</div><div class="line">    LayerParameter* layer_param = param-&gt;add_layer();</div><div class="line">    layers_[i]-&gt;ToProto(layer_param, write_diff);</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>Solver在开始训练时会尝试调用Restore进行断点恢复，根据文件名后缀判断文件格式，并选择RestoreSolverStateFromHDF5还是RestoreSolverStateFromBinaryProto。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// src/caffe/solver.cpp:Solve</span></div><div class="line"><span class="keyword">if</span> (resume_file) &#123;</div><div class="line">  LOG(INFO) &lt;&lt; <span class="string">"Restoring previous solver status from "</span> &lt;&lt; resume_file;</div><div class="line">  Restore(resume_file);</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="in-place计算">in-place计算</h2>
<p>为了节约显存，caffe支持原址计算，就是输入与输出都是同一个blob。如果前一层的输出和本层的输入都与后向计算时无关，而且本层的输入和输出blob大小相同，就可以使用in-place计算，比如卷积层后面的Sigmoid、Relu等都可以用同址计算，而BatchNorm层也支持in-place计算，是因为BatchNorm在实现时会将输入数据进行备份。使用同址计算只要在protobuf文件中指定该层的top和bottom是同名的就可以了，比如：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">layer &#123;</div><div class="line">        bottom: <span class="string">"conv1"</span></div><div class="line">        top: <span class="string">"conv1"</span></div><div class="line">        name: <span class="string">"conv1_relu"</span></div><div class="line">        type: <span class="string">"ReLU"</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="参数初始化方法">参数初始化方法</h2>
<p>由于神经网络的目标函数往往是非凸的，参数初始化会对最终的收敛结果造成非常大的影响。为了满足不同的参数初始化需求，caffe提供了多种初始化方法，并且在net的配置文件中可以为每个参数选择一个初始化方法。比如下面的weight_filler和bias_filler：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">layer &#123;</div><div class="line">        bottom: <span class="string">"data"</span></div><div class="line">        top: <span class="string">"conv1"</span></div><div class="line">        name: <span class="string">"conv1"</span></div><div class="line">        type: <span class="string">"Convolution"</span></div><div class="line">        convolution_param &#123;</div><div class="line">                num_output: <span class="number">64</span></div><div class="line">                kernel_size: <span class="number">7</span></div><div class="line">                pad: <span class="number">3</span></div><div class="line">                stride: <span class="number">2</span></div><div class="line">                weight_filler &#123;</div><div class="line">                  type: <span class="string">"xavier"</span></div><div class="line">                &#125;</div><div class="line">                bias_filler &#123;</div><div class="line">                  type: <span class="string">"constant"</span></div><div class="line">                  value: <span class="number">0.2</span></div><div class="line">               &#125;</div><div class="line">        &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在include/caffe/filler.hpp中caffe提供如下的初始化方法：</p>
<ul>
<li>constant：常量初始化，参数所有的值都被初始化为相同的值</li>
<li>uniform：均匀初始化，参数的值按照指定区间均匀分布随机初始化</li>
<li>gaussian：高斯初始化，参数的值按照指定均值和方差的正态分布随机初始化</li>
<li>positive unitball</li>
<li>xavier：本质上也是一种指定区间均匀分布的随机初始化方式，只是区间是通过参数大小计算得到</li>
<li>msra：与xavier类似，不过使用的是指定均值和方差的正态分布随机初始化方式</li>
<li>bilinear</li>
</ul>
<h2 id="多卡并行策略">多卡并行策略</h2>
<p>为了提高效率，caffe支持单机多GPU并行训练，目前采用的是数据并行方式，暂不支持模型并行，为此caffe增加了一个P2PSync类，下面主要介绍一下P2PSync如何实现多卡并行的。</p>
<p>P2PSync封装了一个Solver负责训练，每张GPU都会对应一个P2PSync，并且P2PSync之间具有主从关系，它们之间构成一个二叉树的结构。在前向计算时，主P2PSync需要将模型分发给从P2PSync，而在后向传导时，从P2PSync就需要把梯度传给主P2PSync，主P2PSync会在聚合从P2PSync的梯度后传给更上一层的主P2PSync。在二叉树结构中，根节点P2PSync的Solver被叫做root solver，其他solver叫做worker solver，只有root solver才能进行参数更新，worker solver只是将梯度聚合并传递给root solver。</p>
<div align="center">
<img src="https://github.com/hjchen2/personal/blob/master/blog/caffe框架学习/caffe-005.jpg?raw=true" width="720">
</div>
<p>在P2PSync中主要的函数就InternalThreadEntry、on_start和on_gradients_ready。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// src/caffe/parallel.cpp</span></div><div class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> P2PSync&lt;Dtype&gt;::InternalThreadEntry() &#123;</div><div class="line">...</div><div class="line"> solver_-&gt;Step(solver_-&gt;param().max_iter() - initial_iter_);</div><div class="line"> &#125;</div></pre></td></tr></table></figure>
<p>InternalThreadEntry是一个线程函数，Solver调用Step进行训练，在Step中每次前向计算前都会回调on_start获取最新模型，而在后向计算结束后又会回调on_gradients_ready传递梯度。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// src/caffe/solver.cpp</span></div><div class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> Solver&lt;Dtype&gt;::Step(<span class="keyword">int</span> iters) &#123;</div><div class="line"> ...</div><div class="line"> <span class="keyword">while</span> (iter_ &lt; stop_iter) &#123;</div><div class="line"> <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; callbacks_.size(); ++i) &#123;</div><div class="line"> callbacks_[i]-&gt;on_start(); <span class="comment">// 回调P2PSync中的on_start，从主P2PSync获取新模型</span></div><div class="line"> &#125;</div><div class="line"> ...</div><div class="line"> Dtype loss = Forward_backward();</div><div class="line"> ...</div><div class="line"> <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; callbacks_.size(); ++i) &#123;</div><div class="line"> callbacks_[i]-&gt;on_gradients_ready(); <span class="comment">// 回调P2PSync中的on_gradients_ready，依次聚合从P2PSync和自身的梯度，并将梯度发送给主P2PSync</span></div><div class="line"> &#125;</div><div class="line"> <span class="keyword">if</span> (!param().disabled_update()) &#123;</div><div class="line"> ApplyUpdate(); <span class="comment">// 这里只有root solver才会进行参数更新</span></div><div class="line"> &#125;</div><div class="line"> ...</div><div class="line"></div><div class="line"> &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> P2PSync&lt;Dtype&gt;::on_start() &#123;</div><div class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> CPU_ONLY</span></div><div class="line"> ...</div><div class="line"> <span class="comment">// Wait for update from parent</span></div><div class="line"> <span class="keyword">if</span> (parent_) &#123;</div><div class="line">   <span class="comment">/* 除了root solver，其他worker solver都有对应的parent</span></div><div class="line">      程序执行到这里时会阻塞，当主P2PSync将自身入队后就会通知从P2PSync，pop就能返回</div><div class="line">   */</div><div class="line">   P2PSync&lt;Dtype&gt; *parent = queue_.pop(); <span class="comment">// 等待主P2PSync入队</span></div><div class="line">   CHECK(parent == parent_);</div><div class="line"> &#125;</div><div class="line"> <span class="comment">// Update children</span></div><div class="line"> <span class="keyword">for</span> (<span class="keyword">int</span> i = children_.size() - <span class="number">1</span>; i &gt;= <span class="number">0</span>; i--) &#123;</div><div class="line">   Dtype* src = data_;</div><div class="line">   Dtype* dst = children_[i]-&gt;data_;</div><div class="line">   ...</div><div class="line">   <span class="comment">// 主P2PSync将模型直接拷贝给从P2PSync</span></div><div class="line">   CUDA_CHECK(cudaMemcpyAsync(dst, src, size_ * <span class="keyword">sizeof</span>(Dtype),</div><div class="line">   cudaMemcpyDeviceToDevice, cudaStreamDefault));</div><div class="line">   CUDA_CHECK(cudaStreamSynchronize(cudaStreamDefault));</div><div class="line">   <span class="comment">// 主P2PSync将自身入队，并通知从P2PSync</span></div><div class="line">   children_[i]-&gt;queue_.push(<span class="keyword">this</span>);</div><div class="line"> &#125;</div><div class="line"> <span class="meta">#<span class="meta-keyword">endif</span></span></div><div class="line"> &#125;</div></pre></td></tr></table></figure>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> Dtype&gt;</div><div class="line"><span class="keyword">void</span> P2PSync&lt;Dtype&gt;::on_gradients_ready() &#123;</div><div class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> CPU_ONLY</span></div><div class="line">  ...</div><div class="line">  <span class="comment">// Sum children gradients as they appear in the queue</span></div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; children_.size(); ++i) &#123;</div><div class="line">    P2PSync&lt;Dtype&gt; *child = queue_.pop(); <span class="comment">// 等待从P2PSync入队</span></div><div class="line">    <span class="comment">// 由于parent_grads_是在主P2PSync设备上开辟的一块缓冲区，因此child-&gt;parent_grads_其实就是当前设备上的缓冲区</span></div><div class="line">    Dtype* src = child-&gt;parent_grads_; <span class="comment">// 获取从P2PSync的梯度</span></div><div class="line">    Dtype* dst = diff_;</div><div class="line">    <span class="comment">// 合并从P2PSync的梯度</span></div><div class="line">    caffe_gpu_add(size_, src, dst, dst);</div><div class="line">  &#125;</div><div class="line">  ...</div><div class="line">  <span class="comment">// Send gradients to parent</span></div><div class="line">  <span class="keyword">if</span> (parent_) &#123;</div><div class="line">    Dtype* src = diff_;</div><div class="line">    Dtype* dst = parent_grads_; </div><div class="line">    <span class="comment">// 从P2PSync将梯度复制到主P2PSync的缓冲区</span></div><div class="line">    CUDA_CHECK(cudaMemcpyAsync(dst, src, size_ * <span class="keyword">sizeof</span>(Dtype),  <span class="comment">//</span></div><div class="line">        cudaMemcpyDeviceToDevice, cudaStreamDefault));</div><div class="line">    CUDA_CHECK(cudaStreamSynchronize(cudaStreamDefault));</div><div class="line">    <span class="comment">// 自身入队，通知主P2PSync</span></div><div class="line">    parent_-&gt;queue_.push(<span class="keyword">this</span>);</div><div class="line">  &#125; <span class="keyword">else</span> &#123;</div><div class="line">    <span class="comment">// Loss functions divide gradients by the batch size, so to compensate</span></div><div class="line">    <span class="comment">// for split batch, the root solver divides by number of solvers.</span></div><div class="line">    caffe_gpu_scal(size_, Dtype(<span class="number">1.0</span> / Caffe::solver_count()), diff_);</div><div class="line">  &#125;</div><div class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></div></pre></td></tr></table></figure>
<h2 id="intel-caffe多机并行策略">intel caffe多机并行策略</h2>
<p>单机多卡的训练方式已经足够解决目前大部分模型训练的需求了，但随着数据量越来越大、模型越来越复杂，分布式异构计算成为行业通行的解决方案。BVLC caffe是不支持分布式训练的，intel有两个部门将caffe进行了再次开发以支持分布式和最新的Intel MKL-DNN，分别为intel caffe和caffe multinode。目前BML API已经支持intel caffe的模型训练、评估和预测了。</p>
<p>intel caffe采用的是数据并行的方式，但不同于目前主流的centralized parameter server通信模型，intel caffe借鉴了单机多卡的策略，采用的是一种all-reduce的binary tree模型，也就是将节点按照二叉树组织起来，每个父节点负责1-2个子节点和自己父节点的通信，相比一个中心的PS需要同时与其他多个节点通信的方式，这种binary tree方式将一部分PS的计算平均到了每个节点上，而且相同level的父节点之间可以并行，增加了梯度合并的并行度。</p>
<p>[待图]</p>
<p>为了更好地掩盖通信开销，子节点不需要等到整个模型的梯度都计算完才发送，而是每个layer计算完梯度后就会立即发送给父节点，父节点收到所有子节点的梯度后将本层的梯度合并后也可以立即发送给上一层的父节点。每个layer的参数会按照buffer的大小分成多个part，每个part都会异步地进行发送，当进行下一次迭代时，除了根节点的所有节点都会被阻塞，等待根节点将最终的梯度进行合并，并更新模型后发送给子节点。</p>
<p>除了分层通信外，intel caffe也支持梯度量化压缩，可以将全精浮点数编码成指定字节数的数值，减少节点间通信量。</p>
<p>intel caffe为了支持多种协议的通信，使用了boost的asio::io_service接口，底层实现支持MPI、TCP和UDP，不过目前只实现了MPI接口。</p>
<p>训练时交叉验证是在单节点(准确来说是根节点)上进行的，但每个节点上都需要存在验证集文件，这是因为即使不进行test，其他节点也会初始化test网络。</p>
<h2 id="实战">实战</h2>
<h2 id="参考">参考</h2>
<p>贾扬清自述http://www.yangfenzi.com/keji/59535.html<br>
caffe官网http://caffe.berkeleyvision.org<br>
http://ucb-icsi-vision-group.github.io/caffe-paper/caffe.pdf<br>
https://www.zhihu.com/question/27982282<br>
http://blog.csdn.net/myarrow/article/details/52064608</p>
]]></content>
      
        <categories>
            
            <category> ML framework </category>
            
        </categories>
        
        
        <tags>
            
            <tag> caffe </tag>
            
            <tag> deep learning </tag>
            
            <tag> framework </tag>
            
        </tags>
        
    </entry>
    
  
  
</search>
